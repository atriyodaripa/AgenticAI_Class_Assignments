{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ee280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AgenticAI KrishNaikAcademy\\Classroom HandsOn\\agentic_class_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pprint\n",
    "import pdfplumber\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from qdrant_client import QdrantClient as RawQdrantClient\n",
    "from qdrant_client.http import models\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import uuid\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64edb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langchain and Langsmith tracing\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "\n",
    "## Getting Froq API key\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Getting qdrant vector db API key\n",
    "os.environ['QDRANT_API_KEY'] = os.getenv('QDRANT_API_KEY')\n",
    "os.environ['QDRANT_URL'] = os.getenv('QDRANT_URL')\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "INFERENCE_MODEL = \"gemma2-9b-it\"\n",
    "HNSW_COLLECTION_NAME = \"hnsw_collection\"\n",
    "IVF_COLLECTION_NAME = \"ivf_collection\"\n",
    "FLAT_COLLECTION_NAME = \"flat_collection\"\n",
    "\n",
    "CONTENT_KEY = \"text_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b91d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"Generative AI with LangChain (2024).pdf\"\n",
    "image_folder = \"images\"\n",
    "os.makedirs(image_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a509c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] += os.pathsep + r'C:\\\\Program Files\\\\Tesseract-OCR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f052532",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b5e30",
   "metadata": {},
   "source": [
    "#### Function to extract texts, images and tables from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d433ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elements_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts texts, table contents and images from a PDF.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        documents = []\n",
    "        print(f\"Extracting text contents from '{os.path.basename(pdf_path)}'\")\n",
    "        # Extract text contents using pdfplumber\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text()\n",
    "                if text.strip():\n",
    "                    metadata = {\n",
    "                        \"source\": os.path.basename(pdf_path),\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"type\": \"text\"\n",
    "                    }\n",
    "                    documents.append(Document(page_content=text, metadata=metadata))\n",
    "        \n",
    "        print(f\"Extracting table contents from '{os.path.basename(pdf_path)}'\")\n",
    "        # Extract table contents using pdfplumber\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()\n",
    "                for table_num, table_data in enumerate(tables):\n",
    "                    if table_data: # Ensure table_data is not None or empty\n",
    "                        # Flatten table data into a string format\n",
    "                        table_content = \"\\n\".join([\"\\t\".join(map(str, row)) for row in table_data if row])\n",
    "                        if table_content.strip(): # Only add non-empty tables\n",
    "                            metadata = {\n",
    "                                \"source\": os.path.basename(pdf_path),\n",
    "                                \"page\": page_num + 1,\n",
    "                                \"table_num\": table_num + 1,\n",
    "                                \"type\": \"table\"\n",
    "                            }\n",
    "                            # Add a header to table content to distinguish it\n",
    "                            documents.append(Document(page_content=f\"Table {table_num+1} on page {page_num+1} contains:\\n{table_content}\", metadata=metadata))\n",
    "        \n",
    "        print(f\"Extracting images from '{os.path.basename(pdf_path)}'\")\n",
    "        # Extract images using pdfplumber\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                images = page.images\n",
    "                for image_num, image in enumerate(images):\n",
    "                    if image: # Ensure table_data is not None or empty\n",
    "                        bbox = [image['x0'], page.cropbox[3]-image['y1'],  image['x1'], page.cropbox[3]-image['y0']]\n",
    "                        img_page = page.crop(bbox=bbox)\n",
    "                        img_obj = img_page.to_image(resolution=500)\n",
    "                        # page_number = image['page_number']\n",
    "                        image_name_prefix = f'{page_num}-{image_num + 1}'\n",
    "                        image_name = f'{image_name_prefix}' + \".png\"\n",
    "                        image_path = f'{image_folder}\\\\{image_name}'\n",
    "                        img_obj.save(image_path)\n",
    "                        image_content = pytesseract.image_to_string(Image.open(image_path), lang='eng')\n",
    "                        metadata = {\n",
    "                                \"source\": os.path.basename(pdf_path),\n",
    "                                \"page\": page_num + 1,\n",
    "                                \"image_num\": image_num + 1,\n",
    "                                \"type\": \"image\",\n",
    "                                \"image_path\":image_path\n",
    "                            }\n",
    "                        # Add a header to table content to distinguish it\n",
    "                        documents.append(Document(page_content=f\"Image {image_num+1} on page {page_num+1} contains:\\n{image_content}\", metadata=metadata))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "            print(f\"Error extracting details from {pdf_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd059e7",
   "metadata": {},
   "source": [
    "#### Prompt Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8008232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(query,reranked_docs):\n",
    "    prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"\"\"\n",
    "    You are an expert AI assistant. Answer the following question based on the provided context.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Instructions:\n",
    "    1. Provide a clear, concise answer.\n",
    "    2. If the context doesn't contain the answer, say \"I could not find an answer.\"\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    )\n",
    "    # Example: Format the prompt with retrieved docs\n",
    "    documents_str = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content[:500]}...\" for i, doc in enumerate(reranked_docs)])\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "    query=query,\n",
    "    context=documents_str\n",
    "    )\n",
    "    print(\"===== GENERATED PROMPT =====\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03288d82",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995855b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_docs(doc_chunks, embeddings):\n",
    "    chunk_texts_for_embedding = [chunk.page_content for chunk in doc_chunks]\n",
    "    chunk_embeddings = embeddings.embed_documents(chunk_texts_for_embedding)\n",
    "    upload_data = []\n",
    "    for i, (chunk, vector) in enumerate(zip(doc_chunks, chunk_embeddings)):\n",
    "        current_chunk_metadata = {}\n",
    "        for k, v_meta in chunk.metadata.items():\n",
    "            current_chunk_metadata[k] = v_meta\n",
    "\n",
    "        payload_for_qdrant = {\n",
    "            CONTENT_KEY: chunk.page_content,\n",
    "            **current_chunk_metadata\n",
    "        }\n",
    "        # Generate a unique UUID for each point\n",
    "        unique_id = str(uuid.uuid4()) \n",
    "\n",
    "        upload_data.append(\n",
    "            models.PointStruct(\n",
    "                id=unique_id, \n",
    "                payload=payload_for_qdrant,\n",
    "                vector=vector\n",
    "            )\n",
    "        )\n",
    "    return upload_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dddd13",
   "metadata": {},
   "source": [
    "#### QDRANT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c80d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qdrant:\n",
    "    def __init__(self, url=os.environ['QDRANT_URL'], api_key=os.environ['QDRANT_API_KEY'], timeout=60):\n",
    "        self.url = url\n",
    "        self.api_key = api_key\n",
    "        self.timeout = timeout\n",
    "    \n",
    "    def create_connnection(self):\n",
    "        try:\n",
    "            self.qdrant_client = RawQdrantClient(\n",
    "                url=self.url,\n",
    "                api_key=self.api_key,\n",
    "                timeout=self.timeout # Increased timeout for potentially long operations\n",
    "            )\n",
    "            print(\"Connection successful.\")\n",
    "            return self.qdrant_client\n",
    "        except Exception as e:\n",
    "            self.qdrant_client = None # Set to None if fails\n",
    "            raise Exception (f\"Error connecting to Qdrant: {e}\")\n",
    "    \n",
    "    def get_collection(self):\n",
    "        try:\n",
    "            self.collections = self.qdrant_client.get_collections()\n",
    "            return self.collections\n",
    "        except Exception as e:\n",
    "            raise Exception (f\"Error connecting to Qdrant: {e}\")\n",
    "    \n",
    "    def create_collections(self, collection_name, index_type=\"flat\"):\n",
    "        existing_collections = []\n",
    "        try:\n",
    "            collections_response = self.get_collection()\n",
    "            self.existing_collections = [c.name for c in collections_response.collections]\n",
    "            if collection_name in existing_collections:\n",
    "                print(f\"  Collection '{collection_name}' already exists. No action needed.\")\n",
    "            else:\n",
    "                if index_type == \"flat\":\n",
    "                    self.qdrant_client.create_collection(\n",
    "                        collection_name=collection_name,\n",
    "                        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)\n",
    "                    )\n",
    "                    self.existing_collections.append(collection_name)\n",
    "                elif index_type == \"hnsw\":\n",
    "                    self.qdrant_client.create_collection(\n",
    "                        collection_name=collection_name,\n",
    "                        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "                        hnsw_config={\n",
    "                            \"m\": 16,  # number of connections per layer\n",
    "                            \"ef_construct\": 100,  # size of the dynamic candidate list\n",
    "                            \"full_scan_threshold\": 10000  # threshold for switching to brute force search\n",
    "                        }\n",
    "                    )\n",
    "                    self.existing_collections.append(collection_name)\n",
    "            return self.existing_collections\n",
    "        except Exception as e:\n",
    "                raise Exception (f\"Error connecting to Qdrant: {e}\")\n",
    "    \n",
    "    def upsert_embeddings(self, embeddings_list):\n",
    "        BATCH_SIZE = 100\n",
    "        for collection_name in self.existing_collections:\n",
    "            for i in range(0, len(embeddings_list), BATCH_SIZE):\n",
    "                batch_of_points = embeddings_list[i : i + BATCH_SIZE]\n",
    "                try:\n",
    "                    self.qdrant_client.upsert(\n",
    "                        collection_name=collection_name,\n",
    "                        points=batch_of_points,\n",
    "                        wait=True # Wait for the batch to complete\n",
    "                    )\n",
    "                    count_result = self.qdrant_client.count(collection_name=collection_name, exact=True)\n",
    "                    print(f\"{count_result} is currently present in collection {collection_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error upserting data into '{collection_name}': {e}\")\n",
    "                    continue\n",
    "\n",
    "    def search_vectors(self, collection_name, query_vector, top_results):\n",
    "        search_result_points = self.qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_results,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        manually_created_docs = []\n",
    "        if search_result_points:\n",
    "            for i, point in enumerate(search_result_points):\n",
    "                if point.payload and CONTENT_KEY in point.payload:\n",
    "                    doc_content = point.payload[CONTENT_KEY]\n",
    "                    doc_metadata = {k: v for k, v in point.payload.items() if k != CONTENT_KEY}\n",
    "                    doc_metadata[\"score\"] = point.score\n",
    "                    manually_created_docs.append(\n",
    "                        Document(page_content=doc_content, metadata=doc_metadata)\n",
    "                    )\n",
    "        return manually_created_docs       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b44afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function for deleting collections\n",
    "# qdrant_client = RawQdrantClient(\n",
    "#                 url=os.environ['QDRANT_URL'],\n",
    "#                 api_key=os.environ['QDRANT_API_KEY'],\n",
    "#                 timeout=60 # Increased timeout for potentially long operations\n",
    "#             )\n",
    "# qdrant_client.delete_collection(collection_name=\"hnsw_collection\")\n",
    "# qdrant_client.delete_collection(collection_name=\"flat_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717bdc5",
   "metadata": {},
   "source": [
    "#### Retriever Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344edb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def retrieve_documents(query, collection_name, embeddings_model, vector_db, index_type, top_results):\n",
    "    print('Retrieving Documents initially from the Vector Database.....')\n",
    "    retrieval_times = {}\n",
    "    query_vector = embeddings_model.embed_query(query)\n",
    "    start_time = time.time()\n",
    "    retrieved_docs = vector_db.search_vectors(collection_name, query_vector, top_results)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    retrieval_times[index_type] = duration\n",
    "    print(f\"  Retrieved {len(retrieved_docs)} documents in {duration:.4f} seconds.\")\n",
    "    return retrieved_docs   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd102dd",
   "metadata": {},
   "source": [
    "### Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e334f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_rank_retrieved_docs(query, collection, embeddings_model, vector_db, index_type, top_reranked_retrievals=5):\n",
    "    print('Re Ranking the Retrieved Documents using CrossEncoder')\n",
    "    reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    docs_without_reranking = retrieve_documents(\n",
    "        query=query,\n",
    "        collection_name=collection,\n",
    "        embeddings_model=embeddings_model,\n",
    "        vector_db=vector_db,\n",
    "        index_type=index_type,\n",
    "        top_results=10\n",
    "    )\n",
    "    if docs_without_reranking:\n",
    "        sentence_pairs = [[query, doc.page_content] for doc in docs_without_reranking]\n",
    "        rerank_scores = reranker_model.predict(sentence_pairs)\n",
    "\n",
    "        # Combine documents with their rerank scores\n",
    "        docs_with_rerank_scores = list(zip(docs_without_reranking, rerank_scores))\n",
    "\n",
    "        # Sort the documents by rerank score in descending order\n",
    "        reranked_docs_with_scores = sorted(docs_with_rerank_scores, key=lambda item: item[1], reverse=True)\n",
    "        reranked_docs = [doc[0] for doc in reranked_docs_with_scores]\n",
    "\n",
    "        print(\"Reranked Documents (Top 5):\")\n",
    "        for i, (doc, score) in enumerate(reranked_docs_with_scores[:top_reranked_retrievals]): # Print top 5 reranked results\n",
    "            print(f\"{i+1}. Rerank Score: {score:.4f}, Vector Score: {doc.metadata['score']:.4f}, Page: {doc.metadata['page']}, Source: {doc.metadata['source']}\")\n",
    "        \n",
    "        return reranked_docs[:top_reranked_retrievals]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4ee6b",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f8a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag(pdf_path, embedding_model=EMBEDDING_MODEL):\n",
    "    # PDF Ingestion\n",
    "    print(f'--------Extracting Images, Texts and Tables from PDFs in {pdf_path}--------')\n",
    "    raw_docs = extract_elements_from_pdf(pdf_path)\n",
    "    # Text Splitting\n",
    "    print('--------Splitting the texts extracted from the PDFs--------')\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90)\n",
    "    chunks = text_splitter.split_documents([doc for doc in raw_docs if doc.metadata['type']=='text'])\n",
    "    chunks.extend(doc for doc in raw_docs if doc.metadata['type']!='text')\n",
    "    # Text Embedding\n",
    "    print(f'--------Embedding the Texts, Table data and Image data found in {pdf_path}--------')\n",
    "    data_to_upload = embed_docs(chunks, embeddings)\n",
    "    # Upload embedded data to Vector DB\n",
    "    print('--------Initialising the QDRANT object--------')\n",
    "    qdrant = Qdrant()\n",
    "    print('--------Creating QDRANT API connection--------')\n",
    "    qdrant.create_connnection()\n",
    "    print(f'--------Creating collections {FLAT_COLLECTION_NAME} with FLAT Indexing--------')\n",
    "    qdrant.create_collections(FLAT_COLLECTION_NAME)    # create collection with flat indexing\n",
    "    print(f'--------Creating collections {HNSW_COLLECTION_NAME} with HNSW Indexing--------')\n",
    "    qdrant.create_collections(HNSW_COLLECTION_NAME, index_type=\"hnsw\")    # create collection with hnsw indexing\n",
    "    print('--------Uploading the embedded texts in the Vector DB collections--------')\n",
    "    qdrant.upsert_embeddings(data_to_upload)\n",
    "    return embeddings, qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db818f4",
   "metadata": {},
   "source": [
    "## Retriever Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321953ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data_from_vector_db(query: str, embeddings, vector_db, index_type: str='flat', top_result: int=5):\n",
    "    collection_name = HNSW_COLLECTION_NAME if 'hnsw'=='hnsw' else FLAT_COLLECTION_NAME if 'hnsw'=='flat' else None\n",
    "    reranked_retrieved_docs = re_rank_retrieved_docs(\n",
    "        query=query,\n",
    "        collection=collection_name,\n",
    "        embeddings_model=embeddings, \n",
    "        vector_db=vector_db,\n",
    "        index_type=index_type,\n",
    "        top_reranked_retrievals=top_result\n",
    "    )\n",
    "    return reranked_retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf7993",
   "metadata": {},
   "source": [
    "## Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c190a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_llm(query, context, inference_model):\n",
    "    # Creating the Prompt\n",
    "    print('--------Creating Prompts--------')\n",
    "    prompt = create_prompt(query, context)\n",
    "\n",
    "    # print(\"***********************************************\")\n",
    "\n",
    "    # print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    # print(\"***********************************************\")\n",
    "    print(f'--------Initializing the llm with Groq model {inference_model}--------')\n",
    "    llm = ChatGroq(model=inference_model, temperature=0.3)\n",
    "    print('--------Creating the response from llm based on contexts--------')\n",
    "    llm_response_obj = llm.invoke(prompt)\n",
    "    return llm_response_obj.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779a7d5",
   "metadata": {},
   "source": [
    "## Creating RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "299a6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_chain(query, embeddings, vector_db, inference_model=INFERENCE_MODEL, indexing_type='hnsw'):\n",
    "    print('--------Executing Retriever Pipeline--------')\n",
    "    retrieved_reranked_data = retrieve_data_from_vector_db(query, embeddings, vector_db, index_type=indexing_type)\n",
    "    print('--------Executing Generation Pipeline--------')\n",
    "    response_content = generate_response_from_llm(query, retrieved_reranked_data, inference_model)\n",
    "    return response_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bf2c3",
   "metadata": {},
   "source": [
    "## Main Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4f686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [\"What is Fake LLM?\",\n",
    "              \"How are Chains and agents similar in concepts?\",\n",
    "              \"What are the use of Embeddings?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d5f574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Executing RAG Pipeline--------\n",
      "--------Extracting Images, Texts and Tables from PDFs in Generative AI with LangChain (2024).pdf--------\n",
      "Extracting text contents from 'Generative AI with LangChain (2024).pdf'\n",
      "Extracting table contents from 'Generative AI with LangChain (2024).pdf'\n",
      "Extracting images from 'Generative AI with LangChain (2024).pdf'\n",
      "--------Splitting the texts extracted from the PDFs--------\n",
      "--------Embedding the Texts, Table data and Image data found in Generative AI with LangChain (2024).pdf--------\n",
      "--------Initialising the QDRANT object--------\n",
      "--------Creating QDRANT API connection--------\n",
      "Connection successful.\n",
      "--------Creating collections flat_collection with FLAT Indexing--------\n",
      "--------Creating collections hnsw_collection with HNSW Indexing--------\n",
      "--------Uploading the embedded texts in the Vector DB collections--------\n",
      "count=100 is currently present in collection flat_collection\n",
      "count=200 is currently present in collection flat_collection\n",
      "count=300 is currently present in collection flat_collection\n",
      "count=400 is currently present in collection flat_collection\n",
      "count=500 is currently present in collection flat_collection\n",
      "count=600 is currently present in collection flat_collection\n",
      "count=700 is currently present in collection flat_collection\n",
      "count=800 is currently present in collection flat_collection\n",
      "count=900 is currently present in collection flat_collection\n",
      "count=984 is currently present in collection flat_collection\n",
      "count=100 is currently present in collection hnsw_collection\n",
      "count=200 is currently present in collection hnsw_collection\n",
      "count=300 is currently present in collection hnsw_collection\n",
      "count=400 is currently present in collection hnsw_collection\n",
      "count=500 is currently present in collection hnsw_collection\n",
      "count=600 is currently present in collection hnsw_collection\n",
      "count=700 is currently present in collection hnsw_collection\n",
      "count=800 is currently present in collection hnsw_collection\n",
      "count=900 is currently present in collection hnsw_collection\n",
      "count=984 is currently present in collection hnsw_collection\n",
      "--------Executing Retriever Pipeline--------\n",
      "Re Ranking the Retrieved Documents using CrossEncoder\n",
      "Retrieving Documents initially from the Vector Database.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atriy\\AppData\\Local\\Temp\\ipykernel_9812\\1229100529.py:74: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result_points = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retrieved 10 documents in 0.2168 seconds.\n",
      "Reranked Documents (Top 5):\n",
      "1. Rerank Score: 5.7613, Vector Score: 0.5781, Page: 95, Source: Generative AI with LangChain (2024).pdf\n",
      "2. Rerank Score: -1.0813, Vector Score: 0.4570, Page: 123, Source: Generative AI with LangChain (2024).pdf\n",
      "3. Rerank Score: -1.7648, Vector Score: 0.4845, Page: 66, Source: Generative AI with LangChain (2024).pdf\n",
      "4. Rerank Score: -2.1071, Vector Score: 0.4860, Page: 62, Source: Generative AI with LangChain (2024).pdf\n",
      "5. Rerank Score: -2.6208, Vector Score: 0.4486, Page: 150, Source: Generative AI with LangChain (2024).pdf\n",
      "--------Executing Generation Pipeline--------\n",
      "--------Creating Prompts--------\n",
      "===== GENERATED PROMPT =====\n",
      "--------Initializing the llm with Groq model gemma2-9b-it--------\n",
      "--------Creating the response from llm based on contexts--------\n",
      "Fake LLM is a tool used in Langchain for rapid prototyping and unit testing agents. It allows you to mock various responses from a real LLM, avoiding rate limits and enabling faster iteration on your agent's logic.  \n",
      "\n",
      "--------Executing Retriever Pipeline--------\n",
      "Re Ranking the Retrieved Documents using CrossEncoder\n",
      "Retrieving Documents initially from the Vector Database.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atriy\\AppData\\Local\\Temp\\ipykernel_9812\\1229100529.py:74: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result_points = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retrieved 10 documents in 0.2240 seconds.\n",
      "Reranked Documents (Top 5):\n",
      "1. Rerank Score: 9.0328, Vector Score: 0.7051, Page: 75, Source: Generative AI with LangChain (2024).pdf\n",
      "2. Rerank Score: 4.3803, Vector Score: 0.4958, Page: 76, Source: Generative AI with LangChain (2024).pdf\n",
      "3. Rerank Score: 2.8446, Vector Score: 0.5077, Page: 75, Source: Generative AI with LangChain (2024).pdf\n",
      "4. Rerank Score: 1.4546, Vector Score: 0.4644, Page: 74, Source: Generative AI with LangChain (2024).pdf\n",
      "5. Rerank Score: 0.1742, Vector Score: 0.4905, Page: 216, Source: Generative AI with LangChain (2024).pdf\n",
      "--------Executing Generation Pipeline--------\n",
      "--------Creating Prompts--------\n",
      "===== GENERATED PROMPT =====\n",
      "--------Initializing the llm with Groq model gemma2-9b-it--------\n",
      "--------Creating the response from llm based on contexts--------\n",
      "Both chains and agents in LangChain are designed for composing and combining different components (like LLMs and tools) to achieve complex tasks. \n",
      "\n",
      "They share similarities in their modularity and composability, allowing for flexible and reusable workflows. \n",
      "\n",
      "However, agents go a step further by introducing statefulness through memory, enabling them to maintain context across interactions and handle more dynamic scenarios. \n",
      "\n",
      "--------Executing Retriever Pipeline--------\n",
      "Re Ranking the Retrieved Documents using CrossEncoder\n",
      "Retrieving Documents initially from the Vector Database.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atriy\\AppData\\Local\\Temp\\ipykernel_9812\\1229100529.py:74: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result_points = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retrieved 10 documents in 0.8477 seconds.\n",
      "Reranked Documents (Top 5):\n",
      "1. Rerank Score: 6.5444, Vector Score: 0.6220, Page: 160, Source: Generative AI with LangChain (2024).pdf\n",
      "2. Rerank Score: 5.8518, Vector Score: 0.5761, Page: 158, Source: Generative AI with LangChain (2024).pdf\n",
      "3. Rerank Score: 5.3412, Vector Score: 0.4948, Page: 162, Source: Generative AI with LangChain (2024).pdf\n",
      "4. Rerank Score: 4.6748, Vector Score: 0.4696, Page: 161, Source: Generative AI with LangChain (2024).pdf\n",
      "5. Rerank Score: 4.2470, Vector Score: 0.6658, Page: 158, Source: Generative AI with LangChain (2024).pdf\n",
      "--------Executing Generation Pipeline--------\n",
      "--------Creating Prompts--------\n",
      "===== GENERATED PROMPT =====\n",
      "--------Initializing the llm with Groq model gemma2-9b-it--------\n",
      "--------Creating the response from llm based on contexts--------\n",
      "Embeddings are numerical representations of data objects (like words or sentences) generated by machine learning models. They capture semantic content and allow for comparisons based on similarity. \n",
      "\n",
      "Here are some uses of embeddings:\n",
      "\n",
      "* **Recommendation systems:**  Suggesting items based on user preferences.\n",
      "* **Image and text search:** Finding relevant content based on similarity.\n",
      "* **Anomaly detection:** Identifying unusual patterns or data points.\n",
      "* **Enhancing chatbot responses:**  Improving accuracy and alignment with domain knowledge by incorporating retrieved information based on embedding similarity. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('--------Executing RAG Pipeline--------')\n",
    "embeddings, qdrant_obj = create_rag(pdf_path, EMBEDDING_MODEL)\n",
    "for query in query_list:\n",
    "    response = run_rag_chain(query, embeddings, qdrant_obj)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
