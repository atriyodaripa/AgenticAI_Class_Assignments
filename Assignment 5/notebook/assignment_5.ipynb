{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56db56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a1e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## Langchain and Langsmith tracing\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "\n",
    "## Getting Froq API key\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "INFERENCE_MODEL = \"deepseek-r1-distill-llama-70b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38586823",
   "metadata": {},
   "source": [
    "### Initializing LLM and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50137eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AgenticAI KrishNaikAcademy\\Classroom HandsOn\\agentic_class_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=INFERENCE_MODEL)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "len(embeddings.embed_query('Hi'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aabe30",
   "metadata": {},
   "source": [
    "### Creating the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21ff51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    \"https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\",\n",
    "    \"https://jalammar.github.io/illustrated-gpt2/\",\n",
    "    \"https://jalammar.github.io/illustrated-word2vec/\",\n",
    "    \"https://jalammar.github.io/illustrated-bert/\",\n",
    "    \"https://jalammar.github.io/illustrated-transformer/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "38d941ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nA Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nA Visual Guide to Using BERT for the First Time\\n\\nTranslations: Chinese, Korean, Russian\\n\\n\\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\nDataset: SST2\\nThe dataset we will use in this example is SST2, which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):\\n\\n\\n\\n    sentence\\n    \\n\\n    label\\n    \\n\\n\\n\\n      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\\n    \\n\\n      1\\n    \\n\\n\\n\\n      apparently reassembled from the cutting room floor of any given daytime soap\\n    \\n\\n      0\\n    \\n\\n\\n\\n      they presume their audience won\\'t sit still for a sociology lesson\\n    \\n\\n      0\\n    \\n\\n\\n\\n      this is a visually stunning rumination on love , memory , history and the war between art and commerce\\n    \\n\\n      1\\n    \\n\\n\\n\\n      jonathan parker \\'s bartleby should have been the be all end all of the modern office anomie films\\n    \\n\\n      1\\n    \\n\\n\\nModels: Sentence Sentiment Classification\\nOur goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\\n\\n\\n\\n\\nUnder the hood, the model is actually made up of two model.\\n\\nDistilBERT  processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\\nThe next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the  sentence as either positive or negative (1 or 0, respectively).\\n\\nThe data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\\n\\n\\n\\n\\nIf you’ve read my previous post, Illustrated BERT, this vector is the result of the first position (which receives the [CLS] token as input).\\nModel Training\\nWhile we’ll be using two models, we will only train the logistic regression model. For DistillBERT, we’ll use a model that’s already pre-trained and has a grasp on the English language. This model, however is neither trained not fine-tuned to do sentence classification. We get some sentence classification capability, however, from the general objectives BERT is trained on. This is especially the case with BERT’s output for the first position (associated with the [CLS] token). I believe that’s due to BERT’s second training object – Next sentence classification. That objective seemingly trains the model to encapsulate a sentence-wide sense to the output at the first position. The transformers library provides us with an implementation of DistilBERT as well as pretrained versions of the model.\\n\\n\\n\\n\\nTutorial Overview\\nSo here’s the game plan with this tutorial. We will first use the trained distilBERT to generate sentence embeddings for 2,000 sentences.\\n\\n\\n\\n\\nWe will not touch distilBERT after this step. It’s all Scikit Learn from here. We do the usual train/test split on this dataset:\\n\\n\\n\\n  Train/test split for the output of distilBert (model #1) creates the dataset we\\'ll train and evaluate logistic regression on (model #2). Note that in reality, sklearn\\'s train/test split shuffles the examples before making the split, it doesn\\'t just take the first 75% of examples as they appear in the dataset.\\n\\nThen we train the logistic regression model on the training set:\\n\\n\\n\\n\\nHow a single prediction is calculated\\nBefore we dig into the code and explain how to train the model, let’s look at how a trained model calculates its prediction.\\nLet’s try to classify the sentence “a visually stunning rumination on love”. The first step is to use the BERT tokenizer to first split the word into tokens. Then, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence).\\n\\n\\n\\n\\nThe third step the tokenizer does is to replace each token with its id from the embedding table which is a component we get with the trained model. Read The Illustrated Word2vec for a background on word embeddings.\\n\\n\\n\\n\\nNote that the tokenizer does all these steps in a single line of code:\\ntokenizer.encode(\"a visually stunning rumination on love\", add_special_tokens=True)\\n\\nOur input sentence is now the proper shape to be passed to DistilBERT.\\nIf you’ve read Illustrated BERT, this step can also be visualized in this manner:\\n\\n\\n\\n\\nFlowing Through DistilBERT\\nPassing the input vector through DistilBERT works just like BERT. The output would be a vector for each input token. each vector is made up of 768 numbers (floats).\\n\\n\\n\\n\\nBecause this is a sentence classification task, we ignore all except the first vector (the one associated with the [CLS] token). The one vector we pass as the input to the logistic regression model.\\n\\n\\n\\n\\nFrom here, it’s the logistic regression model’s job to classify this vector based on what it learned from its training phase. We can think of a prediction calculation as looking like this:\\n\\n\\n\\n\\nThe training is what we’ll discuss in the next section, along with the code of the entire process.\\nThe Code\\nIn this section we’ll highlight the code to train this sentence classification model. A notebook containing all this code is available on colab and github.\\nLet’s start by importing the tools of the trade\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport transformers as ppb # pytorch transformers\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\n\\nThe dataset is available as a file on github, so we just import it directly into a pandas dataframe\\ndf = pd.read_csv(\\'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv\\', delimiter=\\'\\\\t\\', header=None)\\n\\nWe can use df.head() to look at the first five rows of the dataframe to see how the data looks.\\ndf.head()\\n\\nWhich outputs:\\n\\n\\n\\n\\nImporting pre-trained DistilBERT model and tokenizer\\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, \\'distilbert-base-uncased\\')\\n\\n## Want BERT instead of distilBERT? Uncomment the following line:\\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, \\'bert-base-uncased\\')\\n\\n# Load pretrained model/tokenizer\\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\\nmodel = model_class.from_pretrained(pretrained_weights)\\n\\nWe can now tokenize the dataset. Note that we’re going to do things a little differently here from the example above. The example above tokenized and processed only one sentence. Here, we’ll tokenize and process all sentences together as a batch (the notebook processes a smaller group of examples just for resource considerations, let’s say 2000 examples).\\nTokenization\\ntokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\\n\\nThis turns every sentence into the list of ids.\\n\\n\\n\\n\\nThe dataset is currently a list (or pandas Series/DataFrame) of lists. Before DistilBERT can process this as input, we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0. You can refer to the notebook for the padding step, it’s basic python string and array manipulation.\\nAfter the padding, we have a matrix/tensor that is ready to be passed to BERT:\\n\\n\\n\\n\\nProcessing with DistilBERT\\nWe now create an input tensor out of the padded token matrix, and send that to DistilBERT\\ninput_ids = torch.tensor(np.array(padded))\\n\\nwith torch.no_grad():\\n    last_hidden_states = model(input_ids)\\n\\nAfter running this step, last_hidden_states holds the outputs of DistilBERT. It is a tuple with the shape (number of examples, max number of tokens in the sequence, number of hidden units in the DistilBERT model). In our case, this will be 2000 (since we only limited ourselves to 2000 examples), 66 (which is the number of tokens in the longest sequence from the 2000 examples), 768 (the number of hidden units in the DistilBERT model).\\n\\n\\n\\n\\nUnpacking the BERT output tensor\\nLet’s unpack this 3-d output tensor. We can first start by examining its dimensions:\\n\\n\\n\\n\\nRecapping a sentence’s journey\\nEach row is associated with a sentence from our dataset. To recap the processing path of the first sentence, we can think of it as looking like this:\\n\\n\\n\\n\\nSlicing the important part\\nFor sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.\\n\\n\\n\\n\\nThis is how we slice that 3d tensor to get the 2d tensor we’re interested in:\\n # Slice the output for the first position for all the sequences, take all hidden unit outputs\\nfeatures = last_hidden_states[0][:,0,:].numpy()\\n\\nAnd now features is a 2d numpy array containing the sentence embeddings of all the sentences in our dataset.\\n\\n\\n\\n  The tensor we sliced from BERT\\'s output\\n\\nDataset for Logistic Regression\\nNow that we have the output of BERT, we have assembled the dataset we need to train our logistic regression model. The 768 columns are the features, and the labels we just get from our initial dataset.\\n\\n\\n\\n  The labeled dataset we use to train the Logistic Regression. The features are the output vectors of BERT for the [CLS] token (position #0) that we sliced in the previous figure. Each row corresponds to a sentence in our dataset, each column corresponds to the output of a hidden unit from the feed-forward neural network at the top transformer block of the Bert/DistilBERT model.\\n\\nAfter doing the traditional train/test split of machine learning, we can declare our Logistic Regression model and train it against the dataset.\\nlabels = df[1]\\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels)\\n\\n\\nWhich splits the dataset into training/testing sets:\\n\\n\\n\\n\\nNext, we train the Logistic Regression model on the training set.\\nlr_clf = LogisticRegression()\\nlr_clf.fit(train_features, train_labels)\\n\\nNow that the model is trained, we can score it against the test set:\\nlr_clf.score(test_features, test_labels)\\n\\nWhich shows the model achieves around 81% accuracy.\\nScore Benchmarks\\nFor reference, the highest accuracy score for this dataset is currently 96.8. DistilBERT can be trained to improve its score on this task – a process called fine-tuning which updates BERT’s weights to make it achieve a better performance in the sentence classification (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of 90.7. The full size BERT model achieves 94.9.\\nThe Notebook\\nDive right into the notebook or run it on colab.\\nAnd that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at fine-tuning. You can also go back and switch from distilBERT to BERT and see how that works.\\nThanks to Clément Delangue, Victor Sanh, and the Huggingface team for providing feedback to earlier versions of this tutorial.\\n\\n\\n    Written on November 26, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated GPT-2 (Visualizing Transformer Language Models)\\n\\nDiscussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n\\n\\nContents\\n\\n\\nPart 1: GPT2 And Language Modeling\\n\\nWhat is a Language Model\\nTransformers for Language Modeling\\nOne Difference From BERT\\nThe Evolution of The Transformer Block\\nCrash Course in Brain Surgery: Looking Inside GPT-2\\nA Deeper Look Inside\\nEnd of part #1: The GPT-2, Ladies and Gentlemen\\n\\n\\nPart 2: The Illustrated Self-Attention\\n\\nSelf-Attention (without masking)\\n1- Create Query, Key, and Value Vectors\\n2- Score\\n3- Sum\\nThe Illustrated Masked Self-Attention\\nGPT-2 Masked Self-Attention\\nBeyond Language modeling\\nYou’ve Made it!\\n\\n\\nPart 3: Beyond Language Modeling\\n\\nMachine Translation\\nSummarization\\nTransfer Learning\\nMusic Generation\\n\\n\\n\\n\\nPart #1: GPT2 And Language Modeling #\\nSo what exactly is a language model?\\nWhat is a Language Model\\nIn The Illustrated Word2vec, we’ve looked at what a language model is – basically a machine learning model that is able to look at part of a sentence and predict the next word. The most famous language models are smartphone keyboards that suggest the next word based on what you’ve currently typed.\\n\\n\\n\\n\\nIn this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest variant of the trained GPT-2, takes up 500MBs of storage to store all of its parameters. The largest GPT-2 variant is 13 times the size so it could take up more than 6.5 GBs of storage space.\\n\\n\\n\\n\\nOne great way to experiment with GPT-2 is using the AllenAI GPT-2 Explorer. It uses GPT-2 to display ten possible predictions for the next word (alongside their probability score). You can select a word then see the next list of predictions to continue writing the passage.\\nTransformers for Language Modeling\\nAs we’ve seen in The Illustrated Transformer, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. That architecture was appropriate because the model tackled machine translation  – a problem where encoder-decoder architectures have been successful in the past.\\n\\n\\n\\n\\nA lot of the subsequent research work saw the architecture shed either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible, feeding them massive amounts of training text, and throwing vast amounts of compute at them (hundreds of thousands of dollars to train some of these language models, likely millions in the case of AlphaStar).\\n\\n\\n\\n\\nHow high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:\\n\\n\\n\\n\\nOne Difference From BERT\\n\\nFirst Law of Robotics\\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\\n\\nThe GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let’s for example prompt a well-trained GPT-2 to recite the first law of robotics:\\n\\n\\n\\n\\nThe way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “auto-regression”. This is one of the ideas that made RNNs unreasonably effective.\\n\\n\\n\\n\\nThe GPT2, and some later models like TransformerXL and XLNet are auto-regressive in nature. BERT is not. That is a trade off. In losing auto-regression, BERT gained the ability to incorporate the context on both sides of a word to gain better results. XLNet brings back autoregression while finding an alternative way to incorporate the context on both sides.\\nThe Evolution of the Transformer Block\\nThe initial transformer paper introduced two types of transformer blocks:\\nThe Encoder Block\\nFirst is the encoder block:\\n\\n\\n\\n  An encoder block from the original transformer paper can take inputs up until a certain max sequence length (e.g. 512 tokens). It\\'s okay if an input sequence is shorter than this limit, we can just pad the rest of the sequence.\\n\\nThe Decoder Block\\nSecond, there’s the decoder block which has a small architectural variation from the encoder block – a layer to allow it to pay attention to specific segments from the encoder:\\n\\n\\n\\n\\nOne key difference in the self-attention layer here, is that it masks future tokens – not by changing the word to [mask] like BERT, but by interfering in the self-attention calculation blocking information from tokens that are to the right of the position being calculated.\\nIf, for example, we’re to highlight the path of position #4, we can see that it is only allowed to attend to the present and previous tokens:\\n\\n\\n\\n\\nIt’s important that the distinction between self-attention (what BERT uses) and masked self-attention (what GPT-2 uses) is clear. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening:\\n\\n\\n\\n\\nThe Decoder-Only Block\\nSubsequent to the original paper, Generating Wikipedia by Summarizing Long Sequences proposed another arrangement of the transformer block that is capable of doing language modeling. This model threw away the Transformer encoder. For that reason, let’s call the model the “Transformer-Decoder”. This early transformer-based language model was made up of a stack of six transformer decoder blocks:\\n\\n\\n\\n  The decoder blocks are identical. I have expanded the first one so you can see its self-attention layer is the masked variant. Notice that the model now can address up to 4,000 tokens in a certain segment -- a massive upgrade from the 512 in the original transformer.\\n\\nThese blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in Character-Level Language Modeling with Deeper Self-Attention to create a language model that predicts one letter/character at a time.\\nThe OpenAI GPT-2 model uses these decoder-only blocks.\\nCrash Course in Brain Surgery: Looking Inside GPT-2\\n\\nLook inside and you will see,\\nThe words are cutting deep inside my brain.\\nThunder burning, quickly burning,\\nKnife of words is driving me insane, insane yeah.\\n~Budgie\\n\\nLet’s lay a trained GPT-2 on our surgery table and look at how it works.\\n\\n\\n\\n  The GPT-2 can process 1024 tokens. Each token flows through all the decoder blocks along its own path.\\n\\nThe simplest way to run a trained GPT-2 is to allow it to ramble on its own (which is technically called generating unconditional samples) – alternatively, we can give it a prompt to have it speak about a certain topic (a.k.a generating interactive conditional samples). In the rambling case, we can simply hand it the start token and have it start generating words (the trained model uses <|endoftext|> as its start token. Let’s call it <s> instead).\\n\\n\\n\\n\\nThe model only has one input token, so that path would be the only active one. The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model’s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2). In this case we selected the token with the highest probability, ‘the’. But we can certainly mix things up – you know how if you keep clicking the suggested word in your keyboard app, it sometimes can stuck in repetitive loops where the only way out is if you click the second or third suggested word. The same can happen here. GPT-2 has a parameter called top-k that we can use to have the model consider sampling words other than the top word (which is the case when top-k = 1).\\nIn the next step, we add the output from the first step to our input sequence, and have the model make its next prediction:\\n\\n\\n\\n\\nNotice that the second path is the only one that’s active in this calculation. Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we’ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token.\\nA Deeper Look Inside\\nInput Encoding\\nLet’s look at more details to get to know the model more intimately. Let’s start from the input. As in other NLP models we’ve discussed before, the model looks up the embedding of the input word in its embedding matrix – one of the components we get as part of a trained model.\\n\\n\\n\\n  Each row is a word embedding: a list of numbers representing a word and capturing some of its meaning. The size of that list is different in different GPT2 model sizes. The smallest model uses an embedding size of 768 per word/token.\\n\\nSo in the beginning, we look up the embedding of the start token <s> in the embedding matrix. Before handing that to the first block in the model, we need to incorporate positional encoding – a signal that indicates the order of the words in the sequence to the transformer blocks. Part of the trained model is a matrix that contains a positional encoding vector for each of the 1024 positions in the input.\\n\\n\\n\\n\\nWith this, we’ve covered how input words are processed before being handed to the first transformer block. We also know two of the weight matrices that constitute the trained GPT-2.\\n\\n\\n\\n  Sending a word to the first transformer block means looking up its embedding and adding up the positional encoding vector for position #1.\\n\\nA journey up the Stack\\nThe first block can now process the token by first passing it through the self-attention process, then passing it through its neural network layer. Once the first transformer block processes the token, it sends its resulting vector up the stack to be processed by the next block. The process is identical in each block, but each block has its own weights in both self-attention and the neural network sublayers.\\n\\n\\n\\n\\nSelf-Attention Recap\\nLanguage heavily relies on context. For example, look at the second law:\\n\\nSecond Law of Robotics\\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\\n\\n\\nI have highlighted three places in the sentence where the words are referring to other words. There is no way to understand or process these words without incorporating the context they are referring to. When a model processes this sentence, it has to be able to know that:\\n\\nit refers to the robot\\nsuch orders refers to the earlier part of the law, namely “the orders given it by human beings”\\nThe First Law refers to the entire First Law\\n\\nThis is what self-attention does. It bakes in the model’s understanding of relevant and associated words that explain the context of a certain word before processing that word (passing it through a neural network). It does that by assigning scores to how relevant each word in the segment is, and adding up their vector representation.\\nAs an example, this self-attention layer in the top block is paying attention to “a robot” when it processes the word “it”. The vector it will pass to its neural network is a sum of the vectors for each of the three words multiplied by their scores.\\n\\n\\n\\n\\nSelf-Attention Process\\nSelf-attention is processed along the path of each token in the segment. The significant components are three vectors:\\n\\nQuery: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.\\nKey: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.\\nValue: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word.\\n\\n\\n\\n\\n\\nA crude analogy is to think of it like searching through a filing cabinet. The query is like a sticky note with the topic you’re researching. The keys are like the labels of the folders inside the cabinet. When you match the tag with a sticky note, we take out the contents of that folder, these contents are the value vector. Except you’re not only looking for one value, but a blend of values from a blend of folders.\\nMultiplying the query vector by each key vector produces a score for each folder (technically: dot product followed by softmax).\\n\\n\\n\\n\\nWe multiply each value by its score and sum up – resulting in our self-attention outcome.\\n\\n\\n\\n\\nThis weighted blend of value vectors results in a vector that paid 50% of its “attention” to the word robot, 30% to the word a, and 19% to the word it. Later in the post, we’ll got deeper into self-attention. But first, let’s continue our journey up the stack towards the output of the model.\\nModel Output\\nWhen the top block in the model produces its output vector (the result of its own self-attention followed by its own neural network), the model multiplies that vector by the embedding matrix.\\n\\n\\n\\n\\nRecall that each row in the embedding matrix corresponds to the embedding of a word in the model’s vocabulary. The result of this multiplication is interpreted as a score for each word in the model’s vocabulary.\\n\\n\\n\\n\\nWe can simply select the token with the highest score (top_k = 1). But better results are achieved if the model considers other words as well. So a better strategy is to sample a word from the entire list using the score as the probability of selecting that word (so words with a higher score have a higher chance of being selected). A middle ground is setting top_k to 40, and having the model consider the 40 words with the highest scores.\\n\\n\\n\\n\\nWith that, the model has completed an iteration resulting in outputting a single word. The model continues iterating until the entire context is generated (1024 tokens) or until an end-of-sequence token is produced.\\nEnd of part #1: The GPT-2, Ladies and Gentlemen\\nAnd there we have it. A run down of how the GPT2 works. If you’re curious to know exactly what happens inside the self-attention layer, then the following bonus section is for you. I created it to introduce more visual language to describe self-attention in order to make describing later transformer models easier to examine and describe (looking at you, TransformerXL and XLNet).\\nI’d like to note a few oversimplifications in this post:\\n\\nI used “words” and “tokens” interchangeably. But in reality, GPT2 uses Byte Pair Encoding to create the tokens in its vocabulary. This means the tokens are usually parts of words.\\nThe example we showed runs GPT2 in its inference/evaluation mode. That’s why it’s only processing one word at a time. At training time, the model would be trained against longer sequences of text and processing multiple tokens at once. Also at training time, the model would process larger batch sizes (512) vs. the batch size of one that evaluation uses.\\nI took liberties in rotating/transposing vectors to better manage the spaces in the images. At implementation time, one has to be more precise.\\nTransformers use a lot of layer normalization, which is pretty important. We’ve noted a few of these in the Illustrated Transformer, but focused more on self-attention in this post.\\nThere are times when I needed to show more boxes to represent a vector. I indicate those as “zooming in”. For example:\\n\\n\\n\\n\\n\\nPart #2: The Illustrated Self-Attention #\\nEarlier in the post we showed this image to showcase self-attention being applied in a layer that is processing the word it:\\n\\n\\n\\n\\nIn this section, we’ll look at the details of how that is done. Note that we’ll look at it in a way to try to make sense of what happens to individual words. That’s why we’ll be showing many single vectors. The actual implementations are done by multiplying giant matrices together. But I want to focus on the intuition of what happens on a word-level here.\\nSelf-Attention (without masking)\\nLet’s start by looking at the original self-attention as it’s calculated in an encoder block. Let’s look at a toy transformer block that can only process four tokens at a time.\\nSelf-attention is applied through three main steps:\\n\\nCreate the Query, Key, and Value vectors for each path.\\nFor each input token, use its query vector to score against all the other key vectors\\nSum up the value vectors after multiplying them by their associated scores.\\n\\n\\n\\n\\n\\n1- Create Query, Key, and Value Vectors\\nLet’s focus on the first path. We’ll take its query, and compare against all the keys. That produces a score for each key. The first step in self-attention is to calculate the three vectors for each token path (let’s ignore attention heads for now):\\n\\n\\n\\n\\n2- Score\\nNow that we have the vectors, we use the query and key vectors only for step #2. Since we’re focused on the first token, we multiply its query by all the other key vectors resulting in a score for each of the four tokens.\\n\\n\\n\\n\\n3- Sum\\nWe can now multiply the scores by the value vectors. A value with a high score will constitute a large portion of the resulting vector after we sum them up.\\n\\n\\n\\n  The lower the score, the more transparent we\\'re showing the value vector. That\\'s to indicate how multiplying by a small number dilutes the values of the vector.\\n\\nIf we do the same operation for each path, we end up with a vector representing each token containing the appropriate context of that token. Those are then presented to the next sublayer in the transformer block (the feed-forward neural network):\\n\\n\\n\\n\\nThe Illustrated Masked Self-Attention\\nNow that we’ve looked inside a transformer’s self-attention step, let’s proceed to look at masked self-attention. Masked self-attention is identical to self-attention except when it comes to step #2. Assuming the model only has two tokens as input and we’re observing the second token. In this case, the last two tokens are masked. So the model interferes in the scoring step. It basically always scores the future tokens as 0 so the model can’t peak to future words:\\n\\n\\n\\n\\nThis masking is often implemented as a matrix called an attention mask. Think of a sequence of four words (“robot must obey orders”, for example). In a language modeling scenario, this sequence is absorbed in four steps – one per word (assuming for now that every word is a token). As these models work in batches, we can assume a batch size of 4 for this toy model that will process the entire sequence (with its four steps) as one batch.\\n\\n\\n\\n\\nIn matrix form, we calculate the scores by multiplying a queries matrix by a keys matrix. Let’s visualize it as follows, except instead of the word, there would be the query (or key) vector associated with that word in that cell:\\n\\n\\n\\n\\nAfter the multiplication, we slap on our attention mask triangle. It set the cells we want to mask to -infinity or a very large negative number (e.g. -1 billion in GPT2):\\n\\n\\n\\n\\nThen, applying softmax on each row produces the actual scores we use for self-attention:\\n\\n\\n\\n\\nWhat this scores table means is the following:\\n\\nWhen the model processes the first example in the dataset (row #1), which contains only one word (“robot”), 100% of its attention will be on that word.\\nWhen the model processes the second example in the dataset (row #2), which contains the words (“robot must”), when it processes the word “must”, 48% of its attention will be on “robot”, and 52% of its attention will be on “must”.\\nAnd so on\\n\\nGPT-2 Masked Self-Attention\\nLet’s get into more detail on GPT-2’s masked attention.\\nEvaluation Time: Processing One Token at a Time\\nWe can make the GPT-2 operate exactly as masked self-attention works. But during evaluation, when our model is only adding one new word after each iteration, it would be inefficient to recalculate self-attention along earlier paths for tokens that have already been processed.\\nIn this case, we process the first token (ignoring <s> for now).\\n\\n\\n\\n\\nGPT-2 holds on to the key and value vectors of the the a token. Every self-attention layer holds on to its respective key and value vectors for that token:\\n\\n\\n\\n\\nNow in the next iteration, when the model processes the word robot, it does not need to generate query, key, and value queries for the a token. It just reuses the ones it saved from the first iteration:\\n\\n\\n\\n\\nGPT-2 Self-attention: 1- Creating queries, keys, and values\\nLet’s assume the model is processing the word it. If we’re talking about the bottom block, then its input for that token would be the embedding of it + the positional encoding for slot #9:\\n\\n\\n\\n\\nEvery block in a transformer has its own weights (broken down later in the post). The first we encounter is the weight matrix that we use to create the queries, keys, and values.\\n\\n\\n\\n  Self-attention multiplies its input by its weight matrix (and adds a bias vector, not illustrated here).\\n\\nThe multiplication results in a vector that’s basically a concatenation of the query, key, and value vectors for the word it.\\n\\n\\n\\n  Multiplying the input vector by the attention weights vector (and adding a bias vector aftwards) results in the key, value, and query vectors for this token.\\n\\nGPT-2 Self-attention: 1.5- Splitting into attention heads\\nIn the previous examples, we dove straight into self-attention ignoring the “multi-head” part. It would be useful to shed some light on that concept now. Self attention is conducted multiple times on different parts of the Q,K,V vectors. “Splitting” attention heads is simply reshaping the long vector into a matrix. The small GPT2 has 12 attention heads, so that would be the first dimension of the reshaped matrix:\\n\\n\\n\\n\\nIn the previous examples, we’ve looked at what happens inside one attention head. One way to think of multiple attention-heads is like this (if we’re to only visualize three of the twelve attention heads):\\n\\n\\n\\n\\nGPT-2 Self-attention: 2- Scoring\\nWe can now proceed to scoring – knowing that we’re only looking at one attention head (and that all the others are conducting a similar operation):\\n\\n\\n\\n\\nNow the token can get scored against all of keys of the other tokens (that were calculated in attention head #1 in previous iterations):\\n\\n\\n\\n\\nGPT-2 Self-attention: 3- Sum\\nAs we’ve seen before, we now multiply each value with its score, then sum them up, producing the result of self-attention for attention-head #1:\\n\\n\\n\\n\\nGPT-2 Self-attention: 3.5- Merge attention heads\\nThe way we deal with the various attention heads is that we first concatenate them into one vector:\\n\\n\\n\\n\\nBut the vector isn’t ready to be sent to the next sublayer just yet. We need to first turn this Frankenstein’s-monster of hidden states into a homogenous representation.\\nGPT-2 Self-attention: 4- Projecting\\nWe’ll let the model learn how to best map concatenated self-attention results into a vector that the feed-forward neural network can deal with. Here comes our second large weight matrix that projects the results of the attention heads into the output vector of the self-attention sublayer:\\n\\n\\n\\n\\nAnd with this, we have produced the vector we can send along to the next layer:\\n\\n\\n\\n\\nGPT-2 Fully-Connected Neural Network: Layer #1\\nThe fully-connected neural network is where the block processes its input token after self-attention has included the appropriate context in its representation. It is made up of two layers. The first layer is four times the size of the model (Since GPT2 small is 768, this network would have 768*4 = 3072 units). Why four times? That’s just the size the original transformer rolled with (model dimension was 512 and layer #1 in that model was 2048). This seems to give transformer models enough representational capacity to handle the tasks that have been thrown at them so far.\\n\\n\\n\\n  (Not shown: A bias vector)\\n\\nGPT-2 Fully-Connected Neural Network: Layer #2 - Projecting to model dimension\\nThe second layer projects the result from the first layer back into model dimension (768 for the small GPT2). The result of this multiplication is the result of the transformer block for this token.\\n\\n\\n\\n  (Not shown: A bias vector)\\n\\nYou’ve Made It!\\nThat’s the most detailed version of the transformer block we’ll get into! You now pretty much have the vast majority of the picture of what happens inside of a transformer language model. To recap, our brave input vector encounters these weight matrices:\\n\\n\\n\\n\\nAnd each block has its own set of these weights. On the other hand, the model has only one token embedding matrix and one positional encoding matrix:\\n\\n\\n\\n\\nIf you want to see all the parameters of the model, then I have tallied them here:\\n\\n\\n\\n\\nThey add up to 124M parameters instead of 117M for some reason. I’m not sure why, but that’s how many of them seems to be in the published code (please correct me if I’m wrong).\\nPart 3: Beyond Language Modeling #\\nThe decoder-only transformer keeps showing promise beyond language modeling. There are plenty of applications where it has shown success which can be described by similar visuals as the above. Let’s close this post by looking at some of these applications\\nMachine Translation\\nAn encoder is not required to conduct translation. The same task can be addressed by a decoder-only transformer:\\n\\n\\n\\n\\nSummarization\\nThis is the task that the first decoder-only transformer was trained on. Namely, it was trained to read a wikipedia article (without the opening section before the table of contents), and to summarize it. The actual opening sections of the articles were used as the labels in the training datasest:\\n\\n\\n\\n\\nThe paper trained the model against wikipedia articles, and thus the trained model was able to summarize articles:\\n\\n\\n\\n\\nTransfer Learning\\nIn Sample Efficient Text Summarization Using a Single Pre-Trained Transformer, a decoder-only transformer is first pre-trained on language modeling, then finetuned to do summarization. It turns out to achieve better results than a pre-trained encoder-decoder transformer in limited data settings.\\nThe GPT2 paper also shows results of summarization after pre-training the model on language modeling.\\nMusic Generation\\nThe Music Transformer uses a decoder-only transformer to generate music with expressive timing and dynamics. “Music Modeling” is just like language modeling – just let the model learn music in an unsupervised way, then have it sample outputs (what we called “rambling”, earlier).\\nYou might be curious as to how music is represented in this scenario. Remember that language modeling can be done through vector representations of either characters, words, or tokens that are parts of words. With a musical performance (let’s think about the piano for now), we have to represent the notes, but also velocity – a measure of how hard the piano key is pressed.\\n\\n\\n\\n\\nA performance is just a series of these one-hot vectors. A midi file can be converted into such a format. The paper has the following example input sequence:\\n\\n\\n\\n\\nThe one-hot vector representation for this input sequence would look like this:\\n\\n\\n\\n\\nI love a visual in the paper that showcases self-attention in the Music Transformer. I’ve added some annotations to it here:\\n\\n\\n\\n  \"Figure 8: This piece has a recurring triangular contour. The query is at one of the latter peaks and it attends to all of the previous high notes on the peak, all the way to beginning of the piece.\" ... \"[The] figure shows a query (the source of all the attention lines) and previous memories being attended to (the notes that are receiving more softmax probabiliy is highlighted in). The coloring of the attention lines correspond to different heads and the width to the weight of the softmax probability.\"\\n\\nIf you’re unclear on this representation of musical notes, check out this video.\\nConclusion\\nThis concludes our journey into the GPT2, and our exploration of its parent model, the decoder-only transformer. I hope that you come out of this post with a better understanding of self-attention and more comfort that you understand more of what goes on inside a transformer.\\nResources\\n\\nThe GPT2 Implementation from OpenAI\\nCheck out the pytorch-transformers library from Hugging Face in addition to GPT2, it implements BERT, Transformer-XL, XLNet and other cutting-edge transformer models.\\n\\nAcknowledgements\\nThanks to Lukasz Kaiser, Mathias Müller, Peter J. Liu, Ryan Sepassi and Mohammad Saleh for feedback on earlier versions of this post.\\nComments or corrections? Please tweet me at @JayAlammar\\n\\n\\n\\n    Written on August 12, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Word2vec\\n\\n Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\nPersonality Embeddings: What are you like?\\n\\n“I give you the desert chameleon, whose ability to blend itself into the background tells you all you need to know about the roots of ecology and the foundations of a personal identity” ~Children of Dune\\n\\nOn a scale of 0 to 100, how introverted/extraverted are you (where 0 is the most introverted, and 100 is the most extraverted)?\\nHave you ever taken a personality test like MBTI – or even better, the Big Five Personality Traits test? If you haven’t, these are tests that ask you a list of questions, then score you on a number of axes, introversion/extraversion being one of them.\\n\\n\\n\\n  Example of the result of a Big Five Personality Trait test. It can really tell you a lot about yourself and is shown to have predictive ability in academic, personal, and professional success. This is one place to find your results.\\n\\nImagine I’ve scored 38/100 as my introversion/extraversion score. we can plot that in this way:\\n\\n\\n\\nLet’s switch the range to be from -1 to 1:\\n\\n\\n\\nHow well do you feel you know a person knowing only this one piece of information about them? Not much. People are complex. So let’s add another dimension – the score of one other trait from the test.\\n\\n\\n\\n  We can represent the two dimensions as a point on the graph, or better yet, as a vector from the origin to that point. We have incredible tools to deal with vectors that will come in handy very shortly.\\n\\nI’ve hidden which traits we’re plotting just so you get used to not knowing what each dimension represents – but still getting a lot of value from the vector representation of a person’s personality.\\nWe can now say that this vector partially represents my personality. The usefulness of such representation comes when you want to compare two other people to me. Say I get hit by a bus and I need to be replaced by someone with a similar personality. In the following figure, which of the two people is more similar to me?\\n\\n\\n\\nWhen dealing with vectors, a common way to calculate a similarity score is cosine_similarity:\\n\\n\\n\\nPerson #1 is more similar to me in personality. Vectors pointing at the same direction (length plays a role as well) have a higher cosine similarity score.\\n\\nYet again, two dimensions aren’t enough to capture enough information about how different people are. Decades of psychology research have led to five major traits (and plenty of sub-traits). So let’s use all five dimensions in our comparison:\\n\\n\\n\\n\\nThe problem with five dimensions is that we lose the ability to draw neat little arrows in two dimensions. This is a common challenge in machine learning where we often have to think in higher-dimensional space. The good thing is, though, that cosine_similarity still works. It works with any number of dimensions:\\n\\n\\n\\n  cosine_similarity works for any number of dimensions. These are much better scores because they\\'re calculated based on a higher resolution representation of the things being compared.\\n\\nAt the end of this section, I want us to come out with two central ideas:\\n\\nWe can represent people (and things) as vectors of numbers (which is great for machines!).\\nWe can easily calculate how similar vectors are to each other.\\n\\n\\n\\n\\n\\nWord Embeddings\\n\\n“The gift of words is the gift of deception and illusion” ~Children of Dune\\n\\nWith this understanding, we can proceed to look at trained word-vector examples (also called word embeddings) and start looking at some of their interesting properties.\\nThis is a word embedding for the word “king” (GloVe vector trained on Wikipedia):\\n\\n[ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\\n -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,\\n -0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\\n -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,\\n  0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\\n  1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,\\n  0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\\n -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\\n -0.64426 , -0.51042 ]\\n \\nIt’s a list of 50 numbers. We can’t tell much by looking at the values. But let’s visualize it a bit so we can compare it other word vectors. Let’s put all these numbers in one row:\\n\\n\\n\\n\\nLet’s color code the cells based on their values (red if they’re close to 2, white if they’re close to 0, blue if they’re close to -2):\\n\\n\\n\\n\\nWe’ll proceed by ignoring the numbers and only looking at the colors to indicate the values of the cells. Let’s now contrast “King” against other words:\\n\\n\\n\\n\\nSee how “Man” and “Woman” are much more similar to each other than either of them is to “king”? This tells you something. These vector representations capture quite a bit of the information/meaning/associations of these words.\\nHere’s another list of examples (compare by vertically scanning the columns looking for columns with similar colors):\\n\\n\\n\\n\\nA few things to point out:\\n\\nThere’s a straight red column through all of these different words. They’re similar along that dimension (and we don’t know what each dimensions codes for)\\nYou can see how “woman” and “girl” are similar to each other in a lot of places. The same with “man” and “boy”\\n“boy” and “girl” also have places where they are similar to each other, but different from “woman” or “man”. Could these be coding for a vague conception of youth? possible.\\nAll but the last word are words representing people. I added an object (water) to show the differences between categories. You can, for example, see that blue column going all the way down and stopping before the embedding for “water”.\\nThere are clear places where “king” and “queen” are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?\\n\\nAnalogies\\n\\n\"Words can carry any burden we wish. All that\\'s required is agreement and a tradition upon which to build.\" ~God Emperor of Dune\\n\\nThe famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:\\n\\n\\n\\n  Using the Gensim library in python, we can add and subtract word vectors, and it would find the most similar words to the resulting vector. The image shows a list of the most similar words, each with its cosine similarity.\\n\\nWe can visualize this analogy as we did previously:\\n\\n\\n\\n  The resulting vector from \"king-man+woman\" doesn\\'t exactly equal \"queen\", but \"queen\" is the closest word to it from the 400,000 word embeddings we have in this collection.\\n\\nNow that we’ve looked at trained word embeddings, let’s learn more about the training process. But before we get to word2vec, we need to look at a conceptual parent of word embeddings: the neural language model.\\nLanguage Modeling\\n\\n  “The prophet is not diverted by illusions of past, present and future. The fixity of language determines such linear distinctions. Prophets hold a key to the lock in a language.  \\n\\n  This is not a mechanical universe. The linear progression of events is imposed by the observer. Cause and effect? That\\'s not it at all. The prophet utters fateful words. You glimpse a thing \"destined to occur.\" But the prophetic instant releases something of infinite portent and power. The universe undergoes a ghostly shift.” ~God Emperor of Dune\\n\\nIf one wanted to give an example of an NLP application, one of the best examples would be the next-word prediction feature of a smartphone keyboard. It’s a feature that billions of people use hundreds of times every day.\\n\\n\\n\\n\\nNext-word prediction is a task that can be addressed by a language model. A language model can take a list of words (let’s say two words), and attempt to predict the word that follows them.\\nIn the screenshot above, we can think of the model as one that took in these two green words (thou shalt) and returned a list of suggestions (“not” being the one with the highest probability):\\n\\n\\n\\n\\n\\nWe can think of the model as looking like this black box:\\n\\n\\n\\n\\n\\n\\nBut in practice, the model doesn’t output only one word. It actually outputs a probability score for all the words it knows (the model’s “vocabulary”, which can range from a few thousand to over a million words). The keyboard application then has to find the words with the highest scores, and present those to the user.\\n\\n\\n\\n\\n  The output of the neural language model is a probability score for all the words the model knows. We\\'re referring to the probability as a percentage here, but 40% would actually be represented as 0.4 in the output vector.\\n\\n\\nAfter being trained, early neural language models (Bengio 2003) would calculate a prediction in three steps:\\n\\n\\n\\n\\n\\n\\nThe first step is the most relevant for us as we discuss embeddings. One of the results of the training process was this matrix that contains an embedding for each word in our vocabulary. During prediction time, we just look up the embeddings of the input word, and use them to calculate the prediction:\\n\\n\\n\\n\\nLet’s now turn to the training process to learn more about how this embedding matrix was developed.\\nLanguage Model Training\\n\\n“A process cannot be understood by stopping it. Understanding must move with the flow of the process, must join it and flow with it.” ~Dune\\n\\nLanguage models have a huge advantage over most other machine learning models. That advantage is that we are able to train them on running text – which we have an abundance of. Think of all the books, articles, Wikipedia content, and other forms of text data we have lying around. Contrast this with a lot of other machine learning models which need hand-crafted features and specially-collected data.\\n\\n“You shall know a word by the company it keeps” J.R. Firth\\n\\nWords get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that\\n\\nWe get a lot of text data (say, all Wikipedia articles, for example). then\\nWe have a window (say, of three words) that we slide against all of that text.\\nThe sliding window generates training samples for our model\\n\\n\\n\\n\\n\\nAs this window slides against the text, we (virtually) generate a dataset that we use to train a model. To look exactly at how that’s done, let’s see how the sliding window processes this phrase:\\n\\n“Thou shalt not make a machine in the likeness of a human mind” ~Dune\\n\\nWhen we start, the window is on the first three words of the sentence:\\n\\n\\n\\n\\n\\n\\nWe take the first two words to be features, and the third word to be a label:\\n\\n\\n\\n\\n  We now have generated the first sample in the dataset we can later use to train a language model.\\n\\n\\nWe then slide our window to the next position and create a second sample:\\n\\n\\n\\n\\n  An the second example is now generated.\\n\\n\\nAnd pretty soon we have a larger dataset of which words tend to appear after different pairs of words:\\n\\n\\n\\n\\n\\n\\nIn practice, models tend to be trained while we’re sliding the window. But I find it clearer to logically separate the “dataset generation” phase from the training phase. Aside from neural-network-based approaches to language modeling, a technique called N-grams was commonly used to train language models (see: Chapter 3 of Speech and Language Processing). To see how this switch from N-grams to neural models reflects on real-world products, here’s a 2015 blog post from Swiftkey, my favorite Android keyboard, introducing their neural language model and comparing it with their previous N-gram model. I like this example because it shows you how the algorithmic properties of embeddings can be described in marketing speech.\\nLook both ways\\n\\n\"Paradox is a pointer telling you to look beyond it. If paradoxes bother you, that betrays your deep desire for absolutes. The relativist treats a paradox merely as interesting, perhaps amusing or even, dreadful thought, educational.\" ~God Emperor of Dune\\n\\nKnowing what you know from earlier in the post, fill in the blank:\\n\\n\\n\\n\\nThe context I gave you here is five words before the blank word (and an earlier mention of “bus”). I’m sure most people would guess the word bus goes into the blank. But what if I gave you one more piece of information – a word after the blank, would that change your answer?\\n\\n\\n\\n\\nThis completely changes what should go in the blank. the word red is now the most likely to go into the blank. What we learn from this is the words both before and after a specific word carry informational value. It turns out that accounting for both directions (words to the left and to the right of the word we’re guessing) leads to better word embeddings. Let’s see how we can adjust the way we’re training the model to account for this.\\nSkipgram\\n\\n  “Intelligence takes chance with limited data in an arena where mistakes are not only possible but also necessary.” ~Chapterhouse: Dune\\n\\nInstead of only looking two words before the target word, we can also look at two words after it.\\n\\n\\n\\n\\nIf we do this, the dataset we’re virtually building and training the model against would look like this:\\n\\n\\n\\n\\nThis is called a Continuous Bag of Words architecture and is described in one of the word2vec papers [pdf]. Another architecture that also tended to show great results does things a little differently.\\nInstead of guessing a word based on its context (the words before and after it), this other architecture tries to guess neighboring words using the current word. We can think of the window it slides against the training text as looking like this:\\n\\n\\n  \\n  The word in the green slot would be the input word, each pink box would be a possible output.\\n\\nThe pink boxes are in different shades because this sliding window actually creates four separate samples in our training dataset:\\n\\n\\n\\n\\n\\n\\nThis method is called the skipgram architecture. We can visualize the sliding window as doing the following:\\n\\n\\n\\n\\n\\n\\nThis would add these four samples to our training dataset:\\n\\n\\n\\n\\n\\nWe then slide our window to the next position:\\n\\n\\n\\n\\n\\n\\nWhich generates our next four examples:\\n\\n\\n\\n\\n\\nA couple of positions later, we have a lot more examples:\\n\\n\\n\\n\\n\\nRevisiting the training process\\n\\n  \"Muad\\'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It\\'s shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\" ~Dune\\n\\nNow that we have our skipgram training dataset that we extracted from existing running text, let’s glance at how we use it to train a basic neural language model that predicts the neighboring word.\\n\\n\\n\\n\\n\\nWe start with the first sample in our dataset. We grab the feature and feed to the untrained model asking it to predict an appropriate neighboring word.\\n\\n\\n\\n\\n\\nThe model conducts the three steps and outputs a prediction vector (with a probability assigned to each word in its vocabulary). Since the model is untrained, it’s prediction is sure to be wrong at this stage. But that’s okay. We know what word it should have guessed – the label/output cell in the row we’re currently using to train the model:\\n\\n\\n\\n  \\n  The \\'target vector\\' is one where the target word has the probability 1, and all other words have the probability 0.\\n\\n\\nHow far off was the model? We subtract the two vectors resulting in an error vector:\\n\\n\\n\\n\\n\\n\\nThis error vector can now be used to update the model so the next time, it’s a little more likely to guess thou when it gets not as input.\\n\\n\\n\\n\\n\\n\\nAnd that concludes the first step of the training. We proceed to do the same process with the next sample in our dataset, and then the next, until we’ve covered all the samples in the dataset. That concludes one epoch of training. We do it over again for a number of epochs, and then we’d have our trained model and we can extract the embedding matrix from it and use it for any other application.\\nWhile this extends our understanding of the process, it’s still not how word2vec is actually trained. We’re missing a couple of key ideas.\\nNegative Sampling\\n\\n“To attempt an understanding of Muad\\'Dib without understanding his mortal enemies, the Harkonnens, is to attempt seeing Truth without knowing Falsehood. It is the attempt to see the Light without knowing Darkness. It cannot be.” ~Dune\\n\\nRecall the three steps of how this neural language model calculates its prediction:\\n\\n\\n\\n\\n\\n\\nThe third step is very expensive from a computational point of view – especially knowing that we will do it once for every training sample in our dataset (easily tens of millions of times). We need to do something to improve performance.\\nOne way is to split our target into two steps:\\n\\nGenerate high-quality word embeddings (Don’t worry about next-word prediction).\\nUse these high-quality embeddings to train a language model (to do next-word prediction).\\n\\nWe’ll focus on step 1. in this post as we’re focusing on embeddings. To generate high-quality embeddings using a high-performance model, we can switch the model’s task from predicting a neighboring word:\\n\\n\\n\\n\\nAnd switch it to a model that takes the input and output word, and outputs a score indicating if they’re neighbors or not (0 for “not neighbors”, 1 for “neighbors”).\\n\\n\\n\\n\\nThis simple switch changes the model we need from a neural network, to a logistic regression model – thus it becomes much simpler and much faster to calculate.\\nThis switch requires we switch the structure of our dataset – the label is now a new column with values 0 or 1. They will be all 1 since all the words we added are neighbors.\\n\\n\\n\\n\\n\\n\\nThis can now be computed at blazing speed – processing millions of examples in minutes. But there’s one loophole we need to close. If all of our examples are positive (target: 1), we open ourself to the possibility of a smartass model that always returns 1 – achieving 100% accuracy, but learning nothing and generating garbage embeddings.\\n\\n\\n\\n\\nTo address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors.  Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.\\n\\n\\n\\n  \\n  For each sample in our dataset, we add negative examples. Those have the same input word, and a 0 label.\\n\\nBut what do we fill in as output words? We randomly sample words from our vocabulary\\n\\n\\n\\n\\n\\nThis idea is inspired by Noise-contrastive estimation [pdf]. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency.\\nSkipgram with Negative Sampling (SGNS)\\nWe have now covered two of the central ideas in word2vec: as a pair, they’re called skipgram with negative sampling.\\n\\n\\n\\n\\nWord2vec Training Process\\n\\n\"The machine cannot anticipate every problem of importance to humans. It is the difference between serial bits and an unbroken continuum. We have the one; machines are confined to the other.\" ~God Emperor of Dune\\n\\nNow that we’ve established the two central ideas of skipgram and negative sampling, we can proceed to look closer at the actual word2vec training process.\\nBefore the training process starts, we pre-process the text we’re training the model against. In this step, we determine the size of our vocabulary (we’ll call this vocab_size, think of it as, say, 10,000) and which words belong to it.\\nAt the start of the training phase, we create two matrices – an Embedding matrix and a Context matrix. These two matrices have an embedding for each word in our vocabulary (So vocab_size is one of their dimensions). The second dimension is how long we want each embedding to be (embedding_size – 300 is a common value, but we’ve looked at an example of 50 earlier in this post).\\n\\n\\n\\n\\nAt the start of the training process, we initialize these matrices with random values. Then we start the training process. In each training step, we take one positive example and its associated negative examples. Let’s take our first group:\\n\\n\\n\\n\\nNow we have four words: the input word not and output/context words: thou (the actual neighbor), aaron, and taco (the negative examples). We proceed to look up their embeddings – for the input word, we look in the Embedding matrix. For the context words, we look in the Context matrix (even though both matrices have an embedding for every word in our vocabulary).\\n\\n\\n\\n\\nThen, we take the dot product of the input embedding with each of the context embeddings. In each case, that would result in a number, that number indicates the similarity of the input and context embeddings\\n\\n\\n\\n\\nNow we need a way to turn these scores into something that looks like probabilities – we need them to all be positive and have values between zero and one. This is a great task for sigmoid, the logistic operation.\\n\\n\\n\\n\\nAnd we can now treat the output of the sigmoid operations as the model’s output for these examples. You can see that taco has the highest score and aaron still has the lowest score both before and after the sigmoid operations.\\nNow that the untrained model has made a prediction, and seeing as though we have an actual target label to compare against, let’s calculate how much error is in the model’s prediction. To do that, we just subtract the sigmoid scores from the target labels.\\n\\n\\n\\nerror = target - sigmoid_scores\\n\\n\\nHere comes the “learning” part of “machine learning”. We can now use this error score to adjust the embeddings of not, thou, aaron, and taco so that the next time we make this calculation, the result would be closer to the target scores.\\n\\n\\n\\n\\nThis concludes the training step. We emerge from it with slightly better embeddings for the words involved in this step (not, thou, aaron, and taco). We now proceed to our next step (the next positive sample and its associated negative samples) and do the same process again.\\n\\n\\n\\n\\nThe embeddings continue to be improved while we cycle through our entire dataset for a number of times. We can then stop the training process, discard the Context matrix, and use the Embeddings matrix as our pre-trained embeddings for the next task.\\nWindow Size and Number of Negative Samples\\nTwo key hyperparameters in the word2vec training process are the window size and the number of negative samples.\\n\\n\\n\\n\\nDifferent tasks are served better by different window sizes. One heuristic is that smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we’re only looking at their surrounding words – e.g. good and bad often appear in similar contexts). Larger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words. In practice, you’ll often have to provide annotations that guide the embedding process leading to a useful similarity sense for your task. The Gensim default window size is 5 (five words before and five words after the input word, in addition to the input word itself).\\n\\n\\n\\n\\nThe number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples.\\nConclusion\\n\\n“If it falls outside your yardsticks, then you are engaged with intelligence, not with automation”  ~God Emperor of Dune\\n\\nI hope that you now have a sense for word embeddings and the word2vec algorithm. I also hope that now when you read a paper mentioning “skip gram with negative sampling” (SGNS) (like the recommendation system papers at the top), that you have a better sense for these concepts. As always, all feedback is appreciated @JayAlammar.\\nReferences & Further Readings\\n\\nDistributed Representations of Words and Phrases and their Compositionality [pdf]\\nEfficient Estimation of Word Representations in Vector Space [pdf]\\nA Neural Probabilistic Language Model [pdf]\\nSpeech and Language Processing by Dan Jurafsky and James H. Martin is a leading resource for NLP. Word2vec is tackled in Chapter 6.\\nNeural Network Methods in Natural Language Processing by Yoav Goldberg is a great read for neural NLP topics.\\nChris McCormick has written some great blog posts about Word2vec. He also just released The Inner Workings of word2vec, an E-book focused on the internals of word2vec.\\nWant to read the code? Here are two options:\\n    \\nGensim’s python implementation of word2vec\\nMikolov’s original implementation in C – better yet, this version with detailed comments from Chris McCormick.\\n\\n\\nEvaluating distributional models of compositional semantics\\nOn word embeddings, part 2\\nDune\\n\\n\\n\\n\\n\\n\\n\\n    Written on March 27, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)\\n\\nDiscussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n\\n\\n(ULM-FiT has nothing to do with Cookie Monster. But I couldn’t think of anything else..)\\nOne of the latest milestones in this development is the release of BERT, an event described as marking the beginning of a new era in NLP. BERT is a model that broke several records for how well models can handle language-based tasks. Soon after the release of the paper describing the model, the team also open-sourced the code of the model, and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component – saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch.\\n\\n\\n\\n  The two steps of how BERT is developed. You can download the model pre-trained in step 1 (trained on un-annotated data), and only worry about fine-tuning it for step 2. [Source for book icon].\\n\\nBERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).\\nThere are a number of concepts one needs to be aware of to properly wrap one’s head around what BERT is. So let’s start by looking at ways you can use BERT before looking at the concepts involved in the model itself.\\nExample: Sentence Classification\\nThe most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:\\n\\nTo train such a model, you mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning, and has roots in Semi-supervised Sequence Learning and ULMFiT.\\nFor people not versed in the topic, since we’re talking about classifiers, then we are in the supervised-learning domain of machine learning. Which would mean we need a labeled dataset to train such a model. For this spam classifier example, the labeled dataset would be a list of email messages and a label (“spam” or “not spam” for each message).\\n\\n\\n\\nOther examples for such a use-case include:\\n\\nSentiment analysis\\n\\nInput: Movie/Product review. Output: is the review positive or negative?\\nExample dataset: SST\\n\\n\\nFact-checking\\n\\nInput: sentence. Output: “Claim” or “Not Claim”\\nMore ambitious/futuristic example:\\n        \\nInput: Claim sentence. Output: “True” or “False”\\n\\n\\nFull Fact is an organization building automatic fact-checking tools for the benefit of the public. Part of their pipeline is a classifier that reads news articles and detects claims (classifies text as either “claim” or “not claim”) which can later be fact-checked (by humans now, with ML later, hopefully).\\nVideo: Sentence embeddings for automated factchecking - Lev Konstantinovskiy.\\n\\n\\n\\nModel Architecture\\nNow that you have an example use-case in your head for how BERT can be used, let’s take a closer look at how it works.\\n\\nThe paper presents two model sizes for BERT:\\n\\nBERT BASE – Comparable in size to the OpenAI Transformer in order to compare performance\\nBERT LARGE – A ridiculously huge model which achieved the state of the art results reported in the paper\\n\\nBERT is basically a trained Transformer Encoder stack. This is a good time to direct you to read my earlier post The Illustrated Transformer which explains the Transformer model – a foundational concept for BERT and the concepts we’ll discuss next.\\n\\nBoth BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).\\nModel Inputs\\n\\nThe first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.\\nJust like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.\\n\\nIn terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge.\\nModel Outputs\\nEach position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we’ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to).\\n\\nThat vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.\\n\\nIf you have more labels (for example if you’re an email service that tags emails with “spam”, “not spam”, “social”, and “promotion”), you just tweak the classifier network to have more output neurons that then pass through softmax.\\nParallels with Convolutional Nets\\nFor those with a background in computer vision, this vector hand-off should be reminiscent of what happens between the convolution part of a network like VGGNet and the fully-connected classification portion at the end of the network.\\n\\nA New Age of Embedding\\nThese new developments carry with them a new shift in how words are encoded. Up until now, word-embeddings have been a major force in how leading NLP models deal with language. Methods like Word2Vec and Glove have been widely used for such tasks. Let’s recap how those are used before pointing to what has now changed.\\nWord Embedding Recap\\nFor words to be processed by machine learning models, they need some form of numeric representation that models can use in their calculation. Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).\\nThe field quickly realized it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset.  So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe. This is an example of the GloVe embedding of the word “stick” (with an embedding vector size of 200)\\n\\n\\n\\n  The GloVe word embedding of the word \"stick\" - a vector of 200 floats (rounded to two decimals). It goes on for two hundred values.\\n\\nSince these are large and full of numbers, I use the following basic shape in the figures in my posts to show vectors:\\n\\n\\n\\n\\nELMo: Context Matters\\nIf we’re using this GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was. “Wait a minute” said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. And so, contextualized word-embeddings were born.\\n\\n\\n\\n  Contextualized word-embeddings can give words different embeddings based on the meaning they carry in the context of the sentence. Also, RIP Robin Williams\\n\\nInstead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.\\n\\n\\n\\n\\nELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.\\nWhat’s ELMo’s secret?\\nELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.\\n\\n\\n\\n  A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.\\n\\nWe can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding process after this pre-training is done.\\nELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.\\n\\n\\n\\nGreat slides on ELMo\\n\\nELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).\\n\\n\\n\\nULM-FiT: Nailing down Transfer Learning in NLP\\nULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training – more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks.\\nNLP finally had a way to do transfer learning probably as well as Computer Vision could.\\nThe Transformer: Going beyond LSTMs\\nThe release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependancies better than LSTMs.\\nThe Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component).\\nOpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling\\nIt turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.\\n\\n\\n\\n  The OpenAI Transformer is made up of the decoder stack from the Transformer\\n\\nThe model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).\\nWith this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.\\n\\n\\n\\n  The OpenAI Transformer is now ready to be trained to predict the next word on a dataset made up of 7,000 books.\\n\\nTransfer Learning to Downstream Tasks\\nNow that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):\\n\\n\\n\\n\\n  How to use a pre-trained OpenAI transformer to do sentence clasification\\n\\nThe OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.\\n\\n\\n\\n\\nIsn’t that clever?\\nBERT: From Decoders to Encoders\\nThe openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?\\n“Hold my beer”, said R-rated BERT.\\nMasked Language Model\\n“We’ll use transformer encoders”, said BERT.\\n“This is madness”, replied Ernie, “Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.”\\n“We’ll use masks”, said BERT confidently.\\n\\n\\n\\n  BERT\\'s clever language modeling task masks 15% of words in the input and asks the model to predict the missing word.\\n\\nFinding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a “masked language model” concept from earlier literature (where it’s called a Cloze task).\\nBeyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.\\nTwo-sentence Tasks\\nIf you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).\\nTo make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?\\n\\n\\n\\n  The second task BERT is pre-trained on is a two-sentence classification task. The tokenization is oversimplified in this graphic as BERT actually uses WordPieces as tokens rather than words --- so some words are broken down into smaller chunks.\\n\\nTask specific-Models\\nThe BERT paper shows a number of ways to use BERT for different tasks.\\n\\n\\n\\n\\nBERT for feature extraction\\nThe fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.\\n\\n\\n\\n\\nWhich vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):\\n\\n\\n\\n\\nTake BERT out for a spin\\nThe best way to try out BERT is through the BERT FineTuning with Cloud TPUs notebook hosted on Google Colab. If you’ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well.\\nThe next step would be to look at the code in the BERT repo:\\n\\nThe model is constructed in modeling.py (class BertModel) and is pretty much identical to a vanilla Transformer encoder.\\n\\nrun_classifier.py is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the create_model() method in that file.\\n\\n\\nSeveral pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia.\\n\\nBERT doesn’t look at words as tokens. Rather, it looks at WordPieces. tokenization.py is the tokenizer that would turns your words into wordPieces appropriate for BERT.\\n\\nYou can also check out the PyTorch implementation of BERT. The AllenNLP library uses this implementation to allow using BERT embeddings with any model.\\nAcknowledgements\\nThanks to Jacob Devlin, Matt Gardner, Kenton Lee,  Mark Neumann, and Matthew Peters for providing feedback on earlier drafts of this post.\\n\\n\\n    Written on December  3, 2018\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Transformer\\n\\nDiscussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n\\n\\n\\nUpdate: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they\\'ve evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n\\n\\nPopping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.\\n\\n\\n\\nThe encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.\\n\\n\\n\\nThe encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:\\n\\n\\n\\nThe encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.\\nThe outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.\\nThe decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).\\n\\n\\n\\nBringing The Tensors Into The Picture\\nNow that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.\\nAs is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.\\n\\n\\n\\n\\n  Each word is embedded into a vector of size 512. We\\'ll represent those vectors with these simple boxes.\\n\\nThe embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.\\nAfter embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.\\n\\n\\n\\n\\nHere we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\\nNext, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.\\nNow We’re Encoding!\\nAs we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.\\n\\n\\n\\n  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.\\n\\nSelf-Attention at a High Level\\nDon’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.\\nSay the following sentence is an input sentence we want to translate:\\n”The animal didn\\'t cross the street because it was too tired”\\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\\nWhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\\nIf you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\\n\\n\\n\\n  As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".\\n\\nBe sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\\nSelf-Attention in Detail\\nLet’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.\\nThe first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\\nNotice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.\\n\\n\\n\\n\\n  Multiplying x1 by the WQ weight matrix produces q1, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\\n\\n\\n\\nWhat are the “query”, “key”, and “value” vectors?\\n\\n\\nThey’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.\\nThe second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\\n\\n\\n\\n\\n\\n\\nThe third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\\n\\n\\n\\n\\n\\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\\n\\nThe fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\\n\\n\\n\\n\\n\\nThat concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.\\nMatrix Calculation of Self-Attention\\nThe first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).\\n\\n\\n\\n  Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)\\n\\n\\nFinally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.\\n\\n\\n\\n  The self-attention calculation in matrix form\\n\\n\\n\\nThe Beast With Many Heads\\nThe paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:\\n\\n\\nIt expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.\\n\\n\\nIt gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\\n\\n\\n\\n\\n\\n   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.\\n \\n\\nIf we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices\\n\\n\\n\\n\\n\\nThis leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\\nHow do we do that? We concat the matrices then multiply them by an additional weights matrix WO.\\n\\n\\n\\n\\nThat’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place\\n\\n\\n\\n\\n\\n\\nNow that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:\\n\\n\\n\\n  As we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model\\'s representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\".\\n\\n\\nIf we add all the attention heads to the picture, however, things can be harder to interpret:\\n\\n\\n\\n\\nRepresenting The Order of The Sequence Using Positional Encoding\\nOne thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.\\nTo address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\\n\\n\\n\\n\\n  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.\\n\\n\\nIf we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:\\n\\n\\n\\n  A real example of positional encoding with a toy embedding size of 4\\n\\n\\nWhat might this pattern look like?\\nIn the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.\\n\\n\\n\\n  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That\\'s because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They\\'re then concatenated to form each of the positional encoding vectors.\\n\\nThe formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d(). This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).\\nJuly 2020 Update: \\nThe positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it:\\n\\n\\n\\n\\nThe Residuals\\nOne detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.\\n\\n\\n\\n\\nIf we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:\\n\\n\\n\\n\\nThis goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:\\n\\n\\n\\n\\nThe Decoder Side\\nNow that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.\\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\\n\\n\\n\\n  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).\\n\\nThe following steps repeat the process until a special  symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\\n\\n\\n\\n\\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\\nThe Final Linear and Softmax Layer\\nThe decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\\n\\n\\n\\n\\n  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.\\n\\n\\nRecap Of Training\\nNow that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\\nDuring training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.\\nTo visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).\\n\\n\\n\\n   The output vocabulary of our model is created in the preprocessing phase before we even begin training.\\n \\nOnce we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:\\n\\n\\n\\n  Example: one-hot encoding of our output vocabulary\\n\\nFollowing this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\\nThe Loss Function\\nSay we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.\\nWhat this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.\\n\\n\\n\\n  Since the model\\'s parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model\\'s weights using backpropagation to make the output closer to the desired output.\\n\\n\\nHow do you compare two probability distributions? We simply subtract one from the other. For more details, look at  cross-entropy and Kullback–Leibler divergence.\\nBut note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:\\n\\nEach probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)\\nThe first probability distribution has the highest probability at the cell associated with the word “i”\\nThe second probability distribution has the highest probability at the cell associated with the word “am”\\nAnd so on, until the fifth output distribution indicates ‘<end of sentence>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.\\n\\n\\n\\n\\n   The targeted probability distributions we\\'ll train our model against in the training example for one sample sentence.\\n \\n\\nAfter training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\\n\\n\\n\\n    Hopefully upon training, the model would output the right translation we expect. Of course it\\'s no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it\\'s unlikely to be the output of that time step -- that\\'s a very useful property of softmax which helps the training process.\\n\\nNow, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.\\nGo Forth And Transform\\nI hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:\\n\\nRead the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.\\nWatch Łukasz Kaiser’s talk walking through the model and its details\\nPlay with the Jupyter Notebook provided as part of the Tensor2Tensor repo\\nExplore the Tensor2Tensor repo.\\n\\nFollow-up works:\\n\\nDepthwise Separable Convolutions for Neural Machine Translation\\nOne Model To Learn Them All\\nDiscrete Autoencoders for Sequence Models\\nGenerating Wikipedia by Summarizing Long Sequences\\nImage Transformer\\nTraining Tips for the Transformer Model\\nSelf-Attention with Relative Position Representations\\nFast Decoding in Sequence Models using Discrete Latent Variables\\nAdafactor: Adaptive Learning Rates with Sublinear Memory Cost\\n\\nAcknowledgements\\nThanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.\\nPlease hit me up on Twitter for any corrections or feedback.\\n\\n\\n    Written on June 27, 2018\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "87cd4311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nA Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nA Visual Guide to Using BERT for the First Time\\n\\nTranslations: Chinese, Korean, Russian\\n\\n\\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\nDataset: SST2\\nThe dataset we will use in this example is SST2, which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):\\n\\n\\n\\n    sentence\\n    \\n\\n    label\\n    \\n\\n\\n\\n      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\\n    \\n\\n      1\\n    \\n\\n\\n\\n      apparently reassembled from the cutting room floor of any given daytime soap\\n    \\n\\n      0\\n    \\n\\n\\n\\n      they presume their audience won\\'t sit still for a sociology lesson\\n    \\n\\n      0\\n    \\n\\n\\n\\n      this is a visually stunning rumination on love , memory , history and the war between art and commerce\\n    \\n\\n      1\\n    \\n\\n\\n\\n      jonathan parker \\'s bartleby should have been the be all end all of the modern office anomie films\\n    \\n\\n      1\\n    \\n\\n\\nModels: Sentence Sentiment Classification\\nOur goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\\n\\n\\n\\n\\nUnder the hood, the model is actually made up of two model.\\n\\nDistilBERT  processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\\nThe next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the  sentence as either positive or negative (1 or 0, respectively).\\n\\nThe data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\\n\\n\\n\\n\\nIf you’ve read my previous post, Illustrated BERT, this vector is the result of the first position (which receives the [CLS] token as input).\\nModel Training\\nWhile we’ll be using two models, we will only train the logistic regression model. For DistillBERT, we’ll use a model that’s already pre-trained and has a grasp on the English language. This model, however is neither trained not fine-tuned to do sentence classification. We get some sentence classification capability, however, from the general objectives BERT is trained on. This is especially the case with BERT’s output for the first position (associated with the [CLS] token). I believe that’s due to BERT’s second training object – Next sentence classification. That objective seemingly trains the model to encapsulate a sentence-wide sense to the output at the first position. The transformers library provides us with an implementation of DistilBERT as well as pretrained versions of the model.\\n\\n\\n\\n\\nTutorial Overview\\nSo here’s the game plan with this tutorial. We will first use the trained distilBERT to generate sentence embeddings for 2,000 sentences.\\n\\n\\n\\n\\nWe will not touch distilBERT after this step. It’s all Scikit Learn from here. We do the usual train/test split on this dataset:\\n\\n\\n\\n  Train/test split for the output of distilBert (model #1) creates the dataset we\\'ll train and evaluate logistic regression on (model #2). Note that in reality, sklearn\\'s train/test split shuffles the examples before making the split, it doesn\\'t just take the first 75% of examples as they appear in the dataset.\\n\\nThen we train the logistic regression model on the training set:\\n\\n\\n\\n\\nHow a single prediction is calculated\\nBefore we dig into the code and explain how to train the model, let’s look at how a trained model calculates its prediction.\\nLet’s try to classify the sentence “a visually stunning rumination on love”. The first step is to use the BERT tokenizer to first split the word into tokens. Then, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence).\\n\\n\\n\\n\\nThe third step the tokenizer does is to replace each token with its id from the embedding table which is a component we get with the trained model. Read The Illustrated Word2vec for a background on word embeddings.\\n\\n\\n\\n\\nNote that the tokenizer does all these steps in a single line of code:\\ntokenizer.encode(\"a visually stunning rumination on love\", add_special_tokens=True)\\n\\nOur input sentence is now the proper shape to be passed to DistilBERT.\\nIf you’ve read Illustrated BERT, this step can also be visualized in this manner:\\n\\n\\n\\n\\nFlowing Through DistilBERT\\nPassing the input vector through DistilBERT works just like BERT. The output would be a vector for each input token. each vector is made up of 768 numbers (floats).\\n\\n\\n\\n\\nBecause this is a sentence classification task, we ignore all except the first vector (the one associated with the [CLS] token). The one vector we pass as the input to the logistic regression model.\\n\\n\\n\\n\\nFrom here, it’s the logistic regression model’s job to classify this vector based on what it learned from its training phase. We can think of a prediction calculation as looking like this:\\n\\n\\n\\n\\nThe training is what we’ll discuss in the next section, along with the code of the entire process.\\nThe Code\\nIn this section we’ll highlight the code to train this sentence classification model. A notebook containing all this code is available on colab and github.\\nLet’s start by importing the tools of the trade\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport transformers as ppb # pytorch transformers\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\n\\nThe dataset is available as a file on github, so we just import it directly into a pandas dataframe\\ndf = pd.read_csv(\\'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv\\', delimiter=\\'\\\\t\\', header=None)\\n\\nWe can use df.head() to look at the first five rows of the dataframe to see how the data looks.\\ndf.head()\\n\\nWhich outputs:\\n\\n\\n\\n\\nImporting pre-trained DistilBERT model and tokenizer\\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, \\'distilbert-base-uncased\\')\\n\\n## Want BERT instead of distilBERT? Uncomment the following line:\\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, \\'bert-base-uncased\\')\\n\\n# Load pretrained model/tokenizer\\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\\nmodel = model_class.from_pretrained(pretrained_weights)\\n\\nWe can now tokenize the dataset. Note that we’re going to do things a little differently here from the example above. The example above tokenized and processed only one sentence. Here, we’ll tokenize and process all sentences together as a batch (the notebook processes a smaller group of examples just for resource considerations, let’s say 2000 examples).\\nTokenization\\ntokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\\n\\nThis turns every sentence into the list of ids.\\n\\n\\n\\n\\nThe dataset is currently a list (or pandas Series/DataFrame) of lists. Before DistilBERT can process this as input, we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0. You can refer to the notebook for the padding step, it’s basic python string and array manipulation.\\nAfter the padding, we have a matrix/tensor that is ready to be passed to BERT:\\n\\n\\n\\n\\nProcessing with DistilBERT\\nWe now create an input tensor out of the padded token matrix, and send that to DistilBERT\\ninput_ids = torch.tensor(np.array(padded))\\n\\nwith torch.no_grad():\\n    last_hidden_states = model(input_ids)\\n\\nAfter running this step, last_hidden_states holds the outputs of DistilBERT. It is a tuple with the shape (number of examples, max number of tokens in the sequence, number of hidden units in the DistilBERT model). In our case, this will be 2000 (since we only limited ourselves to 2000 examples), 66 (which is the number of tokens in the longest sequence from the 2000 examples), 768 (the number of hidden units in the DistilBERT model).\\n\\n\\n\\n\\nUnpacking the BERT output tensor\\nLet’s unpack this 3-d output tensor. We can first start by examining its dimensions:\\n\\n\\n\\n\\nRecapping a sentence’s journey\\nEach row is associated with a sentence from our dataset. To recap the processing path of the first sentence, we can think of it as looking like this:\\n\\n\\n\\n\\nSlicing the important part\\nFor sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.\\n\\n\\n\\n\\nThis is how we slice that 3d tensor to get the 2d tensor we’re interested in:\\n # Slice the output for the first position for all the sequences, take all hidden unit outputs\\nfeatures = last_hidden_states[0][:,0,:].numpy()\\n\\nAnd now features is a 2d numpy array containing the sentence embeddings of all the sentences in our dataset.\\n\\n\\n\\n  The tensor we sliced from BERT\\'s output\\n\\nDataset for Logistic Regression\\nNow that we have the output of BERT, we have assembled the dataset we need to train our logistic regression model. The 768 columns are the features, and the labels we just get from our initial dataset.\\n\\n\\n\\n  The labeled dataset we use to train the Logistic Regression. The features are the output vectors of BERT for the [CLS] token (position #0) that we sliced in the previous figure. Each row corresponds to a sentence in our dataset, each column corresponds to the output of a hidden unit from the feed-forward neural network at the top transformer block of the Bert/DistilBERT model.\\n\\nAfter doing the traditional train/test split of machine learning, we can declare our Logistic Regression model and train it against the dataset.\\nlabels = df[1]\\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels)\\n\\n\\nWhich splits the dataset into training/testing sets:\\n\\n\\n\\n\\nNext, we train the Logistic Regression model on the training set.\\nlr_clf = LogisticRegression()\\nlr_clf.fit(train_features, train_labels)\\n\\nNow that the model is trained, we can score it against the test set:\\nlr_clf.score(test_features, test_labels)\\n\\nWhich shows the model achieves around 81% accuracy.\\nScore Benchmarks\\nFor reference, the highest accuracy score for this dataset is currently 96.8. DistilBERT can be trained to improve its score on this task – a process called fine-tuning which updates BERT’s weights to make it achieve a better performance in the sentence classification (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of 90.7. The full size BERT model achieves 94.9.\\nThe Notebook\\nDive right into the notebook or run it on colab.\\nAnd that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at fine-tuning. You can also go back and switch from distilBERT to BERT and see how that works.\\nThanks to Clément Delangue, Victor Sanh, and the Huggingface team for providing feedback to earlier versions of this tutorial.\\n\\n\\n    Written on November 26, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated GPT-2 (Visualizing Transformer Language Models)\\n\\nDiscussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n\\n\\nContents\\n\\n\\nPart 1: GPT2 And Language Modeling\\n\\nWhat is a Language Model\\nTransformers for Language Modeling\\nOne Difference From BERT\\nThe Evolution of The Transformer Block\\nCrash Course in Brain Surgery: Looking Inside GPT-2\\nA Deeper Look Inside\\nEnd of part #1: The GPT-2, Ladies and Gentlemen\\n\\n\\nPart 2: The Illustrated Self-Attention\\n\\nSelf-Attention (without masking)\\n1- Create Query, Key, and Value Vectors\\n2- Score\\n3- Sum\\nThe Illustrated Masked Self-Attention\\nGPT-2 Masked Self-Attention\\nBeyond Language modeling\\nYou’ve Made it!\\n\\n\\nPart 3: Beyond Language Modeling\\n\\nMachine Translation\\nSummarization\\nTransfer Learning\\nMusic Generation\\n\\n\\n\\n\\nPart #1: GPT2 And Language Modeling #\\nSo what exactly is a language model?\\nWhat is a Language Model\\nIn The Illustrated Word2vec, we’ve looked at what a language model is – basically a machine learning model that is able to look at part of a sentence and predict the next word. The most famous language models are smartphone keyboards that suggest the next word based on what you’ve currently typed.\\n\\n\\n\\n\\nIn this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest variant of the trained GPT-2, takes up 500MBs of storage to store all of its parameters. The largest GPT-2 variant is 13 times the size so it could take up more than 6.5 GBs of storage space.\\n\\n\\n\\n\\nOne great way to experiment with GPT-2 is using the AllenAI GPT-2 Explorer. It uses GPT-2 to display ten possible predictions for the next word (alongside their probability score). You can select a word then see the next list of predictions to continue writing the passage.\\nTransformers for Language Modeling\\nAs we’ve seen in The Illustrated Transformer, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. That architecture was appropriate because the model tackled machine translation  – a problem where encoder-decoder architectures have been successful in the past.\\n\\n\\n\\n\\nA lot of the subsequent research work saw the architecture shed either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible, feeding them massive amounts of training text, and throwing vast amounts of compute at them (hundreds of thousands of dollars to train some of these language models, likely millions in the case of AlphaStar).\\n\\n\\n\\n\\nHow high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:\\n\\n\\n\\n\\nOne Difference From BERT\\n\\nFirst Law of Robotics\\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\\n\\nThe GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let’s for example prompt a well-trained GPT-2 to recite the first law of robotics:\\n\\n\\n\\n\\nThe way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “auto-regression”. This is one of the ideas that made RNNs unreasonably effective.\\n\\n\\n\\n\\nThe GPT2, and some later models like TransformerXL and XLNet are auto-regressive in nature. BERT is not. That is a trade off. In losing auto-regression, BERT gained the ability to incorporate the context on both sides of a word to gain better results. XLNet brings back autoregression while finding an alternative way to incorporate the context on both sides.\\nThe Evolution of the Transformer Block\\nThe initial transformer paper introduced two types of transformer blocks:\\nThe Encoder Block\\nFirst is the encoder block:\\n\\n\\n\\n  An encoder block from the original transformer paper can take inputs up until a certain max sequence length (e.g. 512 tokens). It\\'s okay if an input sequence is shorter than this limit, we can just pad the rest of the sequence.\\n\\nThe Decoder Block\\nSecond, there’s the decoder block which has a small architectural variation from the encoder block – a layer to allow it to pay attention to specific segments from the encoder:\\n\\n\\n\\n\\nOne key difference in the self-attention layer here, is that it masks future tokens – not by changing the word to [mask] like BERT, but by interfering in the self-attention calculation blocking information from tokens that are to the right of the position being calculated.\\nIf, for example, we’re to highlight the path of position #4, we can see that it is only allowed to attend to the present and previous tokens:\\n\\n\\n\\n\\nIt’s important that the distinction between self-attention (what BERT uses) and masked self-attention (what GPT-2 uses) is clear. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening:\\n\\n\\n\\n\\nThe Decoder-Only Block\\nSubsequent to the original paper, Generating Wikipedia by Summarizing Long Sequences proposed another arrangement of the transformer block that is capable of doing language modeling. This model threw away the Transformer encoder. For that reason, let’s call the model the “Transformer-Decoder”. This early transformer-based language model was made up of a stack of six transformer decoder blocks:\\n\\n\\n\\n  The decoder blocks are identical. I have expanded the first one so you can see its self-attention layer is the masked variant. Notice that the model now can address up to 4,000 tokens in a certain segment -- a massive upgrade from the 512 in the original transformer.\\n\\nThese blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in Character-Level Language Modeling with Deeper Self-Attention to create a language model that predicts one letter/character at a time.\\nThe OpenAI GPT-2 model uses these decoder-only blocks.\\nCrash Course in Brain Surgery: Looking Inside GPT-2\\n\\nLook inside and you will see,\\nThe words are cutting deep inside my brain.\\nThunder burning, quickly burning,\\nKnife of words is driving me insane, insane yeah.\\n~Budgie\\n\\nLet’s lay a trained GPT-2 on our surgery table and look at how it works.\\n\\n\\n\\n  The GPT-2 can process 1024 tokens. Each token flows through all the decoder blocks along its own path.\\n\\nThe simplest way to run a trained GPT-2 is to allow it to ramble on its own (which is technically called generating unconditional samples) – alternatively, we can give it a prompt to have it speak about a certain topic (a.k.a generating interactive conditional samples). In the rambling case, we can simply hand it the start token and have it start generating words (the trained model uses <|endoftext|> as its start token. Let’s call it <s> instead).\\n\\n\\n\\n\\nThe model only has one input token, so that path would be the only active one. The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model’s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2). In this case we selected the token with the highest probability, ‘the’. But we can certainly mix things up – you know how if you keep clicking the suggested word in your keyboard app, it sometimes can stuck in repetitive loops where the only way out is if you click the second or third suggested word. The same can happen here. GPT-2 has a parameter called top-k that we can use to have the model consider sampling words other than the top word (which is the case when top-k = 1).\\nIn the next step, we add the output from the first step to our input sequence, and have the model make its next prediction:\\n\\n\\n\\n\\nNotice that the second path is the only one that’s active in this calculation. Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we’ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token.\\nA Deeper Look Inside\\nInput Encoding\\nLet’s look at more details to get to know the model more intimately. Let’s start from the input. As in other NLP models we’ve discussed before, the model looks up the embedding of the input word in its embedding matrix – one of the components we get as part of a trained model.\\n\\n\\n\\n  Each row is a word embedding: a list of numbers representing a word and capturing some of its meaning. The size of that list is different in different GPT2 model sizes. The smallest model uses an embedding size of 768 per word/token.\\n\\nSo in the beginning, we look up the embedding of the start token <s> in the embedding matrix. Before handing that to the first block in the model, we need to incorporate positional encoding – a signal that indicates the order of the words in the sequence to the transformer blocks. Part of the trained model is a matrix that contains a positional encoding vector for each of the 1024 positions in the input.\\n\\n\\n\\n\\nWith this, we’ve covered how input words are processed before being handed to the first transformer block. We also know two of the weight matrices that constitute the trained GPT-2.\\n\\n\\n\\n  Sending a word to the first transformer block means looking up its embedding and adding up the positional encoding vector for position #1.\\n\\nA journey up the Stack\\nThe first block can now process the token by first passing it through the self-attention process, then passing it through its neural network layer. Once the first transformer block processes the token, it sends its resulting vector up the stack to be processed by the next block. The process is identical in each block, but each block has its own weights in both self-attention and the neural network sublayers.\\n\\n\\n\\n\\nSelf-Attention Recap\\nLanguage heavily relies on context. For example, look at the second law:\\n\\nSecond Law of Robotics\\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\\n\\n\\nI have highlighted three places in the sentence where the words are referring to other words. There is no way to understand or process these words without incorporating the context they are referring to. When a model processes this sentence, it has to be able to know that:\\n\\nit refers to the robot\\nsuch orders refers to the earlier part of the law, namely “the orders given it by human beings”\\nThe First Law refers to the entire First Law\\n\\nThis is what self-attention does. It bakes in the model’s understanding of relevant and associated words that explain the context of a certain word before processing that word (passing it through a neural network). It does that by assigning scores to how relevant each word in the segment is, and adding up their vector representation.\\nAs an example, this self-attention layer in the top block is paying attention to “a robot” when it processes the word “it”. The vector it will pass to its neural network is a sum of the vectors for each of the three words multiplied by their scores.\\n\\n\\n\\n\\nSelf-Attention Process\\nSelf-attention is processed along the path of each token in the segment. The significant components are three vectors:\\n\\nQuery: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.\\nKey: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.\\nValue: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word.\\n\\n\\n\\n\\n\\nA crude analogy is to think of it like searching through a filing cabinet. The query is like a sticky note with the topic you’re researching. The keys are like the labels of the folders inside the cabinet. When you match the tag with a sticky note, we take out the contents of that folder, these contents are the value vector. Except you’re not only looking for one value, but a blend of values from a blend of folders.\\nMultiplying the query vector by each key vector produces a score for each folder (technically: dot product followed by softmax).\\n\\n\\n\\n\\nWe multiply each value by its score and sum up – resulting in our self-attention outcome.\\n\\n\\n\\n\\nThis weighted blend of value vectors results in a vector that paid 50% of its “attention” to the word robot, 30% to the word a, and 19% to the word it. Later in the post, we’ll got deeper into self-attention. But first, let’s continue our journey up the stack towards the output of the model.\\nModel Output\\nWhen the top block in the model produces its output vector (the result of its own self-attention followed by its own neural network), the model multiplies that vector by the embedding matrix.\\n\\n\\n\\n\\nRecall that each row in the embedding matrix corresponds to the embedding of a word in the model’s vocabulary. The result of this multiplication is interpreted as a score for each word in the model’s vocabulary.\\n\\n\\n\\n\\nWe can simply select the token with the highest score (top_k = 1). But better results are achieved if the model considers other words as well. So a better strategy is to sample a word from the entire list using the score as the probability of selecting that word (so words with a higher score have a higher chance of being selected). A middle ground is setting top_k to 40, and having the model consider the 40 words with the highest scores.\\n\\n\\n\\n\\nWith that, the model has completed an iteration resulting in outputting a single word. The model continues iterating until the entire context is generated (1024 tokens) or until an end-of-sequence token is produced.\\nEnd of part #1: The GPT-2, Ladies and Gentlemen\\nAnd there we have it. A run down of how the GPT2 works. If you’re curious to know exactly what happens inside the self-attention layer, then the following bonus section is for you. I created it to introduce more visual language to describe self-attention in order to make describing later transformer models easier to examine and describe (looking at you, TransformerXL and XLNet).\\nI’d like to note a few oversimplifications in this post:\\n\\nI used “words” and “tokens” interchangeably. But in reality, GPT2 uses Byte Pair Encoding to create the tokens in its vocabulary. This means the tokens are usually parts of words.\\nThe example we showed runs GPT2 in its inference/evaluation mode. That’s why it’s only processing one word at a time. At training time, the model would be trained against longer sequences of text and processing multiple tokens at once. Also at training time, the model would process larger batch sizes (512) vs. the batch size of one that evaluation uses.\\nI took liberties in rotating/transposing vectors to better manage the spaces in the images. At implementation time, one has to be more precise.\\nTransformers use a lot of layer normalization, which is pretty important. We’ve noted a few of these in the Illustrated Transformer, but focused more on self-attention in this post.\\nThere are times when I needed to show more boxes to represent a vector. I indicate those as “zooming in”. For example:\\n\\n\\n\\n\\n\\nPart #2: The Illustrated Self-Attention #\\nEarlier in the post we showed this image to showcase self-attention being applied in a layer that is processing the word it:\\n\\n\\n\\n\\nIn this section, we’ll look at the details of how that is done. Note that we’ll look at it in a way to try to make sense of what happens to individual words. That’s why we’ll be showing many single vectors. The actual implementations are done by multiplying giant matrices together. But I want to focus on the intuition of what happens on a word-level here.\\nSelf-Attention (without masking)\\nLet’s start by looking at the original self-attention as it’s calculated in an encoder block. Let’s look at a toy transformer block that can only process four tokens at a time.\\nSelf-attention is applied through three main steps:\\n\\nCreate the Query, Key, and Value vectors for each path.\\nFor each input token, use its query vector to score against all the other key vectors\\nSum up the value vectors after multiplying them by their associated scores.\\n\\n\\n\\n\\n\\n1- Create Query, Key, and Value Vectors\\nLet’s focus on the first path. We’ll take its query, and compare against all the keys. That produces a score for each key. The first step in self-attention is to calculate the three vectors for each token path (let’s ignore attention heads for now):\\n\\n\\n\\n\\n2- Score\\nNow that we have the vectors, we use the query and key vectors only for step #2. Since we’re focused on the first token, we multiply its query by all the other key vectors resulting in a score for each of the four tokens.\\n\\n\\n\\n\\n3- Sum\\nWe can now multiply the scores by the value vectors. A value with a high score will constitute a large portion of the resulting vector after we sum them up.\\n\\n\\n\\n  The lower the score, the more transparent we\\'re showing the value vector. That\\'s to indicate how multiplying by a small number dilutes the values of the vector.\\n\\nIf we do the same operation for each path, we end up with a vector representing each token containing the appropriate context of that token. Those are then presented to the next sublayer in the transformer block (the feed-forward neural network):\\n\\n\\n\\n\\nThe Illustrated Masked Self-Attention\\nNow that we’ve looked inside a transformer’s self-attention step, let’s proceed to look at masked self-attention. Masked self-attention is identical to self-attention except when it comes to step #2. Assuming the model only has two tokens as input and we’re observing the second token. In this case, the last two tokens are masked. So the model interferes in the scoring step. It basically always scores the future tokens as 0 so the model can’t peak to future words:\\n\\n\\n\\n\\nThis masking is often implemented as a matrix called an attention mask. Think of a sequence of four words (“robot must obey orders”, for example). In a language modeling scenario, this sequence is absorbed in four steps – one per word (assuming for now that every word is a token). As these models work in batches, we can assume a batch size of 4 for this toy model that will process the entire sequence (with its four steps) as one batch.\\n\\n\\n\\n\\nIn matrix form, we calculate the scores by multiplying a queries matrix by a keys matrix. Let’s visualize it as follows, except instead of the word, there would be the query (or key) vector associated with that word in that cell:\\n\\n\\n\\n\\nAfter the multiplication, we slap on our attention mask triangle. It set the cells we want to mask to -infinity or a very large negative number (e.g. -1 billion in GPT2):\\n\\n\\n\\n\\nThen, applying softmax on each row produces the actual scores we use for self-attention:\\n\\n\\n\\n\\nWhat this scores table means is the following:\\n\\nWhen the model processes the first example in the dataset (row #1), which contains only one word (“robot”), 100% of its attention will be on that word.\\nWhen the model processes the second example in the dataset (row #2), which contains the words (“robot must”), when it processes the word “must”, 48% of its attention will be on “robot”, and 52% of its attention will be on “must”.\\nAnd so on\\n\\nGPT-2 Masked Self-Attention\\nLet’s get into more detail on GPT-2’s masked attention.\\nEvaluation Time: Processing One Token at a Time\\nWe can make the GPT-2 operate exactly as masked self-attention works. But during evaluation, when our model is only adding one new word after each iteration, it would be inefficient to recalculate self-attention along earlier paths for tokens that have already been processed.\\nIn this case, we process the first token (ignoring <s> for now).\\n\\n\\n\\n\\nGPT-2 holds on to the key and value vectors of the the a token. Every self-attention layer holds on to its respective key and value vectors for that token:\\n\\n\\n\\n\\nNow in the next iteration, when the model processes the word robot, it does not need to generate query, key, and value queries for the a token. It just reuses the ones it saved from the first iteration:\\n\\n\\n\\n\\nGPT-2 Self-attention: 1- Creating queries, keys, and values\\nLet’s assume the model is processing the word it. If we’re talking about the bottom block, then its input for that token would be the embedding of it + the positional encoding for slot #9:\\n\\n\\n\\n\\nEvery block in a transformer has its own weights (broken down later in the post). The first we encounter is the weight matrix that we use to create the queries, keys, and values.\\n\\n\\n\\n  Self-attention multiplies its input by its weight matrix (and adds a bias vector, not illustrated here).\\n\\nThe multiplication results in a vector that’s basically a concatenation of the query, key, and value vectors for the word it.\\n\\n\\n\\n  Multiplying the input vector by the attention weights vector (and adding a bias vector aftwards) results in the key, value, and query vectors for this token.\\n\\nGPT-2 Self-attention: 1.5- Splitting into attention heads\\nIn the previous examples, we dove straight into self-attention ignoring the “multi-head” part. It would be useful to shed some light on that concept now. Self attention is conducted multiple times on different parts of the Q,K,V vectors. “Splitting” attention heads is simply reshaping the long vector into a matrix. The small GPT2 has 12 attention heads, so that would be the first dimension of the reshaped matrix:\\n\\n\\n\\n\\nIn the previous examples, we’ve looked at what happens inside one attention head. One way to think of multiple attention-heads is like this (if we’re to only visualize three of the twelve attention heads):\\n\\n\\n\\n\\nGPT-2 Self-attention: 2- Scoring\\nWe can now proceed to scoring – knowing that we’re only looking at one attention head (and that all the others are conducting a similar operation):\\n\\n\\n\\n\\nNow the token can get scored against all of keys of the other tokens (that were calculated in attention head #1 in previous iterations):\\n\\n\\n\\n\\nGPT-2 Self-attention: 3- Sum\\nAs we’ve seen before, we now multiply each value with its score, then sum them up, producing the result of self-attention for attention-head #1:\\n\\n\\n\\n\\nGPT-2 Self-attention: 3.5- Merge attention heads\\nThe way we deal with the various attention heads is that we first concatenate them into one vector:\\n\\n\\n\\n\\nBut the vector isn’t ready to be sent to the next sublayer just yet. We need to first turn this Frankenstein’s-monster of hidden states into a homogenous representation.\\nGPT-2 Self-attention: 4- Projecting\\nWe’ll let the model learn how to best map concatenated self-attention results into a vector that the feed-forward neural network can deal with. Here comes our second large weight matrix that projects the results of the attention heads into the output vector of the self-attention sublayer:\\n\\n\\n\\n\\nAnd with this, we have produced the vector we can send along to the next layer:\\n\\n\\n\\n\\nGPT-2 Fully-Connected Neural Network: Layer #1\\nThe fully-connected neural network is where the block processes its input token after self-attention has included the appropriate context in its representation. It is made up of two layers. The first layer is four times the size of the model (Since GPT2 small is 768, this network would have 768*4 = 3072 units). Why four times? That’s just the size the original transformer rolled with (model dimension was 512 and layer #1 in that model was 2048). This seems to give transformer models enough representational capacity to handle the tasks that have been thrown at them so far.\\n\\n\\n\\n  (Not shown: A bias vector)\\n\\nGPT-2 Fully-Connected Neural Network: Layer #2 - Projecting to model dimension\\nThe second layer projects the result from the first layer back into model dimension (768 for the small GPT2). The result of this multiplication is the result of the transformer block for this token.\\n\\n\\n\\n  (Not shown: A bias vector)\\n\\nYou’ve Made It!\\nThat’s the most detailed version of the transformer block we’ll get into! You now pretty much have the vast majority of the picture of what happens inside of a transformer language model. To recap, our brave input vector encounters these weight matrices:\\n\\n\\n\\n\\nAnd each block has its own set of these weights. On the other hand, the model has only one token embedding matrix and one positional encoding matrix:\\n\\n\\n\\n\\nIf you want to see all the parameters of the model, then I have tallied them here:\\n\\n\\n\\n\\nThey add up to 124M parameters instead of 117M for some reason. I’m not sure why, but that’s how many of them seems to be in the published code (please correct me if I’m wrong).\\nPart 3: Beyond Language Modeling #\\nThe decoder-only transformer keeps showing promise beyond language modeling. There are plenty of applications where it has shown success which can be described by similar visuals as the above. Let’s close this post by looking at some of these applications\\nMachine Translation\\nAn encoder is not required to conduct translation. The same task can be addressed by a decoder-only transformer:\\n\\n\\n\\n\\nSummarization\\nThis is the task that the first decoder-only transformer was trained on. Namely, it was trained to read a wikipedia article (without the opening section before the table of contents), and to summarize it. The actual opening sections of the articles were used as the labels in the training datasest:\\n\\n\\n\\n\\nThe paper trained the model against wikipedia articles, and thus the trained model was able to summarize articles:\\n\\n\\n\\n\\nTransfer Learning\\nIn Sample Efficient Text Summarization Using a Single Pre-Trained Transformer, a decoder-only transformer is first pre-trained on language modeling, then finetuned to do summarization. It turns out to achieve better results than a pre-trained encoder-decoder transformer in limited data settings.\\nThe GPT2 paper also shows results of summarization after pre-training the model on language modeling.\\nMusic Generation\\nThe Music Transformer uses a decoder-only transformer to generate music with expressive timing and dynamics. “Music Modeling” is just like language modeling – just let the model learn music in an unsupervised way, then have it sample outputs (what we called “rambling”, earlier).\\nYou might be curious as to how music is represented in this scenario. Remember that language modeling can be done through vector representations of either characters, words, or tokens that are parts of words. With a musical performance (let’s think about the piano for now), we have to represent the notes, but also velocity – a measure of how hard the piano key is pressed.\\n\\n\\n\\n\\nA performance is just a series of these one-hot vectors. A midi file can be converted into such a format. The paper has the following example input sequence:\\n\\n\\n\\n\\nThe one-hot vector representation for this input sequence would look like this:\\n\\n\\n\\n\\nI love a visual in the paper that showcases self-attention in the Music Transformer. I’ve added some annotations to it here:\\n\\n\\n\\n  \"Figure 8: This piece has a recurring triangular contour. The query is at one of the latter peaks and it attends to all of the previous high notes on the peak, all the way to beginning of the piece.\" ... \"[The] figure shows a query (the source of all the attention lines) and previous memories being attended to (the notes that are receiving more softmax probabiliy is highlighted in). The coloring of the attention lines correspond to different heads and the width to the weight of the softmax probability.\"\\n\\nIf you’re unclear on this representation of musical notes, check out this video.\\nConclusion\\nThis concludes our journey into the GPT2, and our exploration of its parent model, the decoder-only transformer. I hope that you come out of this post with a better understanding of self-attention and more comfort that you understand more of what goes on inside a transformer.\\nResources\\n\\nThe GPT2 Implementation from OpenAI\\nCheck out the pytorch-transformers library from Hugging Face in addition to GPT2, it implements BERT, Transformer-XL, XLNet and other cutting-edge transformer models.\\n\\nAcknowledgements\\nThanks to Lukasz Kaiser, Mathias Müller, Peter J. Liu, Ryan Sepassi and Mohammad Saleh for feedback on earlier versions of this post.\\nComments or corrections? Please tweet me at @JayAlammar\\n\\n\\n\\n    Written on August 12, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Word2vec\\n\\n Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\nPersonality Embeddings: What are you like?\\n\\n“I give you the desert chameleon, whose ability to blend itself into the background tells you all you need to know about the roots of ecology and the foundations of a personal identity” ~Children of Dune\\n\\nOn a scale of 0 to 100, how introverted/extraverted are you (where 0 is the most introverted, and 100 is the most extraverted)?\\nHave you ever taken a personality test like MBTI – or even better, the Big Five Personality Traits test? If you haven’t, these are tests that ask you a list of questions, then score you on a number of axes, introversion/extraversion being one of them.\\n\\n\\n\\n  Example of the result of a Big Five Personality Trait test. It can really tell you a lot about yourself and is shown to have predictive ability in academic, personal, and professional success. This is one place to find your results.\\n\\nImagine I’ve scored 38/100 as my introversion/extraversion score. we can plot that in this way:\\n\\n\\n\\nLet’s switch the range to be from -1 to 1:\\n\\n\\n\\nHow well do you feel you know a person knowing only this one piece of information about them? Not much. People are complex. So let’s add another dimension – the score of one other trait from the test.\\n\\n\\n\\n  We can represent the two dimensions as a point on the graph, or better yet, as a vector from the origin to that point. We have incredible tools to deal with vectors that will come in handy very shortly.\\n\\nI’ve hidden which traits we’re plotting just so you get used to not knowing what each dimension represents – but still getting a lot of value from the vector representation of a person’s personality.\\nWe can now say that this vector partially represents my personality. The usefulness of such representation comes when you want to compare two other people to me. Say I get hit by a bus and I need to be replaced by someone with a similar personality. In the following figure, which of the two people is more similar to me?\\n\\n\\n\\nWhen dealing with vectors, a common way to calculate a similarity score is cosine_similarity:\\n\\n\\n\\nPerson #1 is more similar to me in personality. Vectors pointing at the same direction (length plays a role as well) have a higher cosine similarity score.\\n\\nYet again, two dimensions aren’t enough to capture enough information about how different people are. Decades of psychology research have led to five major traits (and plenty of sub-traits). So let’s use all five dimensions in our comparison:\\n\\n\\n\\n\\nThe problem with five dimensions is that we lose the ability to draw neat little arrows in two dimensions. This is a common challenge in machine learning where we often have to think in higher-dimensional space. The good thing is, though, that cosine_similarity still works. It works with any number of dimensions:\\n\\n\\n\\n  cosine_similarity works for any number of dimensions. These are much better scores because they\\'re calculated based on a higher resolution representation of the things being compared.\\n\\nAt the end of this section, I want us to come out with two central ideas:\\n\\nWe can represent people (and things) as vectors of numbers (which is great for machines!).\\nWe can easily calculate how similar vectors are to each other.\\n\\n\\n\\n\\n\\nWord Embeddings\\n\\n“The gift of words is the gift of deception and illusion” ~Children of Dune\\n\\nWith this understanding, we can proceed to look at trained word-vector examples (also called word embeddings) and start looking at some of their interesting properties.\\nThis is a word embedding for the word “king” (GloVe vector trained on Wikipedia):\\n\\n[ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\\n -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,\\n -0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\\n -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,\\n  0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\\n  1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,\\n  0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\\n -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\\n -0.64426 , -0.51042 ]\\n \\nIt’s a list of 50 numbers. We can’t tell much by looking at the values. But let’s visualize it a bit so we can compare it other word vectors. Let’s put all these numbers in one row:\\n\\n\\n\\n\\nLet’s color code the cells based on their values (red if they’re close to 2, white if they’re close to 0, blue if they’re close to -2):\\n\\n\\n\\n\\nWe’ll proceed by ignoring the numbers and only looking at the colors to indicate the values of the cells. Let’s now contrast “King” against other words:\\n\\n\\n\\n\\nSee how “Man” and “Woman” are much more similar to each other than either of them is to “king”? This tells you something. These vector representations capture quite a bit of the information/meaning/associations of these words.\\nHere’s another list of examples (compare by vertically scanning the columns looking for columns with similar colors):\\n\\n\\n\\n\\nA few things to point out:\\n\\nThere’s a straight red column through all of these different words. They’re similar along that dimension (and we don’t know what each dimensions codes for)\\nYou can see how “woman” and “girl” are similar to each other in a lot of places. The same with “man” and “boy”\\n“boy” and “girl” also have places where they are similar to each other, but different from “woman” or “man”. Could these be coding for a vague conception of youth? possible.\\nAll but the last word are words representing people. I added an object (water) to show the differences between categories. You can, for example, see that blue column going all the way down and stopping before the embedding for “water”.\\nThere are clear places where “king” and “queen” are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?\\n\\nAnalogies\\n\\n\"Words can carry any burden we wish. All that\\'s required is agreement and a tradition upon which to build.\" ~God Emperor of Dune\\n\\nThe famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:\\n\\n\\n\\n  Using the Gensim library in python, we can add and subtract word vectors, and it would find the most similar words to the resulting vector. The image shows a list of the most similar words, each with its cosine similarity.\\n\\nWe can visualize this analogy as we did previously:\\n\\n\\n\\n  The resulting vector from \"king-man+woman\" doesn\\'t exactly equal \"queen\", but \"queen\" is the closest word to it from the 400,000 word embeddings we have in this collection.\\n\\nNow that we’ve looked at trained word embeddings, let’s learn more about the training process. But before we get to word2vec, we need to look at a conceptual parent of word embeddings: the neural language model.\\nLanguage Modeling\\n\\n  “The prophet is not diverted by illusions of past, present and future. The fixity of language determines such linear distinctions. Prophets hold a key to the lock in a language.  \\n\\n  This is not a mechanical universe. The linear progression of events is imposed by the observer. Cause and effect? That\\'s not it at all. The prophet utters fateful words. You glimpse a thing \"destined to occur.\" But the prophetic instant releases something of infinite portent and power. The universe undergoes a ghostly shift.” ~God Emperor of Dune\\n\\nIf one wanted to give an example of an NLP application, one of the best examples would be the next-word prediction feature of a smartphone keyboard. It’s a feature that billions of people use hundreds of times every day.\\n\\n\\n\\n\\nNext-word prediction is a task that can be addressed by a language model. A language model can take a list of words (let’s say two words), and attempt to predict the word that follows them.\\nIn the screenshot above, we can think of the model as one that took in these two green words (thou shalt) and returned a list of suggestions (“not” being the one with the highest probability):\\n\\n\\n\\n\\n\\nWe can think of the model as looking like this black box:\\n\\n\\n\\n\\n\\n\\nBut in practice, the model doesn’t output only one word. It actually outputs a probability score for all the words it knows (the model’s “vocabulary”, which can range from a few thousand to over a million words). The keyboard application then has to find the words with the highest scores, and present those to the user.\\n\\n\\n\\n\\n  The output of the neural language model is a probability score for all the words the model knows. We\\'re referring to the probability as a percentage here, but 40% would actually be represented as 0.4 in the output vector.\\n\\n\\nAfter being trained, early neural language models (Bengio 2003) would calculate a prediction in three steps:\\n\\n\\n\\n\\n\\n\\nThe first step is the most relevant for us as we discuss embeddings. One of the results of the training process was this matrix that contains an embedding for each word in our vocabulary. During prediction time, we just look up the embeddings of the input word, and use them to calculate the prediction:\\n\\n\\n\\n\\nLet’s now turn to the training process to learn more about how this embedding matrix was developed.\\nLanguage Model Training\\n\\n“A process cannot be understood by stopping it. Understanding must move with the flow of the process, must join it and flow with it.” ~Dune\\n\\nLanguage models have a huge advantage over most other machine learning models. That advantage is that we are able to train them on running text – which we have an abundance of. Think of all the books, articles, Wikipedia content, and other forms of text data we have lying around. Contrast this with a lot of other machine learning models which need hand-crafted features and specially-collected data.\\n\\n“You shall know a word by the company it keeps” J.R. Firth\\n\\nWords get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that\\n\\nWe get a lot of text data (say, all Wikipedia articles, for example). then\\nWe have a window (say, of three words) that we slide against all of that text.\\nThe sliding window generates training samples for our model\\n\\n\\n\\n\\n\\nAs this window slides against the text, we (virtually) generate a dataset that we use to train a model. To look exactly at how that’s done, let’s see how the sliding window processes this phrase:\\n\\n“Thou shalt not make a machine in the likeness of a human mind” ~Dune\\n\\nWhen we start, the window is on the first three words of the sentence:\\n\\n\\n\\n\\n\\n\\nWe take the first two words to be features, and the third word to be a label:\\n\\n\\n\\n\\n  We now have generated the first sample in the dataset we can later use to train a language model.\\n\\n\\nWe then slide our window to the next position and create a second sample:\\n\\n\\n\\n\\n  An the second example is now generated.\\n\\n\\nAnd pretty soon we have a larger dataset of which words tend to appear after different pairs of words:\\n\\n\\n\\n\\n\\n\\nIn practice, models tend to be trained while we’re sliding the window. But I find it clearer to logically separate the “dataset generation” phase from the training phase. Aside from neural-network-based approaches to language modeling, a technique called N-grams was commonly used to train language models (see: Chapter 3 of Speech and Language Processing). To see how this switch from N-grams to neural models reflects on real-world products, here’s a 2015 blog post from Swiftkey, my favorite Android keyboard, introducing their neural language model and comparing it with their previous N-gram model. I like this example because it shows you how the algorithmic properties of embeddings can be described in marketing speech.\\nLook both ways\\n\\n\"Paradox is a pointer telling you to look beyond it. If paradoxes bother you, that betrays your deep desire for absolutes. The relativist treats a paradox merely as interesting, perhaps amusing or even, dreadful thought, educational.\" ~God Emperor of Dune\\n\\nKnowing what you know from earlier in the post, fill in the blank:\\n\\n\\n\\n\\nThe context I gave you here is five words before the blank word (and an earlier mention of “bus”). I’m sure most people would guess the word bus goes into the blank. But what if I gave you one more piece of information – a word after the blank, would that change your answer?\\n\\n\\n\\n\\nThis completely changes what should go in the blank. the word red is now the most likely to go into the blank. What we learn from this is the words both before and after a specific word carry informational value. It turns out that accounting for both directions (words to the left and to the right of the word we’re guessing) leads to better word embeddings. Let’s see how we can adjust the way we’re training the model to account for this.\\nSkipgram\\n\\n  “Intelligence takes chance with limited data in an arena where mistakes are not only possible but also necessary.” ~Chapterhouse: Dune\\n\\nInstead of only looking two words before the target word, we can also look at two words after it.\\n\\n\\n\\n\\nIf we do this, the dataset we’re virtually building and training the model against would look like this:\\n\\n\\n\\n\\nThis is called a Continuous Bag of Words architecture and is described in one of the word2vec papers [pdf]. Another architecture that also tended to show great results does things a little differently.\\nInstead of guessing a word based on its context (the words before and after it), this other architecture tries to guess neighboring words using the current word. We can think of the window it slides against the training text as looking like this:\\n\\n\\n  \\n  The word in the green slot would be the input word, each pink box would be a possible output.\\n\\nThe pink boxes are in different shades because this sliding window actually creates four separate samples in our training dataset:\\n\\n\\n\\n\\n\\n\\nThis method is called the skipgram architecture. We can visualize the sliding window as doing the following:\\n\\n\\n\\n\\n\\n\\nThis would add these four samples to our training dataset:\\n\\n\\n\\n\\n\\nWe then slide our window to the next position:\\n\\n\\n\\n\\n\\n\\nWhich generates our next four examples:\\n\\n\\n\\n\\n\\nA couple of positions later, we have a lot more examples:\\n\\n\\n\\n\\n\\nRevisiting the training process\\n\\n  \"Muad\\'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It\\'s shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\" ~Dune\\n\\nNow that we have our skipgram training dataset that we extracted from existing running text, let’s glance at how we use it to train a basic neural language model that predicts the neighboring word.\\n\\n\\n\\n\\n\\nWe start with the first sample in our dataset. We grab the feature and feed to the untrained model asking it to predict an appropriate neighboring word.\\n\\n\\n\\n\\n\\nThe model conducts the three steps and outputs a prediction vector (with a probability assigned to each word in its vocabulary). Since the model is untrained, it’s prediction is sure to be wrong at this stage. But that’s okay. We know what word it should have guessed – the label/output cell in the row we’re currently using to train the model:\\n\\n\\n\\n  \\n  The \\'target vector\\' is one where the target word has the probability 1, and all other words have the probability 0.\\n\\n\\nHow far off was the model? We subtract the two vectors resulting in an error vector:\\n\\n\\n\\n\\n\\n\\nThis error vector can now be used to update the model so the next time, it’s a little more likely to guess thou when it gets not as input.\\n\\n\\n\\n\\n\\n\\nAnd that concludes the first step of the training. We proceed to do the same process with the next sample in our dataset, and then the next, until we’ve covered all the samples in the dataset. That concludes one epoch of training. We do it over again for a number of epochs, and then we’d have our trained model and we can extract the embedding matrix from it and use it for any other application.\\nWhile this extends our understanding of the process, it’s still not how word2vec is actually trained. We’re missing a couple of key ideas.\\nNegative Sampling\\n\\n“To attempt an understanding of Muad\\'Dib without understanding his mortal enemies, the Harkonnens, is to attempt seeing Truth without knowing Falsehood. It is the attempt to see the Light without knowing Darkness. It cannot be.” ~Dune\\n\\nRecall the three steps of how this neural language model calculates its prediction:\\n\\n\\n\\n\\n\\n\\nThe third step is very expensive from a computational point of view – especially knowing that we will do it once for every training sample in our dataset (easily tens of millions of times). We need to do something to improve performance.\\nOne way is to split our target into two steps:\\n\\nGenerate high-quality word embeddings (Don’t worry about next-word prediction).\\nUse these high-quality embeddings to train a language model (to do next-word prediction).\\n\\nWe’ll focus on step 1. in this post as we’re focusing on embeddings. To generate high-quality embeddings using a high-performance model, we can switch the model’s task from predicting a neighboring word:\\n\\n\\n\\n\\nAnd switch it to a model that takes the input and output word, and outputs a score indicating if they’re neighbors or not (0 for “not neighbors”, 1 for “neighbors”).\\n\\n\\n\\n\\nThis simple switch changes the model we need from a neural network, to a logistic regression model – thus it becomes much simpler and much faster to calculate.\\nThis switch requires we switch the structure of our dataset – the label is now a new column with values 0 or 1. They will be all 1 since all the words we added are neighbors.\\n\\n\\n\\n\\n\\n\\nThis can now be computed at blazing speed – processing millions of examples in minutes. But there’s one loophole we need to close. If all of our examples are positive (target: 1), we open ourself to the possibility of a smartass model that always returns 1 – achieving 100% accuracy, but learning nothing and generating garbage embeddings.\\n\\n\\n\\n\\nTo address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors.  Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.\\n\\n\\n\\n  \\n  For each sample in our dataset, we add negative examples. Those have the same input word, and a 0 label.\\n\\nBut what do we fill in as output words? We randomly sample words from our vocabulary\\n\\n\\n\\n\\n\\nThis idea is inspired by Noise-contrastive estimation [pdf]. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency.\\nSkipgram with Negative Sampling (SGNS)\\nWe have now covered two of the central ideas in word2vec: as a pair, they’re called skipgram with negative sampling.\\n\\n\\n\\n\\nWord2vec Training Process\\n\\n\"The machine cannot anticipate every problem of importance to humans. It is the difference between serial bits and an unbroken continuum. We have the one; machines are confined to the other.\" ~God Emperor of Dune\\n\\nNow that we’ve established the two central ideas of skipgram and negative sampling, we can proceed to look closer at the actual word2vec training process.\\nBefore the training process starts, we pre-process the text we’re training the model against. In this step, we determine the size of our vocabulary (we’ll call this vocab_size, think of it as, say, 10,000) and which words belong to it.\\nAt the start of the training phase, we create two matrices – an Embedding matrix and a Context matrix. These two matrices have an embedding for each word in our vocabulary (So vocab_size is one of their dimensions). The second dimension is how long we want each embedding to be (embedding_size – 300 is a common value, but we’ve looked at an example of 50 earlier in this post).\\n\\n\\n\\n\\nAt the start of the training process, we initialize these matrices with random values. Then we start the training process. In each training step, we take one positive example and its associated negative examples. Let’s take our first group:\\n\\n\\n\\n\\nNow we have four words: the input word not and output/context words: thou (the actual neighbor), aaron, and taco (the negative examples). We proceed to look up their embeddings – for the input word, we look in the Embedding matrix. For the context words, we look in the Context matrix (even though both matrices have an embedding for every word in our vocabulary).\\n\\n\\n\\n\\nThen, we take the dot product of the input embedding with each of the context embeddings. In each case, that would result in a number, that number indicates the similarity of the input and context embeddings\\n\\n\\n\\n\\nNow we need a way to turn these scores into something that looks like probabilities – we need them to all be positive and have values between zero and one. This is a great task for sigmoid, the logistic operation.\\n\\n\\n\\n\\nAnd we can now treat the output of the sigmoid operations as the model’s output for these examples. You can see that taco has the highest score and aaron still has the lowest score both before and after the sigmoid operations.\\nNow that the untrained model has made a prediction, and seeing as though we have an actual target label to compare against, let’s calculate how much error is in the model’s prediction. To do that, we just subtract the sigmoid scores from the target labels.\\n\\n\\n\\nerror = target - sigmoid_scores\\n\\n\\nHere comes the “learning” part of “machine learning”. We can now use this error score to adjust the embeddings of not, thou, aaron, and taco so that the next time we make this calculation, the result would be closer to the target scores.\\n\\n\\n\\n\\nThis concludes the training step. We emerge from it with slightly better embeddings for the words involved in this step (not, thou, aaron, and taco). We now proceed to our next step (the next positive sample and its associated negative samples) and do the same process again.\\n\\n\\n\\n\\nThe embeddings continue to be improved while we cycle through our entire dataset for a number of times. We can then stop the training process, discard the Context matrix, and use the Embeddings matrix as our pre-trained embeddings for the next task.\\nWindow Size and Number of Negative Samples\\nTwo key hyperparameters in the word2vec training process are the window size and the number of negative samples.\\n\\n\\n\\n\\nDifferent tasks are served better by different window sizes. One heuristic is that smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we’re only looking at their surrounding words – e.g. good and bad often appear in similar contexts). Larger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words. In practice, you’ll often have to provide annotations that guide the embedding process leading to a useful similarity sense for your task. The Gensim default window size is 5 (five words before and five words after the input word, in addition to the input word itself).\\n\\n\\n\\n\\nThe number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples.\\nConclusion\\n\\n“If it falls outside your yardsticks, then you are engaged with intelligence, not with automation”  ~God Emperor of Dune\\n\\nI hope that you now have a sense for word embeddings and the word2vec algorithm. I also hope that now when you read a paper mentioning “skip gram with negative sampling” (SGNS) (like the recommendation system papers at the top), that you have a better sense for these concepts. As always, all feedback is appreciated @JayAlammar.\\nReferences & Further Readings\\n\\nDistributed Representations of Words and Phrases and their Compositionality [pdf]\\nEfficient Estimation of Word Representations in Vector Space [pdf]\\nA Neural Probabilistic Language Model [pdf]\\nSpeech and Language Processing by Dan Jurafsky and James H. Martin is a leading resource for NLP. Word2vec is tackled in Chapter 6.\\nNeural Network Methods in Natural Language Processing by Yoav Goldberg is a great read for neural NLP topics.\\nChris McCormick has written some great blog posts about Word2vec. He also just released The Inner Workings of word2vec, an E-book focused on the internals of word2vec.\\nWant to read the code? Here are two options:\\n    \\nGensim’s python implementation of word2vec\\nMikolov’s original implementation in C – better yet, this version with detailed comments from Chris McCormick.\\n\\n\\nEvaluating distributional models of compositional semantics\\nOn word embeddings, part 2\\nDune\\n\\n\\n\\n\\n\\n\\n\\n    Written on March 27, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)\\n\\nDiscussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n\\n\\n(ULM-FiT has nothing to do with Cookie Monster. But I couldn’t think of anything else..)\\nOne of the latest milestones in this development is the release of BERT, an event described as marking the beginning of a new era in NLP. BERT is a model that broke several records for how well models can handle language-based tasks. Soon after the release of the paper describing the model, the team also open-sourced the code of the model, and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component – saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch.\\n\\n\\n\\n  The two steps of how BERT is developed. You can download the model pre-trained in step 1 (trained on un-annotated data), and only worry about fine-tuning it for step 2. [Source for book icon].\\n\\nBERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).\\nThere are a number of concepts one needs to be aware of to properly wrap one’s head around what BERT is. So let’s start by looking at ways you can use BERT before looking at the concepts involved in the model itself.\\nExample: Sentence Classification\\nThe most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:\\n\\nTo train such a model, you mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning, and has roots in Semi-supervised Sequence Learning and ULMFiT.\\nFor people not versed in the topic, since we’re talking about classifiers, then we are in the supervised-learning domain of machine learning. Which would mean we need a labeled dataset to train such a model. For this spam classifier example, the labeled dataset would be a list of email messages and a label (“spam” or “not spam” for each message).\\n\\n\\n\\nOther examples for such a use-case include:\\n\\nSentiment analysis\\n\\nInput: Movie/Product review. Output: is the review positive or negative?\\nExample dataset: SST\\n\\n\\nFact-checking\\n\\nInput: sentence. Output: “Claim” or “Not Claim”\\nMore ambitious/futuristic example:\\n        \\nInput: Claim sentence. Output: “True” or “False”\\n\\n\\nFull Fact is an organization building automatic fact-checking tools for the benefit of the public. Part of their pipeline is a classifier that reads news articles and detects claims (classifies text as either “claim” or “not claim”) which can later be fact-checked (by humans now, with ML later, hopefully).\\nVideo: Sentence embeddings for automated factchecking - Lev Konstantinovskiy.\\n\\n\\n\\nModel Architecture\\nNow that you have an example use-case in your head for how BERT can be used, let’s take a closer look at how it works.\\n\\nThe paper presents two model sizes for BERT:\\n\\nBERT BASE – Comparable in size to the OpenAI Transformer in order to compare performance\\nBERT LARGE – A ridiculously huge model which achieved the state of the art results reported in the paper\\n\\nBERT is basically a trained Transformer Encoder stack. This is a good time to direct you to read my earlier post The Illustrated Transformer which explains the Transformer model – a foundational concept for BERT and the concepts we’ll discuss next.\\n\\nBoth BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).\\nModel Inputs\\n\\nThe first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.\\nJust like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.\\n\\nIn terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge.\\nModel Outputs\\nEach position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we’ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to).\\n\\nThat vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.\\n\\nIf you have more labels (for example if you’re an email service that tags emails with “spam”, “not spam”, “social”, and “promotion”), you just tweak the classifier network to have more output neurons that then pass through softmax.\\nParallels with Convolutional Nets\\nFor those with a background in computer vision, this vector hand-off should be reminiscent of what happens between the convolution part of a network like VGGNet and the fully-connected classification portion at the end of the network.\\n\\nA New Age of Embedding\\nThese new developments carry with them a new shift in how words are encoded. Up until now, word-embeddings have been a major force in how leading NLP models deal with language. Methods like Word2Vec and Glove have been widely used for such tasks. Let’s recap how those are used before pointing to what has now changed.\\nWord Embedding Recap\\nFor words to be processed by machine learning models, they need some form of numeric representation that models can use in their calculation. Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).\\nThe field quickly realized it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset.  So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe. This is an example of the GloVe embedding of the word “stick” (with an embedding vector size of 200)\\n\\n\\n\\n  The GloVe word embedding of the word \"stick\" - a vector of 200 floats (rounded to two decimals). It goes on for two hundred values.\\n\\nSince these are large and full of numbers, I use the following basic shape in the figures in my posts to show vectors:\\n\\n\\n\\n\\nELMo: Context Matters\\nIf we’re using this GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was. “Wait a minute” said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. And so, contextualized word-embeddings were born.\\n\\n\\n\\n  Contextualized word-embeddings can give words different embeddings based on the meaning they carry in the context of the sentence. Also, RIP Robin Williams\\n\\nInstead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.\\n\\n\\n\\n\\nELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.\\nWhat’s ELMo’s secret?\\nELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.\\n\\n\\n\\n  A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.\\n\\nWe can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding process after this pre-training is done.\\nELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.\\n\\n\\n\\nGreat slides on ELMo\\n\\nELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).\\n\\n\\n\\nULM-FiT: Nailing down Transfer Learning in NLP\\nULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training – more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks.\\nNLP finally had a way to do transfer learning probably as well as Computer Vision could.\\nThe Transformer: Going beyond LSTMs\\nThe release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependancies better than LSTMs.\\nThe Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component).\\nOpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling\\nIt turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.\\n\\n\\n\\n  The OpenAI Transformer is made up of the decoder stack from the Transformer\\n\\nThe model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).\\nWith this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.\\n\\n\\n\\n  The OpenAI Transformer is now ready to be trained to predict the next word on a dataset made up of 7,000 books.\\n\\nTransfer Learning to Downstream Tasks\\nNow that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):\\n\\n\\n\\n\\n  How to use a pre-trained OpenAI transformer to do sentence clasification\\n\\nThe OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.\\n\\n\\n\\n\\nIsn’t that clever?\\nBERT: From Decoders to Encoders\\nThe openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?\\n“Hold my beer”, said R-rated BERT.\\nMasked Language Model\\n“We’ll use transformer encoders”, said BERT.\\n“This is madness”, replied Ernie, “Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.”\\n“We’ll use masks”, said BERT confidently.\\n\\n\\n\\n  BERT\\'s clever language modeling task masks 15% of words in the input and asks the model to predict the missing word.\\n\\nFinding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a “masked language model” concept from earlier literature (where it’s called a Cloze task).\\nBeyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.\\nTwo-sentence Tasks\\nIf you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).\\nTo make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?\\n\\n\\n\\n  The second task BERT is pre-trained on is a two-sentence classification task. The tokenization is oversimplified in this graphic as BERT actually uses WordPieces as tokens rather than words --- so some words are broken down into smaller chunks.\\n\\nTask specific-Models\\nThe BERT paper shows a number of ways to use BERT for different tasks.\\n\\n\\n\\n\\nBERT for feature extraction\\nThe fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.\\n\\n\\n\\n\\nWhich vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):\\n\\n\\n\\n\\nTake BERT out for a spin\\nThe best way to try out BERT is through the BERT FineTuning with Cloud TPUs notebook hosted on Google Colab. If you’ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well.\\nThe next step would be to look at the code in the BERT repo:\\n\\nThe model is constructed in modeling.py (class BertModel) and is pretty much identical to a vanilla Transformer encoder.\\n\\nrun_classifier.py is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the create_model() method in that file.\\n\\n\\nSeveral pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia.\\n\\nBERT doesn’t look at words as tokens. Rather, it looks at WordPieces. tokenization.py is the tokenizer that would turns your words into wordPieces appropriate for BERT.\\n\\nYou can also check out the PyTorch implementation of BERT. The AllenNLP library uses this implementation to allow using BERT embeddings with any model.\\nAcknowledgements\\nThanks to Jacob Devlin, Matt Gardner, Kenton Lee,  Mark Neumann, and Matthew Peters for providing feedback on earlier drafts of this post.\\n\\n\\n    Written on December  3, 2018\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='\\n\\n\\nThe Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Transformer\\n\\nDiscussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n\\n\\n\\nUpdate: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they\\'ve evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n\\n\\nPopping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.\\n\\n\\n\\nThe encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.\\n\\n\\n\\nThe encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:\\n\\n\\n\\nThe encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.\\nThe outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.\\nThe decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).\\n\\n\\n\\nBringing The Tensors Into The Picture\\nNow that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.\\nAs is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.\\n\\n\\n\\n\\n  Each word is embedded into a vector of size 512. We\\'ll represent those vectors with these simple boxes.\\n\\nThe embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.\\nAfter embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.\\n\\n\\n\\n\\nHere we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\\nNext, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.\\nNow We’re Encoding!\\nAs we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.\\n\\n\\n\\n  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.\\n\\nSelf-Attention at a High Level\\nDon’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.\\nSay the following sentence is an input sentence we want to translate:\\n”The animal didn\\'t cross the street because it was too tired”\\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\\nWhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\\nIf you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\\n\\n\\n\\n  As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".\\n\\nBe sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\\nSelf-Attention in Detail\\nLet’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.\\nThe first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\\nNotice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.\\n\\n\\n\\n\\n  Multiplying x1 by the WQ weight matrix produces q1, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\\n\\n\\n\\nWhat are the “query”, “key”, and “value” vectors?\\n\\n\\nThey’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.\\nThe second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\\n\\n\\n\\n\\n\\n\\nThe third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\\n\\n\\n\\n\\n\\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\\n\\nThe fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\\n\\n\\n\\n\\n\\nThat concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.\\nMatrix Calculation of Self-Attention\\nThe first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).\\n\\n\\n\\n  Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)\\n\\n\\nFinally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.\\n\\n\\n\\n  The self-attention calculation in matrix form\\n\\n\\n\\nThe Beast With Many Heads\\nThe paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:\\n\\n\\nIt expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.\\n\\n\\nIt gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\\n\\n\\n\\n\\n\\n   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.\\n \\n\\nIf we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices\\n\\n\\n\\n\\n\\nThis leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\\nHow do we do that? We concat the matrices then multiply them by an additional weights matrix WO.\\n\\n\\n\\n\\nThat’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place\\n\\n\\n\\n\\n\\n\\nNow that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:\\n\\n\\n\\n  As we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model\\'s representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\".\\n\\n\\nIf we add all the attention heads to the picture, however, things can be harder to interpret:\\n\\n\\n\\n\\nRepresenting The Order of The Sequence Using Positional Encoding\\nOne thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.\\nTo address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\\n\\n\\n\\n\\n  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.\\n\\n\\nIf we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:\\n\\n\\n\\n  A real example of positional encoding with a toy embedding size of 4\\n\\n\\nWhat might this pattern look like?\\nIn the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.\\n\\n\\n\\n  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That\\'s because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They\\'re then concatenated to form each of the positional encoding vectors.\\n\\nThe formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d(). This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).\\nJuly 2020 Update: \\nThe positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it:\\n\\n\\n\\n\\nThe Residuals\\nOne detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.\\n\\n\\n\\n\\nIf we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:\\n\\n\\n\\n\\nThis goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:\\n\\n\\n\\n\\nThe Decoder Side\\nNow that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.\\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\\n\\n\\n\\n  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).\\n\\nThe following steps repeat the process until a special  symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\\n\\n\\n\\n\\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\\nThe Final Linear and Softmax Layer\\nThe decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\\n\\n\\n\\n\\n  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.\\n\\n\\nRecap Of Training\\nNow that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\\nDuring training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.\\nTo visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).\\n\\n\\n\\n   The output vocabulary of our model is created in the preprocessing phase before we even begin training.\\n \\nOnce we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:\\n\\n\\n\\n  Example: one-hot encoding of our output vocabulary\\n\\nFollowing this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\\nThe Loss Function\\nSay we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.\\nWhat this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.\\n\\n\\n\\n  Since the model\\'s parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model\\'s weights using backpropagation to make the output closer to the desired output.\\n\\n\\nHow do you compare two probability distributions? We simply subtract one from the other. For more details, look at  cross-entropy and Kullback–Leibler divergence.\\nBut note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:\\n\\nEach probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)\\nThe first probability distribution has the highest probability at the cell associated with the word “i”\\nThe second probability distribution has the highest probability at the cell associated with the word “am”\\nAnd so on, until the fifth output distribution indicates ‘<end of sentence>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.\\n\\n\\n\\n\\n   The targeted probability distributions we\\'ll train our model against in the training example for one sample sentence.\\n \\n\\nAfter training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\\n\\n\\n\\n    Hopefully upon training, the model would output the right translation we expect. Of course it\\'s no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it\\'s unlikely to be the output of that time step -- that\\'s a very useful property of softmax which helps the training process.\\n\\nNow, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.\\nGo Forth And Transform\\nI hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:\\n\\nRead the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.\\nWatch Łukasz Kaiser’s talk walking through the model and its details\\nPlay with the Jupyter Notebook provided as part of the Tensor2Tensor repo\\nExplore the Tensor2Tensor repo.\\n\\nFollow-up works:\\n\\nDepthwise Separable Convolutions for Neural Machine Translation\\nOne Model To Learn Them All\\nDiscrete Autoencoders for Sequence Models\\nGenerating Wikipedia by Summarizing Long Sequences\\nImage Transformer\\nTraining Tips for the Transformer Model\\nSelf-Attention with Relative Position Representations\\nFast Decoding in Sequence Models using Discrete Latent Variables\\nAdafactor: Adaptive Learning Rates with Sublinear Memory Cost\\n\\nAcknowledgements\\nThanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.\\nPlease hit me up on Twitter for any corrections or feedback.\\n\\n\\n    Written on June 27, 2018\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list = [item for subitem in docs for item in subitem]\n",
    "docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f8e5d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=25,allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4bc0f360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Blog\\nAbout\\n\\n\\n\\n\\n\\n\\nA Visual Guide to Using BERT for the First Time\\n\\nTranslations: Chinese, Korean, Russian'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Progress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='This post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Dataset: SST2\\nThe dataset we will use in this example is SST2, which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):\\n\\n\\n\\n    sentence\\n    \\n\\n    label'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='sentence\\n    \\n\\n    label\\n    \\n\\n\\n\\n      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\\n    \\n\\n      1\\n    \\n\\n\\n\\n      apparently reassembled from the cutting room floor of any given daytime soap'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"apparently reassembled from the cutting room floor of any given daytime soap\\n    \\n\\n      0\\n    \\n\\n\\n\\n      they presume their audience won't sit still for a sociology lesson\\n    \\n\\n      0\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"0\\n    \\n\\n\\n\\n      this is a visually stunning rumination on love , memory , history and the war between art and commerce\\n    \\n\\n      1\\n    \\n\\n\\n\\n      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='1\\n    \\n\\n\\nModels: Sentence Sentiment Classification\\nOur goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\\n\\n\\n\\n\\nUnder the hood, the model is actually made up of two model.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='DistilBERT  processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the  sentence as either positive or negative (1 or 0, respectively).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='If you’ve read my previous post, Illustrated BERT, this vector is the result of the first position (which receives the [CLS] token as input).\\nModel Training'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='While we’ll be using two models, we will only train the logistic regression model. For DistillBERT, we’ll use a model that’s already pre-trained and has a grasp on the English language. This model, however is neither trained not fine-tuned to do sentence classification. We get some sentence classification capability, however, from the general objectives BERT is trained on. This is especially the case with BERT’s output for'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='however, from the general objectives BERT is trained on. This is especially the case with BERT’s output for the first position (associated with the [CLS] token). I believe that’s due to BERT’s second training object – Next sentence classification. That objective seemingly trains the model to encapsulate a sentence-wide sense to the output at the first position. The transformers library provides us with an implementation of DistilBERT as well as pretrained'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='output at the first position. The transformers library provides us with an implementation of DistilBERT as well as pretrained versions of the model.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Tutorial Overview\\nSo here’s the game plan with this tutorial. We will first use the trained distilBERT to generate sentence embeddings for 2,000 sentences.\\n\\n\\n\\n\\nWe will not touch distilBERT after this step. It’s all Scikit Learn from here. We do the usual train/test split on this dataset:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"Train/test split for the output of distilBert (model #1) creates the dataset we'll train and evaluate logistic regression on (model #2). Note that in reality, sklearn's train/test split shuffles the examples before making the split, it doesn't just take the first 75% of examples as they appear in the dataset.\\n\\nThen we train the logistic regression model on the training set:\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='How a single prediction is calculated\\nBefore we dig into the code and explain how to train the model, let’s look at how a trained model calculates its prediction.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Let’s try to classify the sentence “a visually stunning rumination on love”. The first step is to use the BERT tokenizer to first split the word into tokens. Then, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='The third step the tokenizer does is to replace each token with its id from the embedding table which is a component we get with the trained model. Read The Illustrated Word2vec for a background on word embeddings.\\n\\n\\n\\n\\nNote that the tokenizer does all these steps in a single line of code:\\ntokenizer.encode(\"a visually stunning rumination on love\", add_special_tokens=True)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Our input sentence is now the proper shape to be passed to DistilBERT.\\nIf you’ve read Illustrated BERT, this step can also be visualized in this manner:\\n\\n\\n\\n\\nFlowing Through DistilBERT\\nPassing the input vector through DistilBERT works just like BERT. The output would be a vector for each input token. each vector is made up of 768 numbers (floats).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Because this is a sentence classification task, we ignore all except the first vector (the one associated with the [CLS] token). The one vector we pass as the input to the logistic regression model.\\n\\n\\n\\n\\nFrom here, it’s the logistic regression model’s job to classify this vector based on what it learned from its training phase. We can think of a prediction calculation as looking like this:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='The training is what we’ll discuss in the next section, along with the code of the entire process.\\nThe Code\\nIn this section we’ll highlight the code to train this sentence classification model. A notebook containing all this code is available on colab and github.\\nLet’s start by importing the tools of the trade\\nimport numpy as np\\nimport pandas as pd\\nimport torch'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='import numpy as np\\nimport pandas as pd\\nimport torch\\nimport transformers as ppb # pytorch transformers\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"The dataset is available as a file on github, so we just import it directly into a pandas dataframe\\ndf = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\\\t', header=None)\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"We can use df.head() to look at the first five rows of the dataframe to see how the data looks.\\ndf.head()\\n\\nWhich outputs:\\n\\n\\n\\n\\nImporting pre-trained DistilBERT model and tokenizer\\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"## Want BERT instead of distilBERT? Uncomment the following line:\\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='# Load pretrained model/tokenizer\\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\\nmodel = model_class.from_pretrained(pretrained_weights)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='We can now tokenize the dataset. Note that we’re going to do things a little differently here from the example above. The example above tokenized and processed only one sentence. Here, we’ll tokenize and process all sentences together as a batch (the notebook processes a smaller group of examples just for resource considerations, let’s say 2000 examples).\\nTokenization'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Tokenization\\ntokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='This turns every sentence into the list of ids.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='The dataset is currently a list (or pandas Series/DataFrame) of lists. Before DistilBERT can process this as input, we’ll need to make all the vectors the same size by padding shorter sentences with the token id 0. You can refer to the notebook for the padding step, it’s basic python string and array manipulation.\\nAfter the padding, we have a matrix/tensor that is ready to be passed to BERT:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Processing with DistilBERT\\nWe now create an input tensor out of the padded token matrix, and send that to DistilBERT\\ninput_ids = torch.tensor(np.array(padded))\\n\\nwith torch.no_grad():\\n    last_hidden_states = model(input_ids)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='After running this step, last_hidden_states holds the outputs of DistilBERT. It is a tuple with the shape (number of examples, max number of tokens in the sequence, number of hidden units in the DistilBERT model). In our case, this will be 2000 (since we only limited ourselves to 2000 examples), 66 (which is the number of tokens in the longest sequence from the 2000 examples), 768 (the number of hidden units in the DistilBERT'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='number of tokens in the longest sequence from the 2000 examples), 768 (the number of hidden units in the DistilBERT model).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Unpacking the BERT output tensor\\nLet’s unpack this 3-d output tensor. We can first start by examining its dimensions:\\n\\n\\n\\n\\nRecapping a sentence’s journey\\nEach row is associated with a sentence from our dataset. To recap the processing path of the first sentence, we can think of it as looking like this:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Slicing the important part\\nFor sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='This is how we slice that 3d tensor to get the 2d tensor we’re interested in:\\n # Slice the output for the first position for all the sequences, take all hidden unit outputs\\nfeatures = last_hidden_states[0][:,0,:].numpy()\\n\\nAnd now features is a 2d numpy array containing the sentence embeddings of all the sentences in our dataset.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"The tensor we sliced from BERT's output\\n\\nDataset for Logistic Regression\\nNow that we have the output of BERT, we have assembled the dataset we need to train our logistic regression model. The 768 columns are the features, and the labels we just get from our initial dataset.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='The labeled dataset we use to train the Logistic Regression. The features are the output vectors of BERT for the [CLS] token (position #0) that we sliced in the previous figure. Each row corresponds to a sentence in our dataset, each column corresponds to the output of a hidden unit from the feed-forward neural network at the top transformer block of the Bert/DistilBERT model.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='After doing the traditional train/test split of machine learning, we can declare our Logistic Regression model and train it against the dataset.\\nlabels = df[1]\\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels)\\n\\n\\nWhich splits the dataset into training/testing sets:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Which splits the dataset into training/testing sets:\\n\\n\\n\\n\\nNext, we train the Logistic Regression model on the training set.\\nlr_clf = LogisticRegression()\\nlr_clf.fit(train_features, train_labels)\\n\\nNow that the model is trained, we can score it against the test set:\\nlr_clf.score(test_features, test_labels)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Which shows the model achieves around 81% accuracy.\\nScore Benchmarks'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='For reference, the highest accuracy score for this dataset is currently 96.8. DistilBERT can be trained to improve its score on this task – a process called fine-tuning which updates BERT’s weights to make it achieve a better performance in the sentence classification (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of 90.7. The full size BERT model achieves 94.9.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='The Notebook\\nDive right into the notebook or run it on colab.\\nAnd that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at fine-tuning. You can also go back and switch from distilBERT to BERT and see how that works.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Thanks to Clément Delangue, Victor Sanh, and the Huggingface team for providing feedback to earlier versions of this tutorial.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Written on November 26, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content='Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Blog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated GPT-2 (Visualizing Transformer Language Models)\\n\\nDiscussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post,'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Contents\\n\\n\\nPart 1: GPT2 And Language Modeling\\n\\nWhat is a Language Model\\nTransformers for Language Modeling\\nOne Difference From BERT\\nThe Evolution of The Transformer Block\\nCrash Course in Brain Surgery: Looking Inside GPT-2\\nA Deeper Look Inside\\nEnd of part #1: The GPT-2, Ladies and Gentlemen\\n\\n\\nPart 2: The Illustrated Self-Attention'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Part 2: The Illustrated Self-Attention\\n\\nSelf-Attention (without masking)\\n1- Create Query, Key, and Value Vectors\\n2- Score\\n3- Sum\\nThe Illustrated Masked Self-Attention\\nGPT-2 Masked Self-Attention\\nBeyond Language modeling\\nYou’ve Made it!\\n\\n\\nPart 3: Beyond Language Modeling\\n\\nMachine Translation\\nSummarization\\nTransfer Learning\\nMusic Generation'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Part #1: GPT2 And Language Modeling #\\nSo what exactly is a language model?\\nWhat is a Language Model\\nIn The Illustrated Word2vec, we’ve looked at what a language model is – basically a machine learning model that is able to look at part of a sentence and predict the next word. The most famous language models are smartphone keyboards that suggest the next word based on what you’ve currently typed.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest variant of the trained GPT-2, takes up 500MBs of storage to store all of its parameters. The largest GPT-2 variant is 13 times the size so it could take up more than 6.5 GBs of storage space.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='One great way to experiment with GPT-2 is using the AllenAI GPT-2 Explorer. It uses GPT-2 to display ten possible predictions for the next word (alongside their probability score). You can select a word then see the next list of predictions to continue writing the passage.\\nTransformers for Language Modeling'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Transformers for Language Modeling\\nAs we’ve seen in The Illustrated Transformer, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. That architecture was appropriate because the model tackled machine translation  – a problem where encoder-decoder architectures have been successful in the past.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='A lot of the subsequent research work saw the architecture shed either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible, feeding them massive amounts of training text, and throwing vast amounts of compute at them (hundreds of thousands of dollars to train some of these language models, likely millions in the case of AlphaStar).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='How high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:\\n\\n\\n\\n\\nOne Difference From BERT\\n\\nFirst Law of Robotics\\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let’s for example prompt a well-trained GPT-2 to recite the first law of robotics:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “auto-regression”. This is one of the ideas that made RNNs unreasonably effective.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The GPT2, and some later models like TransformerXL and XLNet are auto-regressive in nature. BERT is not. That is a trade off. In losing auto-regression, BERT gained the ability to incorporate the context on both sides of a word to gain better results. XLNet brings back autoregression while finding an alternative way to incorporate the context on both sides.\\nThe Evolution of the Transformer Block'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The Evolution of the Transformer Block\\nThe initial transformer paper introduced two types of transformer blocks:\\nThe Encoder Block\\nFirst is the encoder block:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content=\"An encoder block from the original transformer paper can take inputs up until a certain max sequence length (e.g. 512 tokens). It's okay if an input sequence is shorter than this limit, we can just pad the rest of the sequence.\\n\\nThe Decoder Block\\nSecond, there’s the decoder block which has a small architectural variation from the encoder block – a layer to allow it to pay attention to specific segments from the encoder:\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='One key difference in the self-attention layer here, is that it masks future tokens – not by changing the word to [mask] like BERT, but by interfering in the self-attention calculation blocking information from tokens that are to the right of the position being calculated.\\nIf, for example, we’re to highlight the path of position #4, we can see that it is only allowed to attend to the present and previous tokens:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='It’s important that the distinction between self-attention (what BERT uses) and masked self-attention (what GPT-2 uses) is clear. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The Decoder-Only Block\\nSubsequent to the original paper, Generating Wikipedia by Summarizing Long Sequences proposed another arrangement of the transformer block that is capable of doing language modeling. This model threw away the Transformer encoder. For that reason, let’s call the model the “Transformer-Decoder”. This early transformer-based language model was made up of a stack of six transformer decoder blocks:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The decoder blocks are identical. I have expanded the first one so you can see its self-attention layer is the masked variant. Notice that the model now can address up to 4,000 tokens in a certain segment -- a massive upgrade from the 512 in the original transformer.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='These blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in Character-Level Language Modeling with Deeper Self-Attention to create a language model that predicts one letter/character at a time.\\nThe OpenAI GPT-2 model uses these decoder-only blocks.\\nCrash Course in Brain Surgery: Looking Inside GPT-2'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Look inside and you will see,\\nThe words are cutting deep inside my brain.\\nThunder burning, quickly burning,\\nKnife of words is driving me insane, insane yeah.\\n~Budgie\\n\\nLet’s lay a trained GPT-2 on our surgery table and look at how it works.\\n\\n\\n\\n  The GPT-2 can process 1024 tokens. Each token flows through all the decoder blocks along its own path.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The simplest way to run a trained GPT-2 is to allow it to ramble on its own (which is technically called generating unconditional samples) – alternatively, we can give it a prompt to have it speak about a certain topic (a.k.a generating interactive conditional samples). In the rambling case, we can simply hand it the start token and have it start generating words (the trained model uses <|endoftext|> as its start token. Let’s call it <s>'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='it start generating words (the trained model uses <|endoftext|> as its start token. Let’s call it <s> instead).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The model only has one input token, so that path would be the only active one. The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model’s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2). In this case we selected the token with the highest probability, ‘the’. But we can certainly mix things up – you know'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='we selected the token with the highest probability, ‘the’. But we can certainly mix things up – you know how if you keep clicking the suggested word in your keyboard app, it sometimes can stuck in repetitive loops where the only way out is if you click the second or third suggested word. The same can happen here. GPT-2 has a parameter called top-k that we can use to have the model consider sampling words other than the top word (which is the case when'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='top-k that we can use to have the model consider sampling words other than the top word (which is the case when top-k = 1).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='In the next step, we add the output from the first step to our input sequence, and have the model make its next prediction:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Notice that the second path is the only one that’s active in this calculation. Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we’ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token.\\nA Deeper Look Inside\\nInput Encoding'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='A Deeper Look Inside\\nInput Encoding\\nLet’s look at more details to get to know the model more intimately. Let’s start from the input. As in other NLP models we’ve discussed before, the model looks up the embedding of the input word in its embedding matrix – one of the components we get as part of a trained model.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Each row is a word embedding: a list of numbers representing a word and capturing some of its meaning. The size of that list is different in different GPT2 model sizes. The smallest model uses an embedding size of 768 per word/token.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='So in the beginning, we look up the embedding of the start token <s> in the embedding matrix. Before handing that to the first block in the model, we need to incorporate positional encoding – a signal that indicates the order of the words in the sequence to the transformer blocks. Part of the trained model is a matrix that contains a positional encoding vector for each of the 1024 positions in the input.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='With this, we’ve covered how input words are processed before being handed to the first transformer block. We also know two of the weight matrices that constitute the trained GPT-2.\\n\\n\\n\\n  Sending a word to the first transformer block means looking up its embedding and adding up the positional encoding vector for position #1.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='A journey up the Stack\\nThe first block can now process the token by first passing it through the self-attention process, then passing it through its neural network layer. Once the first transformer block processes the token, it sends its resulting vector up the stack to be processed by the next block. The process is identical in each block, but each block has its own weights in both self-attention and the neural network sublayers.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Self-Attention Recap\\nLanguage heavily relies on context. For example, look at the second law:\\n\\nSecond Law of Robotics\\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='I have highlighted three places in the sentence where the words are referring to other words. There is no way to understand or process these words without incorporating the context they are referring to. When a model processes this sentence, it has to be able to know that:\\n\\nit refers to the robot\\nsuch orders refers to the earlier part of the law, namely “the orders given it by human beings”\\nThe First Law refers to the entire First Law'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='This is what self-attention does. It bakes in the model’s understanding of relevant and associated words that explain the context of a certain word before processing that word (passing it through a neural network). It does that by assigning scores to how relevant each word in the segment is, and adding up their vector representation.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='As an example, this self-attention layer in the top block is paying attention to “a robot” when it processes the word “it”. The vector it will pass to its neural network is a sum of the vectors for each of the three words multiplied by their scores.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Self-Attention Process\\nSelf-attention is processed along the path of each token in the segment. The significant components are three vectors:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Query: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.\\nKey: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Value: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='A crude analogy is to think of it like searching through a filing cabinet. The query is like a sticky note with the topic you’re researching. The keys are like the labels of the folders inside the cabinet. When you match the tag with a sticky note, we take out the contents of that folder, these contents are the value vector. Except you’re not only looking for one value, but a blend of values from a blend of folders.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Multiplying the query vector by each key vector produces a score for each folder (technically: dot product followed by softmax).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='We multiply each value by its score and sum up – resulting in our self-attention outcome.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='This weighted blend of value vectors results in a vector that paid 50% of its “attention” to the word robot, 30% to the word a, and 19% to the word it. Later in the post, we’ll got deeper into self-attention. But first, let’s continue our journey up the stack towards the output of the model.\\nModel Output'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Model Output\\nWhen the top block in the model produces its output vector (the result of its own self-attention followed by its own neural network), the model multiplies that vector by the embedding matrix.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Recall that each row in the embedding matrix corresponds to the embedding of a word in the model’s vocabulary. The result of this multiplication is interpreted as a score for each word in the model’s vocabulary.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='We can simply select the token with the highest score (top_k = 1). But better results are achieved if the model considers other words as well. So a better strategy is to sample a word from the entire list using the score as the probability of selecting that word (so words with a higher score have a higher chance of being selected). A middle ground is setting top_k to 40, and having the model consider the 40 words with the highest scores.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='With that, the model has completed an iteration resulting in outputting a single word. The model continues iterating until the entire context is generated (1024 tokens) or until an end-of-sequence token is produced.\\nEnd of part #1: The GPT-2, Ladies and Gentlemen'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='And there we have it. A run down of how the GPT2 works. If you’re curious to know exactly what happens inside the self-attention layer, then the following bonus section is for you. I created it to introduce more visual language to describe self-attention in order to make describing later transformer models easier to examine and describe (looking at you, TransformerXL and XLNet).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='I’d like to note a few oversimplifications in this post:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='I used “words” and “tokens” interchangeably. But in reality, GPT2 uses Byte Pair Encoding to create the tokens in its vocabulary. This means the tokens are usually parts of words.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The example we showed runs GPT2 in its inference/evaluation mode. That’s why it’s only processing one word at a time. At training time, the model would be trained against longer sequences of text and processing multiple tokens at once. Also at training time, the model would process larger batch sizes (512) vs. the batch size of one that evaluation uses.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='I took liberties in rotating/transposing vectors to better manage the spaces in the images. At implementation time, one has to be more precise.\\nTransformers use a lot of layer normalization, which is pretty important. We’ve noted a few of these in the Illustrated Transformer, but focused more on self-attention in this post.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='There are times when I needed to show more boxes to represent a vector. I indicate those as “zooming in”. For example:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Part #2: The Illustrated Self-Attention #\\nEarlier in the post we showed this image to showcase self-attention being applied in a layer that is processing the word it:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='In this section, we’ll look at the details of how that is done. Note that we’ll look at it in a way to try to make sense of what happens to individual words. That’s why we’ll be showing many single vectors. The actual implementations are done by multiplying giant matrices together. But I want to focus on the intuition of what happens on a word-level here.\\nSelf-Attention (without masking)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Self-Attention (without masking)\\nLet’s start by looking at the original self-attention as it’s calculated in an encoder block. Let’s look at a toy transformer block that can only process four tokens at a time.\\nSelf-attention is applied through three main steps:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Create the Query, Key, and Value vectors for each path.\\nFor each input token, use its query vector to score against all the other key vectors\\nSum up the value vectors after multiplying them by their associated scores.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='1- Create Query, Key, and Value Vectors\\nLet’s focus on the first path. We’ll take its query, and compare against all the keys. That produces a score for each key. The first step in self-attention is to calculate the three vectors for each token path (let’s ignore attention heads for now):'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='2- Score\\nNow that we have the vectors, we use the query and key vectors only for step #2. Since we’re focused on the first token, we multiply its query by all the other key vectors resulting in a score for each of the four tokens.\\n\\n\\n\\n\\n3- Sum\\nWe can now multiply the scores by the value vectors. A value with a high score will constitute a large portion of the resulting vector after we sum them up.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content=\"The lower the score, the more transparent we're showing the value vector. That's to indicate how multiplying by a small number dilutes the values of the vector.\\n\\nIf we do the same operation for each path, we end up with a vector representing each token containing the appropriate context of that token. Those are then presented to the next sublayer in the transformer block (the feed-forward neural network):\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The Illustrated Masked Self-Attention'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Now that we’ve looked inside a transformer’s self-attention step, let’s proceed to look at masked self-attention. Masked self-attention is identical to self-attention except when it comes to step #2. Assuming the model only has two tokens as input and we’re observing the second token. In this case, the last two tokens are masked. So the model interferes in the scoring step. It basically always'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='In this case, the last two tokens are masked. So the model interferes in the scoring step. It basically always scores the future tokens as 0 so the model can’t peak to future words:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='This masking is often implemented as a matrix called an attention mask. Think of a sequence of four words (“robot must obey orders”, for example). In a language modeling scenario, this sequence is absorbed in four steps – one per word (assuming for now that every word is a token). As these models work in batches, we can assume a batch size of 4 for this toy model that will process the entire sequence (with its four steps) as one batch.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='In matrix form, we calculate the scores by multiplying a queries matrix by a keys matrix. Let’s visualize it as follows, except instead of the word, there would be the query (or key) vector associated with that word in that cell:\\n\\n\\n\\n\\nAfter the multiplication, we slap on our attention mask triangle. It set the cells we want to mask to -infinity or a very large negative number (e.g. -1 billion in GPT2):'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Then, applying softmax on each row produces the actual scores we use for self-attention:\\n\\n\\n\\n\\nWhat this scores table means is the following:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='When the model processes the first example in the dataset (row #1), which contains only one word (“robot”), 100% of its attention will be on that word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='When the model processes the second example in the dataset (row #2), which contains the words (“robot must”), when it processes the word “must”, 48% of its attention will be on “robot”, and 52% of its attention will be on “must”.\\nAnd so on'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='GPT-2 Masked Self-Attention\\nLet’s get into more detail on GPT-2’s masked attention.\\nEvaluation Time: Processing One Token at a Time'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Evaluation Time: Processing One Token at a Time\\nWe can make the GPT-2 operate exactly as masked self-attention works. But during evaluation, when our model is only adding one new word after each iteration, it would be inefficient to recalculate self-attention along earlier paths for tokens that have already been processed.\\nIn this case, we process the first token (ignoring <s> for now).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='GPT-2 holds on to the key and value vectors of the the a token. Every self-attention layer holds on to its respective key and value vectors for that token:\\n\\n\\n\\n\\nNow in the next iteration, when the model processes the word robot, it does not need to generate query, key, and value queries for the a token. It just reuses the ones it saved from the first iteration:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='GPT-2 Self-attention: 1- Creating queries, keys, and values\\nLet’s assume the model is processing the word it. If we’re talking about the bottom block, then its input for that token would be the embedding of it + the positional encoding for slot #9:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Every block in a transformer has its own weights (broken down later in the post). The first we encounter is the weight matrix that we use to create the queries, keys, and values.\\n\\n\\n\\n  Self-attention multiplies its input by its weight matrix (and adds a bias vector, not illustrated here).\\n\\nThe multiplication results in a vector that’s basically a concatenation of the query, key, and value vectors for the word it.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Multiplying the input vector by the attention weights vector (and adding a bias vector aftwards) results in the key, value, and query vectors for this token.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='GPT-2 Self-attention: 1.5- Splitting into attention heads'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='In the previous examples, we dove straight into self-attention ignoring the “multi-head” part. It would be useful to shed some light on that concept now. Self attention is conducted multiple times on different parts of the Q,K,V vectors. “Splitting” attention heads is simply reshaping the long vector into a matrix. The small GPT2 has 12 attention heads, so that would be the first dimension of the reshaped matrix:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='In the previous examples, we’ve looked at what happens inside one attention head. One way to think of multiple attention-heads is like this (if we’re to only visualize three of the twelve attention heads):\\n\\n\\n\\n\\nGPT-2 Self-attention: 2- Scoring\\nWe can now proceed to scoring – knowing that we’re only looking at one attention head (and that all the others are conducting a similar operation):'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Now the token can get scored against all of keys of the other tokens (that were calculated in attention head #1 in previous iterations):\\n\\n\\n\\n\\nGPT-2 Self-attention: 3- Sum\\nAs we’ve seen before, we now multiply each value with its score, then sum them up, producing the result of self-attention for attention-head #1:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='GPT-2 Self-attention: 3.5- Merge attention heads\\nThe way we deal with the various attention heads is that we first concatenate them into one vector:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='But the vector isn’t ready to be sent to the next sublayer just yet. We need to first turn this Frankenstein’s-monster of hidden states into a homogenous representation.\\nGPT-2 Self-attention: 4- Projecting'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='GPT-2 Self-attention: 4- Projecting\\nWe’ll let the model learn how to best map concatenated self-attention results into a vector that the feed-forward neural network can deal with. Here comes our second large weight matrix that projects the results of the attention heads into the output vector of the self-attention sublayer:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='And with this, we have produced the vector we can send along to the next layer:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='GPT-2 Fully-Connected Neural Network: Layer #1'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The fully-connected neural network is where the block processes its input token after self-attention has included the appropriate context in its representation. It is made up of two layers. The first layer is four times the size of the model (Since GPT2 small is 768, this network would have 768*4 = 3072 units). Why four times? That’s just the size the original transformer rolled with (model dimension was 512 and layer #1 in that model was 2048).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='just the size the original transformer rolled with (model dimension was 512 and layer #1 in that model was 2048). This seems to give transformer models enough representational capacity to handle the tasks that have been thrown at them so far.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='(Not shown: A bias vector)\\n\\nGPT-2 Fully-Connected Neural Network: Layer #2 - Projecting to model dimension\\nThe second layer projects the result from the first layer back into model dimension (768 for the small GPT2). The result of this multiplication is the result of the transformer block for this token.\\n\\n\\n\\n  (Not shown: A bias vector)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='(Not shown: A bias vector)\\n\\nYou’ve Made It!\\nThat’s the most detailed version of the transformer block we’ll get into! You now pretty much have the vast majority of the picture of what happens inside of a transformer language model. To recap, our brave input vector encounters these weight matrices:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='And each block has its own set of these weights. On the other hand, the model has only one token embedding matrix and one positional encoding matrix:\\n\\n\\n\\n\\nIf you want to see all the parameters of the model, then I have tallied them here:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='They add up to 124M parameters instead of 117M for some reason. I’m not sure why, but that’s how many of them seems to be in the published code (please correct me if I’m wrong).\\nPart 3: Beyond Language Modeling #'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Part 3: Beyond Language Modeling #\\nThe decoder-only transformer keeps showing promise beyond language modeling. There are plenty of applications where it has shown success which can be described by similar visuals as the above. Let’s close this post by looking at some of these applications\\nMachine Translation\\nAn encoder is not required to conduct translation. The same task can be addressed by a decoder-only transformer:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Summarization\\nThis is the task that the first decoder-only transformer was trained on. Namely, it was trained to read a wikipedia article (without the opening section before the table of contents), and to summarize it. The actual opening sections of the articles were used as the labels in the training datasest:\\n\\n\\n\\n\\nThe paper trained the model against wikipedia articles, and thus the trained model was able to summarize articles:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Transfer Learning\\nIn Sample Efficient Text Summarization Using a Single Pre-Trained Transformer, a decoder-only transformer is first pre-trained on language modeling, then finetuned to do summarization. It turns out to achieve better results than a pre-trained encoder-decoder transformer in limited data settings.\\nThe GPT2 paper also shows results of summarization after pre-training the model on language modeling.\\nMusic Generation'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The GPT2 paper also shows results of summarization after pre-training the model on language modeling.\\nMusic Generation\\nThe Music Transformer uses a decoder-only transformer to generate music with expressive timing and dynamics. “Music Modeling” is just like language modeling – just let the model learn music in an unsupervised way, then have it sample outputs (what we called “rambling”, earlier).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='You might be curious as to how music is represented in this scenario. Remember that language modeling can be done through vector representations of either characters, words, or tokens that are parts of words. With a musical performance (let’s think about the piano for now), we have to represent the notes, but also velocity – a measure of how hard the piano key is pressed.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='A performance is just a series of these one-hot vectors. A midi file can be converted into such a format. The paper has the following example input sequence:\\n\\n\\n\\n\\nThe one-hot vector representation for this input sequence would look like this:\\n\\n\\n\\n\\nI love a visual in the paper that showcases self-attention in the Music Transformer. I’ve added some annotations to it here:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='\"Figure 8: This piece has a recurring triangular contour. The query is at one of the latter peaks and it attends to all of the previous high notes on the peak, all the way to beginning of the piece.\" ... \"[The] figure shows a query (the source of all the attention lines) and previous memories being attended to (the notes that are receiving more softmax probabiliy is highlighted in). The coloring of the attention lines correspond to different heads and the width'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='receiving more softmax probabiliy is highlighted in). The coloring of the attention lines correspond to different heads and the width to the weight of the softmax probability.\"'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='If you’re unclear on this representation of musical notes, check out this video.\\nConclusion\\nThis concludes our journey into the GPT2, and our exploration of its parent model, the decoder-only transformer. I hope that you come out of this post with a better understanding of self-attention and more comfort that you understand more of what goes on inside a transformer.\\nResources'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='The GPT2 Implementation from OpenAI\\nCheck out the pytorch-transformers library from Hugging Face in addition to GPT2, it implements BERT, Transformer-XL, XLNet and other cutting-edge transformer models.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Acknowledgements\\nThanks to Lukasz Kaiser, Mathias Müller, Peter J. Liu, Ryan Sepassi and Mohammad Saleh for feedback on earlier versions of this post.\\nComments or corrections? Please tweet me at @JayAlammar\\n\\n\\n\\n    Written on August 12, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-gpt2/', 'title': 'The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments)\\n\\n\\nTranslations: Simplified Chinese, French, Korean, Russian, Turkish\\n\\n\\n  \\n  \\n\\n\\nThis year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.\\n\\nMy goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.\\n\\n', 'language': 'No language found.'}, page_content='Attribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Word2vec'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Blog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Word2vec\\n\\n Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='“There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='I find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Word2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='In this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Personality Embeddings: What are you like?\\n\\n“I give you the desert chameleon, whose ability to blend itself into the background tells you all you need to know about the roots of ecology and the foundations of a personal identity” ~Children of Dune'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='On a scale of 0 to 100, how introverted/extraverted are you (where 0 is the most introverted, and 100 is the most extraverted)?\\nHave you ever taken a personality test like MBTI – or even better, the Big Five Personality Traits test? If you haven’t, these are tests that ask you a list of questions, then score you on a number of axes, introversion/extraversion being one of them.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Example of the result of a Big Five Personality Trait test. It can really tell you a lot about yourself and is shown to have predictive ability in academic, personal, and professional success. This is one place to find your results.\\n\\nImagine I’ve scored 38/100 as my introversion/extraversion score. we can plot that in this way:\\n\\n\\n\\nLet’s switch the range to be from -1 to 1:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Let’s switch the range to be from -1 to 1:\\n\\n\\n\\nHow well do you feel you know a person knowing only this one piece of information about them? Not much. People are complex. So let’s add another dimension – the score of one other trait from the test.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='We can represent the two dimensions as a point on the graph, or better yet, as a vector from the origin to that point. We have incredible tools to deal with vectors that will come in handy very shortly.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='I’ve hidden which traits we’re plotting just so you get used to not knowing what each dimension represents – but still getting a lot of value from the vector representation of a person’s personality.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='We can now say that this vector partially represents my personality. The usefulness of such representation comes when you want to compare two other people to me. Say I get hit by a bus and I need to be replaced by someone with a similar personality. In the following figure, which of the two people is more similar to me?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='When dealing with vectors, a common way to calculate a similarity score is cosine_similarity:\\n\\n\\n\\nPerson #1 is more similar to me in personality. Vectors pointing at the same direction (length plays a role as well) have a higher cosine similarity score.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Yet again, two dimensions aren’t enough to capture enough information about how different people are. Decades of psychology research have led to five major traits (and plenty of sub-traits). So let’s use all five dimensions in our comparison:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='The problem with five dimensions is that we lose the ability to draw neat little arrows in two dimensions. This is a common challenge in machine learning where we often have to think in higher-dimensional space. The good thing is, though, that cosine_similarity still works. It works with any number of dimensions:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content=\"cosine_similarity works for any number of dimensions. These are much better scores because they're calculated based on a higher resolution representation of the things being compared.\\n\\nAt the end of this section, I want us to come out with two central ideas:\\n\\nWe can represent people (and things) as vectors of numbers (which is great for machines!).\\nWe can easily calculate how similar vectors are to each other.\\n\\n\\n\\n\\n\\nWord Embeddings\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Word Embeddings\\n\\n“The gift of words is the gift of deception and illusion” ~Children of Dune\\n\\nWith this understanding, we can proceed to look at trained word-vector examples (also called word embeddings) and start looking at some of their interesting properties.\\nThis is a word embedding for the word “king” (GloVe vector trained on Wikipedia):'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='[ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\\n -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='-0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\\n -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\\n  1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\\n -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\\n -0.64426 , -0.51042 ]'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='-0.64426 , -0.51042 ]\\n \\nIt’s a list of 50 numbers. We can’t tell much by looking at the values. But let’s visualize it a bit so we can compare it other word vectors. Let’s put all these numbers in one row:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Let’s color code the cells based on their values (red if they’re close to 2, white if they’re close to 0, blue if they’re close to -2):\\n\\n\\n\\n\\nWe’ll proceed by ignoring the numbers and only looking at the colors to indicate the values of the cells. Let’s now contrast “King” against other words:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='See how “Man” and “Woman” are much more similar to each other than either of them is to “king”? This tells you something. These vector representations capture quite a bit of the information/meaning/associations of these words.\\nHere’s another list of examples (compare by vertically scanning the columns looking for columns with similar colors):\\n\\n\\n\\n\\nA few things to point out:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='There’s a straight red column through all of these different words. They’re similar along that dimension (and we don’t know what each dimensions codes for)\\nYou can see how “woman” and “girl” are similar to each other in a lot of places. The same with “man” and “boy”'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='“boy” and “girl” also have places where they are similar to each other, but different from “woman” or “man”. Could these be coding for a vague conception of youth? possible.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='All but the last word are words representing people. I added an object (water) to show the differences between categories. You can, for example, see that blue column going all the way down and stopping before the embedding for “water”.\\nThere are clear places where “king” and “queen” are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Analogies\\n\\n\"Words can carry any burden we wish. All that\\'s required is agreement and a tradition upon which to build.\" ~God Emperor of Dune\\n\\nThe famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Using the Gensim library in python, we can add and subtract word vectors, and it would find the most similar words to the resulting vector. The image shows a list of the most similar words, each with its cosine similarity.\\n\\nWe can visualize this analogy as we did previously:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='We can visualize this analogy as we did previously:\\n\\n\\n\\n  The resulting vector from \"king-man+woman\" doesn\\'t exactly equal \"queen\", but \"queen\" is the closest word to it from the 400,000 word embeddings we have in this collection.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Now that we’ve looked at trained word embeddings, let’s learn more about the training process. But before we get to word2vec, we need to look at a conceptual parent of word embeddings: the neural language model.\\nLanguage Modeling'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='“The prophet is not diverted by illusions of past, present and future. The fixity of language determines such linear distinctions. Prophets hold a key to the lock in a language.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This is not a mechanical universe. The linear progression of events is imposed by the observer. Cause and effect? That\\'s not it at all. The prophet utters fateful words. You glimpse a thing \"destined to occur.\" But the prophetic instant releases something of infinite portent and power. The universe undergoes a ghostly shift.” ~God Emperor of Dune'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='If one wanted to give an example of an NLP application, one of the best examples would be the next-word prediction feature of a smartphone keyboard. It’s a feature that billions of people use hundreds of times every day.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Next-word prediction is a task that can be addressed by a language model. A language model can take a list of words (let’s say two words), and attempt to predict the word that follows them.\\nIn the screenshot above, we can think of the model as one that took in these two green words (thou shalt) and returned a list of suggestions (“not” being the one with the highest probability):'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='We can think of the model as looking like this black box:\\n\\n\\n\\n\\n\\n\\nBut in practice, the model doesn’t output only one word. It actually outputs a probability score for all the words it knows (the model’s “vocabulary”, which can range from a few thousand to over a million words). The keyboard application then has to find the words with the highest scores, and present those to the user.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content=\"The output of the neural language model is a probability score for all the words the model knows. We're referring to the probability as a percentage here, but 40% would actually be represented as 0.4 in the output vector.\\n\\n\\nAfter being trained, early neural language models (Bengio 2003) would calculate a prediction in three steps:\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='The first step is the most relevant for us as we discuss embeddings. One of the results of the training process was this matrix that contains an embedding for each word in our vocabulary. During prediction time, we just look up the embeddings of the input word, and use them to calculate the prediction:\\n\\n\\n\\n\\nLet’s now turn to the training process to learn more about how this embedding matrix was developed.\\nLanguage Model Training'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='“A process cannot be understood by stopping it. Understanding must move with the flow of the process, must join it and flow with it.” ~Dune'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Language models have a huge advantage over most other machine learning models. That advantage is that we are able to train them on running text – which we have an abundance of. Think of all the books, articles, Wikipedia content, and other forms of text data we have lying around. Contrast this with a lot of other machine learning models which need hand-crafted features and specially-collected data.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='“You shall know a word by the company it keeps” J.R. Firth\\n\\nWords get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='We get a lot of text data (say, all Wikipedia articles, for example). then\\nWe have a window (say, of three words) that we slide against all of that text.\\nThe sliding window generates training samples for our model'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='As this window slides against the text, we (virtually) generate a dataset that we use to train a model. To look exactly at how that’s done, let’s see how the sliding window processes this phrase:\\n\\n“Thou shalt not make a machine in the likeness of a human mind” ~Dune\\n\\nWhen we start, the window is on the first three words of the sentence:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='When we start, the window is on the first three words of the sentence:\\n\\n\\n\\n\\n\\n\\nWe take the first two words to be features, and the third word to be a label:\\n\\n\\n\\n\\n  We now have generated the first sample in the dataset we can later use to train a language model.\\n\\n\\nWe then slide our window to the next position and create a second sample:\\n\\n\\n\\n\\n  An the second example is now generated.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='An the second example is now generated.\\n\\n\\nAnd pretty soon we have a larger dataset of which words tend to appear after different pairs of words:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='In practice, models tend to be trained while we’re sliding the window. But I find it clearer to logically separate the “dataset generation” phase from the training phase. Aside from neural-network-based approaches to language modeling, a technique called N-grams was commonly used to train language models (see: Chapter 3 of Speech and Language Processing). To see how this switch from N-grams to neural models reflects on real-world products,'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Speech and Language Processing). To see how this switch from N-grams to neural models reflects on real-world products, here’s a 2015 blog post from Swiftkey, my favorite Android keyboard, introducing their neural language model and comparing it with their previous N-gram model. I like this example because it shows you how the algorithmic properties of embeddings can be described in marketing speech.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Look both ways'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='\"Paradox is a pointer telling you to look beyond it. If paradoxes bother you, that betrays your deep desire for absolutes. The relativist treats a paradox merely as interesting, perhaps amusing or even, dreadful thought, educational.\" ~God Emperor of Dune\\n\\nKnowing what you know from earlier in the post, fill in the blank:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Knowing what you know from earlier in the post, fill in the blank:\\n\\n\\n\\n\\nThe context I gave you here is five words before the blank word (and an earlier mention of “bus”). I’m sure most people would guess the word bus goes into the blank. But what if I gave you one more piece of information – a word after the blank, would that change your answer?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This completely changes what should go in the blank. the word red is now the most likely to go into the blank. What we learn from this is the words both before and after a specific word carry informational value. It turns out that accounting for both directions (words to the left and to the right of the word we’re guessing) leads to better word embeddings. Let’s see how we can adjust the way we’re training the model to account for'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='embeddings. Let’s see how we can adjust the way we’re training the model to account for this.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Skipgram'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='“Intelligence takes chance with limited data in an arena where mistakes are not only possible but also necessary.” ~Chapterhouse: Dune\\n\\nInstead of only looking two words before the target word, we can also look at two words after it.\\n\\n\\n\\n\\nIf we do this, the dataset we’re virtually building and training the model against would look like this:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This is called a Continuous Bag of Words architecture and is described in one of the word2vec papers [pdf]. Another architecture that also tended to show great results does things a little differently.\\nInstead of guessing a word based on its context (the words before and after it), this other architecture tries to guess neighboring words using the current word. We can think of the window it slides against the training text as looking like this:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='The word in the green slot would be the input word, each pink box would be a possible output.\\n\\nThe pink boxes are in different shades because this sliding window actually creates four separate samples in our training dataset:\\n\\n\\n\\n\\n\\n\\nThis method is called the skipgram architecture. We can visualize the sliding window as doing the following:\\n\\n\\n\\n\\n\\n\\nThis would add these four samples to our training dataset:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This would add these four samples to our training dataset:\\n\\n\\n\\n\\n\\nWe then slide our window to the next position:\\n\\n\\n\\n\\n\\n\\nWhich generates our next four examples:\\n\\n\\n\\n\\n\\nA couple of positions later, we have a lot more examples:\\n\\n\\n\\n\\n\\nRevisiting the training process'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='A couple of positions later, we have a lot more examples:\\n\\n\\n\\n\\n\\nRevisiting the training process\\n\\n  \"Muad\\'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It\\'s shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\" ~Dune'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Now that we have our skipgram training dataset that we extracted from existing running text, let’s glance at how we use it to train a basic neural language model that predicts the neighboring word.\\n\\n\\n\\n\\n\\nWe start with the first sample in our dataset. We grab the feature and feed to the untrained model asking it to predict an appropriate neighboring word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='The model conducts the three steps and outputs a prediction vector (with a probability assigned to each word in its vocabulary). Since the model is untrained, it’s prediction is sure to be wrong at this stage. But that’s okay. We know what word it should have guessed – the label/output cell in the row we’re currently using to train the model:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content=\"The 'target vector' is one where the target word has the probability 1, and all other words have the probability 0.\\n\\n\\nHow far off was the model? We subtract the two vectors resulting in an error vector:\\n\\n\\n\\n\\n\\n\\nThis error vector can now be used to update the model so the next time, it’s a little more likely to guess thou when it gets not as input.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='And that concludes the first step of the training. We proceed to do the same process with the next sample in our dataset, and then the next, until we’ve covered all the samples in the dataset. That concludes one epoch of training. We do it over again for a number of epochs, and then we’d have our trained model and we can extract the embedding matrix from it and use it for any other application.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='While this extends our understanding of the process, it’s still not how word2vec is actually trained. We’re missing a couple of key ideas.\\nNegative Sampling'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content=\"“To attempt an understanding of Muad'Dib without understanding his mortal enemies, the Harkonnens, is to attempt seeing Truth without knowing Falsehood. It is the attempt to see the Light without knowing Darkness. It cannot be.” ~Dune\\n\\nRecall the three steps of how this neural language model calculates its prediction:\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Recall the three steps of how this neural language model calculates its prediction:\\n\\n\\n\\n\\n\\n\\nThe third step is very expensive from a computational point of view – especially knowing that we will do it once for every training sample in our dataset (easily tens of millions of times). We need to do something to improve performance.\\nOne way is to split our target into two steps:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Generate high-quality word embeddings (Don’t worry about next-word prediction).\\nUse these high-quality embeddings to train a language model (to do next-word prediction).\\n\\nWe’ll focus on step 1. in this post as we’re focusing on embeddings. To generate high-quality embeddings using a high-performance model, we can switch the model’s task from predicting a neighboring word:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='And switch it to a model that takes the input and output word, and outputs a score indicating if they’re neighbors or not (0 for “not neighbors”, 1 for “neighbors”).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This simple switch changes the model we need from a neural network, to a logistic regression model – thus it becomes much simpler and much faster to calculate.\\nThis switch requires we switch the structure of our dataset – the label is now a new column with values 0 or 1. They will be all 1 since all the words we added are neighbors.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This can now be computed at blazing speed – processing millions of examples in minutes. But there’s one loophole we need to close. If all of our examples are positive (target: 1), we open ourself to the possibility of a smartass model that always returns 1 – achieving 100% accuracy, but learning nothing and generating garbage embeddings.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='To address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors.  Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.\\n\\n\\n\\n  \\n  For each sample in our dataset, we add negative examples. Those have the same input word, and a 0 label.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='But what do we fill in as output words? We randomly sample words from our vocabulary'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This idea is inspired by Noise-contrastive estimation [pdf]. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency.\\nSkipgram with Negative Sampling (SGNS)\\nWe have now covered two of the central ideas in word2vec: as a pair, they’re called skipgram with negative sampling.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Word2vec Training Process\\n\\n\"The machine cannot anticipate every problem of importance to humans. It is the difference between serial bits and an unbroken continuum. We have the one; machines are confined to the other.\" ~God Emperor of Dune'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Now that we’ve established the two central ideas of skipgram and negative sampling, we can proceed to look closer at the actual word2vec training process.\\nBefore the training process starts, we pre-process the text we’re training the model against. In this step, we determine the size of our vocabulary (we’ll call this vocab_size, think of it as, say, 10,000) and which words belong to it.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='At the start of the training phase, we create two matrices – an Embedding matrix and a Context matrix. These two matrices have an embedding for each word in our vocabulary (So vocab_size is one of their dimensions). The second dimension is how long we want each embedding to be (embedding_size – 300 is a common value, but we’ve looked at an example of 50 earlier in this post).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='At the start of the training process, we initialize these matrices with random values. Then we start the training process. In each training step, we take one positive example and its associated negative examples. Let’s take our first group:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Now we have four words: the input word not and output/context words: thou (the actual neighbor), aaron, and taco (the negative examples). We proceed to look up their embeddings – for the input word, we look in the Embedding matrix. For the context words, we look in the Context matrix (even though both matrices have an embedding for every word in our vocabulary).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Then, we take the dot product of the input embedding with each of the context embeddings. In each case, that would result in a number, that number indicates the similarity of the input and context embeddings\\n\\n\\n\\n\\nNow we need a way to turn these scores into something that looks like probabilities – we need them to all be positive and have values between zero and one. This is a great task for sigmoid, the logistic operation.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='And we can now treat the output of the sigmoid operations as the model’s output for these examples. You can see that taco has the highest score and aaron still has the lowest score both before and after the sigmoid operations.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Now that the untrained model has made a prediction, and seeing as though we have an actual target label to compare against, let’s calculate how much error is in the model’s prediction. To do that, we just subtract the sigmoid scores from the target labels.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='error = target - sigmoid_scores\\n\\n\\nHere comes the “learning” part of “machine learning”. We can now use this error score to adjust the embeddings of not, thou, aaron, and taco so that the next time we make this calculation, the result would be closer to the target scores.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='This concludes the training step. We emerge from it with slightly better embeddings for the words involved in this step (not, thou, aaron, and taco). We now proceed to our next step (the next positive sample and its associated negative samples) and do the same process again.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='The embeddings continue to be improved while we cycle through our entire dataset for a number of times. We can then stop the training process, discard the Context matrix, and use the Embeddings matrix as our pre-trained embeddings for the next task.\\nWindow Size and Number of Negative Samples\\nTwo key hyperparameters in the word2vec training process are the window size and the number of negative samples.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Different tasks are served better by different window sizes. One heuristic is that smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we’re only looking at their surrounding words – e.g. good and bad often appear in similar contexts). Larger window sizes (15-50, or even more) lead to embeddings'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='bad often appear in similar contexts). Larger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words. In practice, you’ll often have to provide annotations that guide the embedding process leading to a useful similarity sense for your task. The Gensim default window size is 5 (five words before and five words after the input word, in addition to the input word itself).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples.\\nConclusion\\n\\n“If it falls outside your yardsticks, then you are engaged with intelligence, not with automation”  ~God Emperor of Dune'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='I hope that you now have a sense for word embeddings and the word2vec algorithm. I also hope that now when you read a paper mentioning “skip gram with negative sampling” (SGNS) (like the recommendation system papers at the top), that you have a better sense for these concepts. As always, all feedback is appreciated @JayAlammar.\\nReferences & Further Readings'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Distributed Representations of Words and Phrases and their Compositionality [pdf]\\nEfficient Estimation of Word Representations in Vector Space [pdf]\\nA Neural Probabilistic Language Model [pdf]\\nSpeech and Language Processing by Dan Jurafsky and James H. Martin is a leading resource for NLP. Word2vec is tackled in Chapter 6.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Neural Network Methods in Natural Language Processing by Yoav Goldberg is a great read for neural NLP topics.\\nChris McCormick has written some great blog posts about Word2vec. He also just released The Inner Workings of word2vec, an E-book focused on the internals of word2vec.\\nWant to read the code? Here are two options:\\n    \\nGensim’s python implementation of word2vec'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Gensim’s python implementation of word2vec\\nMikolov’s original implementation in C – better yet, this version with detailed comments from Chris McCormick.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Evaluating distributional models of compositional semantics\\nOn word embeddings, part 2\\nDune\\n\\n\\n\\n\\n\\n\\n\\n    Written on March 27, 2019\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-word2vec/', 'title': 'The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': ' Discussions:\\nHacker News (347 points, 37 comments), Reddit r/MachineLearning (151 points, 19 comments)\\n\\n\\n\\nTranslations: Chinese (Simplified), French, Korean, Portuguese, Russian\\n\\n\\n\\n  \\n  \\n\\n\\n\\n  “There is in all things a pattern that is part of our universe. It has symmetry, elegance, and grace - those qualities you find always in that which the true artist captures. You can find it in the turning of the seasons, in the way sand trails along a ridge, in the branch clusters of the creosote\\n  bush or the pattern of its leaves. \\n\\n  We try to copy these patterns in our lives and our society,\\n  seeking the rhythms, the dances, the forms that comfort.\\n  Yet, it is possible to see peril in the finding of\\n  ultimate perfection. It is clear that the ultimate\\n  pattern contains it own fixity. In such\\n  perfection, all things move toward death.”\\n  ~ Dune (1965)\\n\\n\\nI find the concept of embeddings to be one of the most fascinating ideas in machine learning. If you’ve ever used Siri, Google Assistant, Alexa, Google Translate, or even smartphone keyboard with next-word prediction, then chances are you’ve benefitted from this idea that has become central to Natural Language Processing models. There has been quite a development over the last couple of decades in using embeddings for neural models (Recent developments include contextualized word embeddings leading to cutting-edge models like BERT and GPT2).\\n\\n\\n\\nWord2vec is a method to efficiently create word embeddings and has been around since 2013. But in addition to its utility as a word-embedding method, some of its concepts have been shown to be effective in creating recommendation engines and making sense of sequential data even in commercial, non-language tasks. Companies like Airbnb, Alibaba, Spotify, and Anghami have all benefitted from carving out this brilliant piece of machinery from the world of NLP and using it in production to empower a new breed of recommendation engines.\\n\\nIn this post, we’ll go over the concept of embedding, and the mechanics of generating embeddings with word2vec. But let’s start with an example to get familiar with using vectors to represent things. Did you know that a list of five numbers (a vector) can represent so much about your personality?\\n\\n', 'language': 'No language found.'}, page_content='Attribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Blog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)\\n\\nDiscussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n2021 Update: I created this brief and highly accessible video intro to BERT'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='(ULM-FiT has nothing to do with Cookie Monster. But I couldn’t think of anything else..)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='One of the latest milestones in this development is the release of BERT, an event described as marking the beginning of a new era in NLP. BERT is a model that broke several records for how well models can handle language-based tasks. Soon after the release of the paper describing the model, the team also open-sourced the code of the model, and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='and made available for download versions of the model that were already pre-trained on massive datasets. This is a momentous development since it enables anyone building a machine learning model involving language processing to use this powerhouse as a readily-available component – saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The two steps of how BERT is developed. You can download the model pre-trained in step 1 (trained on un-annotated data), and only worry about fine-tuning it for step 2. [Source for book icon].'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='BERT builds on top of a number of clever ideas that have been bubbling up in the NLP community recently – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='There are a number of concepts one needs to be aware of to properly wrap one’s head around what BERT is. So let’s start by looking at ways you can use BERT before looking at the concepts involved in the model itself.\\nExample: Sentence Classification\\nThe most straight-forward way to use BERT is to use it to classify a single piece of text. This model would look like this:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='To train such a model, you mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning, and has roots in Semi-supervised Sequence Learning and ULMFiT.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='For people not versed in the topic, since we’re talking about classifiers, then we are in the supervised-learning domain of machine learning. Which would mean we need a labeled dataset to train such a model. For this spam classifier example, the labeled dataset would be a list of email messages and a label (“spam” or “not spam” for each message).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Other examples for such a use-case include:\\n\\nSentiment analysis\\n\\nInput: Movie/Product review. Output: is the review positive or negative?\\nExample dataset: SST\\n\\n\\nFact-checking'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Fact-checking\\n\\nInput: sentence. Output: “Claim” or “Not Claim”\\nMore ambitious/futuristic example:\\n        \\nInput: Claim sentence. Output: “True” or “False”'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Full Fact is an organization building automatic fact-checking tools for the benefit of the public. Part of their pipeline is a classifier that reads news articles and detects claims (classifies text as either “claim” or “not claim”) which can later be fact-checked (by humans now, with ML later, hopefully).\\nVideo: Sentence embeddings for automated factchecking - Lev Konstantinovskiy.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Model Architecture\\nNow that you have an example use-case in your head for how BERT can be used, let’s take a closer look at how it works.\\n\\nThe paper presents two model sizes for BERT:\\n\\nBERT BASE – Comparable in size to the OpenAI Transformer in order to compare performance\\nBERT LARGE – A ridiculously huge model which achieved the state of the art results reported in the paper'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='BERT is basically a trained Transformer Encoder stack. This is a good time to direct you to read my earlier post The Illustrated Transformer which explains the Transformer model – a foundational concept for BERT and the concepts we’ll discuss next.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the default configuration in the reference implementation of the Transformer in the initial paper (6 encoder layers, 512 hidden units, and 8 attention heads).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Model Inputs'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.\\nJust like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='In terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge.\\nModel Outputs'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Model Outputs\\nEach position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we’ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='If you have more labels (for example if you’re an email service that tags emails with “spam”, “not spam”, “social”, and “promotion”), you just tweak the classifier network to have more output neurons that then pass through softmax.\\nParallels with Convolutional Nets'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Parallels with Convolutional Nets\\nFor those with a background in computer vision, this vector hand-off should be reminiscent of what happens between the convolution part of a network like VGGNet and the fully-connected classification portion at the end of the network.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='A New Age of Embedding\\nThese new developments carry with them a new shift in how words are encoded. Up until now, word-embeddings have been a major force in how leading NLP models deal with language. Methods like Word2Vec and Glove have been widely used for such tasks. Let’s recap how those are used before pointing to what has now changed.\\nWord Embedding Recap'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='For words to be processed by machine learning models, they need some form of numeric representation that models can use in their calculation. Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The field quickly realized it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset.  So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe. This is an example of the GloVe embedding of the word “stick” (with an embedding vector size'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='is an example of the GloVe embedding of the word “stick” (with an embedding vector size of 200)'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The GloVe word embedding of the word \"stick\" - a vector of 200 floats (rounded to two decimals). It goes on for two hundred values.\\n\\nSince these are large and full of numbers, I use the following basic shape in the figures in my posts to show vectors:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='ELMo: Context Matters'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='If we’re using this GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was. “Wait a minute” said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. And so, contextualized word-embeddings were born.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Contextualized word-embeddings can give words different embeddings based on the meaning they carry in the context of the sentence. Also, RIP Robin Williams\\n\\nInstead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.\\nWhat’s ELMo’s secret?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='What’s ELMo’s secret?\\nELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding process after this pre-training is done.\\nELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.\\n\\n\\n\\nGreat slides on ELMo'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Great slides on ELMo\\n\\nELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='ULM-FiT: Nailing down Transfer Learning in NLP\\nULM-FiT introduced methods to effectively utilize a lot of what the model learns during pre-training – more than just embeddings, and more than contextualized embeddings. ULM-FiT introduced a language model and a process to effectively fine-tune that language model for various tasks.\\nNLP finally had a way to do transfer learning probably as well as Computer Vision could.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='NLP finally had a way to do transfer learning probably as well as Computer Vision could.\\nThe Transformer: Going beyond LSTMs\\nThe release of the Transformer paper and code, and the results it achieved on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs. This was compounded by the fact that Transformers deal with long-term dependancies better than LSTMs.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The Encoder-Decoder structure of the transformer made it perfect for machine translation. But how would you use it for sentence classification? How would you use it to pre-train a language model that can be fine-tuned for other tasks (downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component).\\nOpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The OpenAI Transformer is made up of the decoder stack from the Transformer'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets,'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The OpenAI Transformer is now ready to be trained to predict the next word on a dataset made up of 7,000 books.\\n\\nTransfer Learning to Downstream Tasks\\nNow that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='How to use a pre-trained OpenAI transformer to do sentence clasification\\n\\nThe OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Isn’t that clever?\\nBERT: From Decoders to Encoders'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='“Hold my beer”, said R-rated BERT.\\nMasked Language Model\\n“We’ll use transformer encoders”, said BERT.\\n“This is madness”, replied Ernie, “Everybody knows bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context.”\\n“We’ll use masks”, said BERT confidently.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content=\"BERT's clever language modeling task masks 15% of words in the input and asks the model to predict the missing word.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a “masked language model” concept from earlier literature (where it’s called a Cloze task).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.\\nTwo-sentence Tasks'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Two-sentence Tasks\\nIf you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The second task BERT is pre-trained on is a two-sentence classification task. The tokenization is oversimplified in this graphic as BERT actually uses WordPieces as tokens rather than words --- so some words are broken down into smaller chunks.\\n\\nTask specific-Models\\nThe BERT paper shows a number of ways to use BERT for different tasks.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='BERT for feature extraction\\nThe fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Take BERT out for a spin\\nThe best way to try out BERT is through the BERT FineTuning with Cloud TPUs notebook hosted on Google Colab. If you’ve never used Cloud TPUs before, this is also a good starting point to try them as well as the BERT code works on TPUs, CPUs and GPUs as well.\\nThe next step would be to look at the code in the BERT repo:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='The model is constructed in modeling.py (class BertModel) and is pretty much identical to a vanilla Transformer encoder.\\n\\nrun_classifier.py is an example of the fine-tuning process. It also constructs the classification layer for the supervised model. If you want to construct your own classifier, check out the create_model() method in that file.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Several pre-trained models are available for download. These span BERT Base and BERT Large, as well as languages such as English, Chinese, and a multi-lingual model covering 102 languages trained on wikipedia.\\n\\nBERT doesn’t look at words as tokens. Rather, it looks at WordPieces. tokenization.py is the tokenizer that would turns your words into wordPieces appropriate for BERT.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='You can also check out the PyTorch implementation of BERT. The AllenNLP library uses this implementation to allow using BERT embeddings with any model.\\nAcknowledgements\\nThanks to Jacob Devlin, Matt Gardner, Kenton Lee,  Mark Neumann, and Matthew Peters for providing feedback on earlier drafts of this post.\\n\\n\\n    Written on December  3, 2018'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Written on December  3, 2018\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-bert/', 'title': 'The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)\\n\\n\\nTranslations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish\\n\\n2021 Update: I created this brief and highly accessible video intro to BERT\\n\\n\\n\\n\\n\\nThe year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).\\n\\n\\n  \\n\\n\\n\\n', 'language': 'No language found.'}, page_content='Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube \\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Transformer'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Blog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Transformer\\n\\nDiscussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Watch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content=\"Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n\\n\\n\\nUpdate: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='A High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n\\n\\nPopping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.\\n\\n\\n\\nThe encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.\\nThe outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Bringing The Tensors Into The Picture\\nNow that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.\\nAs is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content=\"Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.\\nNow We’re Encoding!'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Now We’re Encoding!\\nAs we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Self-Attention at a High Level\\nDon’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.\\nSay the following sentence is an input sentence we want to translate:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content=\"Say the following sentence is an input sentence we want to translate:\\n”The animal didn't cross the street because it was too tired”\\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\".'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\\nSelf-Attention in Detail\\nLet’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Multiplying x1 by the WQ weight matrix produces q1, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\\n\\n\\n\\nWhat are the “query”, “key”, and “value” vectors?'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.\\nMatrix Calculation of Self-Attention'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Matrix Calculation of Self-Attention\\nThe first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)\\n\\n\\nFinally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The self-attention calculation in matrix form\\n\\n\\n\\nThe Beast With Many Heads\\nThe paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\\nHow do we do that? We concat the matrices then multiply them by an additional weights matrix WO.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place\\n\\n\\n\\n\\n\\n\\nNow that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='As we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model\\'s representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\".\\n\\n\\nIf we add all the attention heads to the picture, however, things can be harder to interpret:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Representing The Order of The Sequence Using Positional Encoding\\nOne thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.\\n\\n\\nIf we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:\\n\\n\\n\\n  A real example of positional encoding with a toy embedding size of 4'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='A real example of positional encoding with a toy embedding size of 4\\n\\n\\nWhat might this pattern look like?\\nIn the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content=\"A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d(). This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).\\nJuly 2020 Update:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='July 2020 Update: \\nThe positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The Residuals\\nOne detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.\\n\\n\\n\\n\\nIf we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The Decoder Side\\nNow that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The following steps repeat the process until a special  symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The self attention layers in the decoder operate in a slightly different way than the one in the encoder:\\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\\nThe Final Linear and Softmax Layer\\nThe decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Recap Of Training\\nNow that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\\nDuring training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The output vocabulary of our model is created in the preprocessing phase before we even begin training.\\n \\nOnce we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:\\n\\n\\n\\n  Example: one-hot encoding of our output vocabulary'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\\nThe Loss Function\\nSay we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content=\"Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  cross-entropy and Kullback–Leibler divergence.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)\\nThe first probability distribution has the highest probability at the cell associated with the word “i”\\nThe second probability distribution has the highest probability at the cell associated with the word “am”'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='The second probability distribution has the highest probability at the cell associated with the word “am”\\nAnd so on, until the fifth output distribution indicates ‘<end of sentence>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content=\"The targeted probability distributions we'll train our model against in the training example for one sample sentence.\\n \\n\\nAfter training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content=\"Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.\"),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example,'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Go Forth And Transform\\nI hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Read the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.\\nWatch Łukasz Kaiser’s talk walking through the model and its details\\nPlay with the Jupyter Notebook provided as part of the Tensor2Tensor repo\\nExplore the Tensor2Tensor repo.\\n\\nFollow-up works:'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Follow-up works:\\n\\nDepthwise Separable Convolutions for Neural Machine Translation\\nOne Model To Learn Them All\\nDiscrete Autoencoders for Sequence Models\\nGenerating Wikipedia by Summarizing Long Sequences\\nImage Transformer\\nTraining Tips for the Transformer Model\\nSelf-Attention with Relative Position Representations\\nFast Decoding in Sequence Models using Discrete Latent Variables\\nAdafactor: Adaptive Learning Rates with Sublinear Memory Cost'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Acknowledgements\\nThanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.\\nPlease hit me up on Twitter for any corrections or feedback.\\n\\n\\n    Written on June 27, 2018'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Written on June 27, 2018\\n  \\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/'),\n",
       " Document(metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': \"Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\n\\n \\n  \\n\\n  \\n  Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).\\n  \\n \\n\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n\", 'language': 'No language found.'}, page_content='Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits=text_splitter.split_documents(docs_list)\n",
    "doc_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797852ef",
   "metadata": {},
   "source": [
    "#### Creating a single vector store for all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3c8ab50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "87d857e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'language': 'No language found.', 'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n'}, page_content='1\\n    \\n\\n\\nModels: Sentence Sentiment Classification\\nOur goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\\n\\n\\n\\n\\nUnder the hood, the model is actually made up of two model.'),\n",
       " Document(metadata={'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'language': 'No language found.'}, page_content=\"From here, it’s the logistic regression model’s job to classify this vector based on what it learned from its training phase. We can think of a prediction calculation as looking like this:\\n\\n\\n\\n\\nThe training is what we’ll discuss in the next section, along with the code of the entire process.\\nThe Code\\nIn this section we’ll highlight the code to train this sentence classification model. A notebook containing all this code is available on colab and github.\\nLet’s start by importing the tools of the trade\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport transformers as ppb # pytorch transformers\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import train_test_split\\n\\nThe dataset is available as a file on github, so we just import it directly into a pandas dataframe\\ndf = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\\\t', header=None)\\n\\nWe can use df.head() to look at the first five rows of the dataframe to see how the data looks.\\ndf.head()\\n\\nWhich outputs:\\n\\n\\n\\n\\nImporting pre-trained DistilBERT model and tokenizer\\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\\n\\n## Want BERT instead of distilBERT? Uncomment the following line:\\n#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\\n\\n# Load pretrained model/tokenizer\\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\\nmodel = model_class.from_pretrained(pretrained_weights)\"),\n",
       " Document(metadata={'language': 'No language found.', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/'}, page_content='The training is what we’ll discuss in the next section, along with the code of the entire process.\\nThe Code\\nIn this section we’ll highlight the code to train this sentence classification model. A notebook containing all this code is available on colab and github.\\nLet’s start by importing the tools of the trade\\nimport numpy as np\\nimport pandas as pd\\nimport torch'),\n",
       " Document(metadata={'language': 'No language found.', 'title': 'A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Translations: Chinese, Korean, Russian\\n\\n\\n  \\n  \\n\\n\\nProgress has been rapidly accelerating in machine learning models that process language over the last couple of years. This progress has left the research lab and started powering some of the leading digital products. A great example of this is the recent announcement of how the BERT model is now a major force behind Google Search. Google believes this step (or progress in natural language understanding as applied in search) represents “the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search”.\\n\\nThis post is a simple tutorial for how to use a variant of BERT to classify sentences. This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.\\n\\nAlongside this post, I’ve prepared a notebook. You can see it here the notebook or run it on colab.\\n\\n', 'source': 'https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/'}, page_content=\"0\\n    \\n\\n\\n\\n      they presume their audience won't sit still for a sociology lesson\\n    \\n\\n      0\\n    \\n\\n\\n\\n      this is a visually stunning rumination on love , memory , history and the war between art and commerce\\n    \\n\\n      1\\n    \\n\\n\\n\\n      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\\n    \\n\\n      1\\n    \\n\\n\\nModels: Sentence Sentiment Classification\\nOur goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\\n\\n\\n\\n\\nUnder the hood, the model is actually made up of two model.\\n\\nDistilBERT  processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\\nThe next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the  sentence as either positive or negative (1 or 0, respectively).\\n\\nThe data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\")]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"How the Sentence Sentiment Classification is performed?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46433fb3",
   "metadata": {},
   "source": [
    "### Create Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0d536dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tools = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"Jalammar_Blog_Post\",\n",
    "    \"Search and return information from Jalammar blog post\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753830f",
   "metadata": {},
   "source": [
    "### Create a Tool Node with the Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ba18c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [retriever_tools]\n",
    "retriever_tool_node = ToolNode(tools_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cd8f51a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tools(tags=None, recurse=True, explode_args=False, func_accepts_config=True, func_accepts={'store': ('__pregel_store', None)}, tools_by_name={'Jalammar_Blog_Post': Tool(name='Jalammar_Blog_Post', description='Search and return information from Jalammar blog post', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002E9EF0EF560>, retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002E9A437B750>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002E9EF121080>, retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002E9A437B750>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))}, tool_to_state_args={'Jalammar_Blog_Post': {}}, tool_to_store_arg={'Jalammar_Blog_Post': None}, handle_tool_errors=True, messages_key='messages')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e72e27",
   "metadata": {},
   "source": [
    "### Bind the tools with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a9f4fa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Let me break down how sentence sentiment classification works under the hood.\\n\\n### 1. **Understanding the Task**\\n   - **Objective**: Determine the sentiment (e.g., positive, negative, neutral) of a given sentence.\\n   - **Example**: \\n     - Input: \"I loved the movie!\"\\n     - Output: Positive\\n\\n### 2. **Data Collection**\\n   - **Datasets**: Use labeled datasets where each sentence is tagged with its sentiment (e.g., IMDB for movie reviews, Stanford Sentiment Treebank).\\n   - **Preprocessing**: Clean the data by removing noise, handling out-of-vocabulary words, and normalizing text (e.g., lowercasing).\\n\\n### 3. **Text Preprocessing**\\n   - **Tokenization**: Split sentences into words or tokens.\\n   - **Stopword Removal**: Remove common words like \"the,\" \"and\" that don\\'t add much value.\\n   - **Stemming/Lemmatization**: Reduce words to their base form (e.g., \"loving\" becomes \"love\").\\n   - **Vectorization**: Convert text into numerical representations (e.g., word embeddings like Word2Vec, GloVe).\\n\\n### 4. **Feature Extraction**\\n   - **Bag of Words (BoW)**: Represent text as a bag (or histogram) of word frequencies.\\n   - **Term Frequency-Inverse Document Frequency (TF-IDF)**: Weight word importance by its frequency in the document and rarity across documents.\\n   - **Word Embeddings**: Use pre-trained embeddings to capture semantic meanings (e.g., \"king\" is close to \"queen\").\\n\\n### 5. **Model Training**\\n   - **Traditional Models**:\\n     - **Support Vector Machines (SVM)**: Linear or nonlinear classifiers to separate sentiments.\\n     - **Naive Bayes (NB)**: Probabilistic model based on Bayes\\' theorem.\\n   - **Deep Learning Models**:\\n     - **Recurrent Neural Networks (RNNs)**: Process sequences with LSTM or GRU layers.\\n     - **Convolutional Neural Networks (CNNs)**: Apply convolutions over text for feature extraction.\\n     - **Transformers**: Use models like BERT, RoBERTa for state-of-the-art performance with self-attention mechanisms.\\n\\n### 6. **Model Evaluation**\\n   - **Metrics**: Accuracy, Precision, Recall, F1-score, ROC-AUC.\\n   - **Cross-Validation**: Ensure model generalizes well by testing on unseen data.\\n\\n### 7. **Prediction**\\n   - **Input**: A new sentence.\\n   - **Process**: Preprocess, convert to features, feed into the model.\\n   - **Output**: Predicted sentiment label.\\n\\n### 8. **Continuous Improvement**\\n   - **Hyperparameter Tuning**: Adjust model parameters for better performance.\\n   - **Ensemble Methods**: Combine multiple models for improved accuracy.\\n   - **Active Learning**: Use human feedback to enhance the model incrementally.\\n\\n### Example Workflow\\n1. **Data**: Collect and preprocess text data.\\n2. **Features**: Convert text to numerical features using embeddings.\\n3. **Model**: Train a classifier (e.g., Logistic Regression, LSTM).\\n4. **Evaluate**: Test on validation set and fine-tune.\\n5. **Deploy**: Use the trained model to predict sentiments on new sentences.\\n\\n### Tools and Libraries\\n- **Python Libraries**: NLTK, SpaCy, Gensim, Scikit-learn, TensorFlow, PyTorch, Hugging Face Transformers.\\n- **Pre-trained Models**: BERT, RoBERTa, DistilBERT for efficient fine-tuning.\\n\\nBy following these steps, you can build a robust sentiment classification model tailored to your specific needs.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools(tools_list)\n",
    "response=llm_with_tools.invoke(\"How is the Sentence Sentiment Classification working under the hood?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3068d8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89764318",
   "metadata": {},
   "source": [
    "### Create the nodes of the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e82a6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7365c",
   "metadata": {},
   "source": [
    "### Create LLM Decision Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "085a218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_Decision_Maker(state:AgentState):\n",
    "    message = state[\"messages\"]\n",
    "    last_message = message[-1]\n",
    "    question = last_message.content\n",
    "    response = llm_with_tools.invoke(question)\n",
    "    return {\"messages\":[response]}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23170174",
   "metadata": {},
   "source": [
    "### Create the Document Grading Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "09280599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grade(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score:str=Field(description=\"Documents are relevant: 'yes' or 'no'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5179ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state:AgentState)->Literal[\"Output Generator\",\"Query Rewriter\"]:\n",
    "    print(\"----CALLING GRADE FOR CHECKING RELEVANCY----\")\n",
    "    llm_with_structured_op = llm.with_structured_output(grade)\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "                    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "                    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "                    Here is the document: {context}\n",
    "                    Here is the user’s question: {question}\"\"\",\n",
    "                    input_variables=[\"context\", \"question\"]\n",
    "                    )\n",
    "    chain = prompt | llm_with_structured_op\n",
    "    message = state[\"messages\"]\n",
    "    last_message = message[-1]\n",
    "    question = message[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\":question, \"context\":docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"----DECISION: DOCS ARE RELEVANT----\")\n",
    "        return \"generator\"\n",
    "    else:\n",
    "        print(\"----DECISION: DOCS ARE NOT RELEVANT----\")\n",
    "        return \"rewriter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70cb7843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a grader assessing relevance of a retrieved document to a user question.\n",
      "\n",
      "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
      "\n",
      "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
      "\n",
      "Give a binary score 1 or 0 score, where 1 means that the document is relevant to the question.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Retrieved documents:  \n",
      "\n",
      "User question: \n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "hub.pull(\"rlm/rag-document-relevance\").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1557f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "hub.pull(\"rlm/rag-prompt\").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170e2a3",
   "metadata": {},
   "source": [
    "### Create the output generation node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f384bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_generation(state:AgentState):\n",
    "    print(\"----RAG OUTPUT GENERATE----\")\n",
    "\n",
    "    # print(f\"State in o/p gen is: {state}\")\n",
    "    \n",
    "    message = state[\"messages\"]\n",
    "    question = message[0].content\n",
    "\n",
    "    last_message = message[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    prompt=hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    rag_chain=prompt | llm\n",
    "\n",
    "    response=rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "    print(f\"this is my response:{response}\")\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa0c298",
   "metadata": {},
   "source": [
    "### Create the Query Rewrite Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "81aa40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state:AgentState):\n",
    "    print(\"----TRANSFORM QUERY----\")\n",
    "    message = state[\"messages\"]\n",
    "    question = message[0].content\n",
    "    input= [HumanMessage(content=f\"\"\"Look at the input and try to reason about the underlying semantic intent or meaning. \n",
    "                    Here is the initial question: {question} \n",
    "                    Formulate an improved question: \"\"\")\n",
    "       ]\n",
    "\n",
    "    response=llm.invoke(input)\n",
    "    print(f\"Transformed Query is: {response}\")\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd855e9",
   "metadata": {},
   "source": [
    "### Create the Web Searcher Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e32186f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "80d5044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def web_retriever(query):\n",
    "    # query = inputs[\"question\"]\n",
    "    result = web_search_tool.invoke({\"query\": query})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in result])\n",
    "    # print(web_results)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    print(web_results)\n",
    "    # snippets = [r[\"content\"] for r in result[\"results\"] if \"content\" in r]\n",
    "    # # return \"\\n\\n\".join(snippets)\n",
    "    # return {\"context\": \"\\n\\n\".join(snippets), \"question\": query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "996d6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# web_retriever(\"What is the capital of Australia?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "501cb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state:AgentState):\n",
    "    print(\"----Searching Web for requisite context----\")\n",
    "    # print(f\"State in web_search: {state}\")\n",
    "    message = state[\"messages\"]\n",
    "    question = message[0].content\n",
    "\n",
    "    last_message = message[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    # print(f\"docs before web_search:{docs}\")\n",
    "\n",
    "    web_docs = web_search_tool.invoke({\"query\":question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    # print(f\"Web Search Results are: {web_results}\")\n",
    "    # web_results = Document(page_content=web_results)\n",
    "    docs = docs + \"\\n\" + web_results\n",
    "    # print(f\"docs after web_search:{docs}\")\n",
    "    # return \"\\n\\n\".join(snippets)\n",
    "    return {\"messages\": [response]}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de5d4d",
   "metadata": {},
   "source": [
    "### Build the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "056b14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"LLM Decision Maker\", LLM_Decision_Maker)\n",
    "workflow.add_node(\"Vector Retriever\", retriever_tool_node)\n",
    "workflow.add_node(\"Output Generator\", output_generation)\n",
    "workflow.add_node(\"Query Rewriter\", rewrite)\n",
    "workflow.add_node(\"Web Search\", web_search)\n",
    "\n",
    "workflow.add_edge(START, \"LLM Decision Maker\")\n",
    "workflow.add_conditional_edges(\"LLM Decision Maker\",\n",
    "                               tools_condition,\n",
    "                               {\n",
    "                                   \"tools\":\"Vector Retriever\",\n",
    "                                   END:END\n",
    "                               })\n",
    "workflow.add_conditional_edges(\"Vector Retriever\",\n",
    "                               grade_documents,\n",
    "                               {\n",
    "                                   \"generator\":\"Output Generator\",\n",
    "                                   \"rewriter\":\"Query Rewriter\"\n",
    "                               })\n",
    "workflow.add_edge(\"Query Rewriter\",\"Web Search\")\n",
    "workflow.add_edge(\"Web Search\",\"Output Generator\")\n",
    "workflow.add_edge(\"Output Generator\",END)\n",
    "\n",
    "CRAG = workflow.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "81e6d22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARkAAAK+CAIAAAAZg/2LAAAAAXNSR0IArs4c6QAAIABJREFUeJzs3XdcE/f/B/BPQiBhJWxkyBZQQIIMcVRFwL1HRRTF1boXjmKte1Wtq1YtjlqtFAfuLe492ShDNsjeISHz98d9f/nyRYwJJrkLeT8f/pHcXe7ewbxz97q73JFEIhECAHwzMt4FANBOQC8BIB/QSwDIB/QSAPIBvQSAfEAvASAfFLwL+FaVn7gNNXxWHb+JLeSyhXiX83VkDUTRJOvQNXTpFEMzLV2GBt4VAfkgqejxpcJMdnZyQ3YKy9Ke1sQW6tApDBNNkVAF3osGhcRuELDqBI11fKEQ8XlCezddJ089Q3MtvEsD30T1eqkoi/3saoVRB6qpFdXeXVffULVXrWUFTTkprJpyLolM6jnMWM9Atd+OOlOxXrp3uqy2gtdzmLG5LQ3vWuQs/W39s6uV7j0ZvsGGeNcC2kJleqm+mh+9PX/4LAtLB228a1GgtBd1WYkNI360xLsQIDPV6CVOozBmZ37oChstWvvf8Zj/ofHh+fKwVbZ4FwJkowK9VF3KvXK4eMpqO7wLUZ6y/Kbrxz+Fr1Gjt9wOqMDXfPT2/LBV6vWpMrOhBow3uxJVjHchQAZEXy/dOlniG2xs1EET70JwkPairrFB4BMEuyJUA6HXSx9e12tokNSzkRBCXfzpSY9rGusFeBcCpELoXnp2taLHMBO8q8BTz2Emz65W4F0FkApxeyntZX3X3ga6dLU+xcbVV5/PFdWW8/AuBHwdcXsp/U2dhb1SD8h+/Phx2LBhbXjhmTNn1q5dq4CKEEKIYaKZldSgoJkDOSJoL/GaRGUFHCsnpR6WTUtLU/ILpWHvppuTwlLc/IG8EPTsr7z3LLceDAXNvL6+/tChQ0+ePKmqqurSpcvgwYNHjRp16NChI0eOIIR8fHyWLFkyadKkx48f37p1Kz4+vra21t3dfebMmT4+PgihrKyskJCQPXv2bNq0ydDQUF9f/927dwiha9eu/fPPP66urvKttoMdTYNCaqwX6Oir9eYu8RG0l6pKuVpURa0z169fX1paGhkZaW9vf+bMma1btzo4OMyePZvL5d6+ffvq1asIIQ6Hs3r1aj8/v/Xr1yOE4uLilixZcvHiRWNjY01NTYTQkSNHwsLCmEymm5tbeHi4ra0tNqUiCAWi2goe9BLBEbSXWHV8Uyuqgmb+7t27KVOm+Pv7I4QWLFgQFBRkYGDQYhoajRYTE6OtrY2Ncnd3P3fuXEJCQmBgIIlEQgj5+/tPmjRJQRW2oEOnsOr4ylkWaDOC9lJjnUC3s6JqYzKZ//zzT01NTbdu3Xr06NG5c+dWJ2OxWPv373/79m1FxX/2SldXV4vHfulViqBL12isg6NMREfQfQ9kDZIGRVG1rVu3LjQ09Pnz50uXLg0ODj548CCf3/Jbv6SkZObMmTweb8uWLc+fP3/x4kWLCahURa02P6dJJRP77BSAiLteomqTG2p5CClkPx6dTp8+ffq0adMSExPv379/9OhRfX39yZMnN5/mzp07XC53/fr12traLdZIyldXxTOxVF7rgrYhaC/p0imsWoUkhNra2ps3b44cOZJGozGZTCaTmZ6e/uHDh88no9PpWCMhhO7evauIYqTEquWr+TFrlUDQbTwDMy2hYgIChUKJiopauXJlYmJiZWXltWvXPnz4wGQyEUI2NjYVFRUPHjzIy8vr1KlTRUVFbGwsn89/9uzZq1evDAwMSkpKWp1nx44dU1JSXr9+XVVVpYiaaToaeoZqelKiCiFoL3V01k55XquIOevq6u7YsaOsrGzGjBkDBw48ceLE4sWLx4wZgxDq3bs3k8lctmzZrVu3Bg4cOGPGjMOHD/v7+0dHR69YsWLIkCHHjx/fsmXL5/McM2YMiUSaN29eZmam3AuuKObWVvLoRgTdggBixP3NxZk9BX3HmJnbqHtOeH2nSsAX+Q82xrsQ8BUEXS8hhFy60Uty2HhXgb+aMp6jhz7eVYCvI+6Wg2cfxoFlWR69DchfSN1xcXGbNm1qdRSDwaitbX0TcdSoUYsXL5Znoc0sXrw4ISGh1VFNTU1f2o1+/PhxO7vWfzicm8pqYgtMreHSeSqAuNt4CKGEhzUNNfzeI1v/CRObzf7Srmo2my3eBdeCjo7O52c5yEtFRQWXy211VF1dHZ1Ob3WUmZkZhdL6l1r09vxBUzoYdYBeUgGE7iWE0JXDxcGhHWi6xN0WVZyPSaySPE6v4ZCUVAPRP6P9vzf7d2ce3lXgoKqE++JGJTSSCiF6L+kyKP2/Nz+/vwjvQpQtekd+6AobvKsAMiD6Nh6m8hP30fny0fOs8C5EGeoqeTG/FczYYK9BIeFdC5CBavQSQqggvfH2qdIJSzu276vXF6Q33j9bFrrSlqIJjaRiVKaXEEKN9YJ7p0t16ZSew0yoOkTfOpVVWX7T06sVRuZafcea4l0LaAtV6iVM6vO6Z1crPPsYdLCj2bjo4F3Ot+JyhDmprPLCpuJsds/hJtbKvcQFkCPV6yVM2ov6zMT6oqzGrr0MhEKRLp2iZ0Qhq8JmEZlMYrMEjXUCVh2f2yTKTWPZu+l28tK3d1P57wU1p6q9hBEKRHkfGuur+Kw6Pq9JyG6Q87nlGRkZhoaGpqby3OjS1CKTNJAunaJLpxiZa1q061vgqBXVzvFkDZK9m67i5v90zQHHzv4Dh3gobhGg3WhvCR4AvEAvASAf0EsAyAf0EgDyAb0EgHxALwEgH9BLAMgH9BIA8gG9BIB8QC8BIB/QSwDIB/QSAPIBvQSAfEAvASAf0EsAyAf0EgDyAb0EgHxALwEgH9BLAMgH9BIA8gG9BIB8QC8BIB/QSwDIB/SSJHp6ehoaX7jFJwD/C3pJkoaGBoFAzteCBe0V9BIA8gG9BIB8QC8BIB/QSwDIB/QSAPIBvQSAfEAvASAf0EsAyAf0EgDyAb0EgHxALwEgH9BLAMgH9BIA8gG9BIB8QC8BIB8kkUiEdw2EExQUpK2tTSKRqqurqVQqjUYjk8lkMvnixYt4lwaIi4J3AURkYmKSnp6O/aKWzWYjhIRC4ahRo/CuCxAabOO1YvLkydra2s2HmJubh4eH41cRUAHQS60YNmyYjY1N8yE9e/ZsMQSAFqCXWhcWFkalUrHH1tbWU6ZMwbsiQHTQS60bOnSog4MD9rhnz562trZ4VwSIDnrpi0JDQ3V0dKytrUNDQ/GuBaiAr+/HYzcIK4ubWPV8pdRDIA5mvT3sB9ra2rJKGeml9XiXo1RkMoluRDHsQNWikvCuRWV85fjS3ZiywsxGurGWti5cclGNUHU0ygrYFC1yJ0+9rt8x8C5HNUjqpSuHP1k66Tp3oyu3JEAgTy6VdbChevWDdvq6L+almydKOjpDI6m73iPNPuVwUl/U4V2ICmi9l0rzmnhNIkcmNBJA/kNNU5/XiYR410F4rfdSZUmTFg0CEkAIIU0qmVXHZ9Wp3c4nWbXeS6xaAcNES+nFAIIytqDVVUEvfUXr+8SFAhGfB+ePg/9oYvMRgs/DV8CxWgDkA3oJAPmAXgJAPqCXAJAP6CUA5AN6CQD5gF4CQD6glwCQD+glAOQDegkA+YBeAkA+5NZLq9dEzPxhoqyjho/sFxDo8/59SovhDx7GBQT6LFg04/OXZGR+CAj0wf4FBvuNnzB4/sLpV69d+Mb6R44OPHHyiIQJYs/HBAb7feNSxM6eOxUQ6LMycuHno2bMCgkI9Hn95oXkOci3HvDt8L9uq6am5u071zp3dm8+8N69WxSKpNqmhc/28GDy+fzi4sL09LS9+3798CE1YunPJFIbr08w4fuwLp09JEzQpbN72OSZbZt5qzQ1NV+/fl5VVWlkZCwe+PFjZn5+rhyXApQG/17y8vK9d//2vLkR4uapq697/uKxm1tXgUDwpVfZ2Tl4MX3ET/v2DfopcqGNjd334ye3rYzQiV+5LGvnzu4tGv4bmZl1QCLR3Xs3x4+bJB4Yd/eGm1vXxMR3clwQUA7885Jn124sVsOLF0/EQx49ustgGNjZOkg/k+5+Pfv1DYo5fQJ7yufz/4zaN23G90OH91kZubD5zAUCQczpE4OH9h48tHfEsjnJyQnYcPE2nkgkOhcbPeuH0EFDev04e/LhI/uxlm6xTXXi5JFJYaMGDu4ZNnXMb7s2C4X/+d3pqDFBly6fO3HySGCw37ARfddv+KmysqLVmgV8vq9vj7i4G+IhIpHo3v1b3t26N5/s+fPHm7esnjBx6OChvZdGzI5PeNPKrASCZcvnTp4yurauFiGUmpq0YuX8ESMDwqaOOXBwN4vFwiaLPR8zdvzAJ08fBAb7HT6yX/o/L5AG3r0kQnQ6w9e3x5246+Jht+9cC+g3QNY59fD/rrq6Ki8vByG07/ft52KjR4+aEH3qSt8+gWvXr3j46C42WdTh3y9dOrth/c7VqzabmpqvjFzQYpvq/PmYf04dGzc2NCb66vDhY69dvyhuUbG/jh+6eOnMnB8Xnzt7a8b0uQ8e3jl77hQ2SlNT8/TpE2Qy+eKFu3//FZucknD87z8/r5ZEIgmEggEDhmVkfsjNzcYGvot/XVFRHtAvGJsAIcThcDZvXd3U1PTTyvVbNu+xsbH7efWSqqrKFnPbvnNDRsb77b/uZ9AZhUUFy1bM5TRx9v/+18b1O7OzM5cs/YHP5yOEtLS0GhtZly+fi/xpw5AhcKsBOcO7lxBCCAX0DX72/FFdfR1CqLS0JDk5ISBA5l4yN7dACFVUljc1Nd26fTV0YviI4WMZdMaQwSMD+w86cfIwQqi2rvbM2X9CQqb6+vj36tV3WcRqH2//yqr/WW8kJr1zcekycOAwAwPDYUNH/7H/eHe/Xs0nqG+o/zfm77DJM3v37qevp9+vb9DoURP+OXWUx+NhE1hZdZw8abq+nr6xsYmvT4+MjPdfqrmzq5uVpfWNm5exp3fuXPf17aGnp4+toxBCNBrtSFRMxNKfvZg+Xkyf2T8uZrPZySkJzWdy4uSR+/dvb9m8x9LCCiEUF3dDk6K5cf1OGxs7OzuHZRG/ZGalP3n6AOtPDocTEjI1KHCQlaW1rH9hIBnevURCWNohk8n379/GVkpmZuZdZE8m4r0OGRnvuVyur08P8Simp3d2dlZtXW1uzkeEkKurGzacQqFsWL+jee5CCLm7e759+3L7jg03b12prau1srR2cnJuPkFBQR6Px2uenZydOzc0NBQVFYifikfp69NZrIbPqxWJRFi3BAYOunX7qkgkampqevT4bkDf4BZTNjayft+/Y9z3gwICfQYP7Y0Qqqmpxt4viUSKu3vzr+OHVkVudHf3xKZPTU10dXVjMAywpx06WFhaWiclx4tn6OriJuOfFkgF/30PCCEqldqrZ987cddHjhh3997NoMDBbZjJp09FCCFTEzPsM/35/vTqqsqGhnqEEI1KkzCfcWNDdXR0nz57+Ov29RQKpV+/4B9nLTQxMRVPUFVV0WIm2to6CCE2uxF7KtO+xODgoSdOHnnz9mVdXS2Px/vuu/5cbpN4bGlpyaIlM7t5+f3y85YuXTxIJFLwQH9slEgkEggE235d26KYhob6D+lpAYH/8wVR3WyzUEsLruShEIToJYRQYP9Bq9dEvIt/nZeXs/aXbW2Yw9XrF6ysOtrY2HGaOAihiKU/W1l1bD6BmVkH7Bu9sZElYT5kMnnY0NHDho7Ozc1+9+7V8RNRLFbDlk27xRPo6uohhNgctngINkMjI5M2lG1t1dG5k+uTJ/fr6mp79+qno6PTvJcePLzD5XJ/Wrkeux8UVn9zEUt/Tkx6t237ur+OnjE0NEIIGRmbeHgwp4XPbj4Zg27QhtqATPDexvt/3bv30tfT/+PAb3Z2Dvb2jrK+/Pz5mJSUxMmTpiOErK1ssNu9YBnDi+ljZ+tga2Ovo6Pj5ORCoVASk/6zx1kkEv20atGtW1ebz+rWras5OR+x3e5jxoSMHTMxKyu9+QSOjs4aGhqpqYniIe/fp+jr6ZuamrXtvQcGDnr9+vnLV0/79g1qMaqurlZfny6+sZp4DwqGTCYPHjRi0YKVOto6m7es/k95Dp3Kyko8u3YTv31DAyMbG7u21QakJ89e4rDZ8Qlvmv8T7w6WMApDoVD69AnMzs6Scg9ebm42Np+Xr55t277u9z929uzZZ9DA4QghHR2d8Kk/njh5ODk5gcvlPnx0d9mKuXv2bkMI6enpBQcNuXTp7I2bl+MT3vy+f8fbty9bHDW6e+/mmnXLnz17VFtX++LFk8dP7rm7eTafgK5PDw4a8s+pY8+ePaqrr7t9+9qFi6fHjZtEJrfxjxkUOPhTSbFAIOjh/12LUQ4OnSorKy5fieXz+S9fPXv37hWDYVBWVtJ8Gm1t7XXrtickvj1z9h+E0Lhxk4RC4f4Dv3E4nIKCvD+j9k2fOSE7J6tttQHpyXMbr6i4cGnE/2xaLF/2y5DBIyWPEgsMHHTt+sX+/QdKs6y/jh/CHnQwt+jSxWP5sl8GBA8Vjw2ZMMXR0Tk65vi7d690dfXcunSNiPjP1/aihSv37N32267NAoHAydF5w7odLb6zI5au3v/Hzp9/WYoQMjIyHjZ09PhxLY//zpsbQSaTN25exefzLS2tQydOmxgyVbo/UiuMjIw9PbuZmpiJ754mFth/YF5e9omTh3fv2err479yxbqY0yei/z1eX19n2+z4m3Mn1ylhsw4f2e/j7e/g4HT0yOmYmL9/nDM5Pz/X1dVt+bJfnDu5trk8IKXWr83/8kYVj4c8+xrhURIgnJvHC3sNN7Z00JZiWvVFlLwEgKqDXgJAPqCXAJAP6CUA5AN6CQD5gF4CQD6glwCQD+glAOQDegkA+YBeAkA+oJcAkA/oJQDkA3oJAPlovZdoumSKJrQZ+A8dfYqmlgbeVRBd6w1jYKb1KadR6cUAgspJbTCxgqtEfEXrvdTRWYfLEQh4rfy0CaibTx/Zrj70tl5bWo203ktkMuoz2vTuv8VKrwcQS0M1//m1ssCQNl7KQq20/rtaTFlB04UDRV37GhmaaFF1YXNZjWhokGrKuZwG/vvXNROX2WjRIDx/naReQghxOcKEBzVlhU2sWr4Sq5JKTW2tpqamro4O3oVIIhKJKioqTE1NpZiWQPQMNSmaqIOttmcfBt61qIyv9BJhlZaWnj9/fs6cOXgX8nWlpaW7d+/etq0tF/0DKkT1eiklJYXL5Xbu3Fl81TgAiEDFtoNzc3N37tzp5eWlco0UHx9/5Iik+w4CVadK6yU2m11QUODs7CzFtET05MmTgoKCiRNbv90oUHWq0UuVlZXjxo2Li4vT0IDdiYCgVGMb7/bt25cvX24fjXTixIl79+7hXQWQP6L30vbt2xFCEydO1NfXx7sW+ZgyZUphYWFSUhLehQA5I/Q23vr16wMCAvr06YN3IQB8HUHXS69evUIILV68uB030qJFi0pLS/GuAsgNEXtp9+7dHz9+RAgxGO35oPvevXuPHj3a1NQkxbRABRBrG4/FYunq6sbFxQUFtbyrFwAER6D10q1bt2JjYxFCatVIFRUVM2a0vLUuUEVE6SU2m/3o0aMpU6bgXYiymZiYbNy48dixY3gXAr4V/tt4tbW1Hz58YDKZn98VDwAVgvN6qbq6euzYse7u7tBIN2/e/O233/CuArQdnuul2tra8vJyJycnvAogmlevXvF4vF69euFdCGgLfHpJIBCEh4cfOHCg3ZzNAAA+23hnzpxZtWoVNFKr1q5dGxcXh3cVQGbKXi9FRUX98MMPylyiKrp8+bK3t7eVlRXehQAZUJS5sKNHj+rq6ipziSpqxIgRAoEA7yqAbJS0jZeeno4dhJ00aZJylqjqNDQ0Bg4cWFlZiXchQFrK6KVz585dvnwZIWRra6uExbUbt27dunr1Kt5VAGkpIy+dPXt2/Pjxil5KuyQSiZqammg0Gt6FgK9T4HopOTl5586dCCFopDYjkUj5+fmhoaF4FwK+TlHrJZFINH369GPHjpHgQtRSEAqFLBbrS2PZbHZ1dbWlpaXiCqBSqVpacPX9b6KQXnrx4oWfnx+ZTJQTZ4mPx+PV1tZKmAD7b1LcF5O2tjbsYv1Gcv64czicPn362NvbQyPJF4lE4nA4EtZdAHfyXC9VVVXV1taam5vrEPsa3wT01fUShs/nI4QoFPkfFYT10reT29pj5cqVPB7P3t4eGklxKBQKmUzG/WcyoFXy6aUbN24MHDjQ3NxcLnMDEpDJ5Lq6Oi6X+9UpN2/eHBkZqZSiAJJDL8XGxgoEgv79+/fv319OJQGEnZKHHVH4HIPBIJFIQqFQ6UUBSb5py/vKlSsZGRkaGhrt44qqhJKZmSlhrKamJp/PJ5FIcMiBONrYS5WVlcbGxp06dRo+fLi8SwJo+fLlycnJCKG4uLj9+/c7OTkVFBTs378/MzOTQqHY2NiEhYV5enpWVFSYmJi0OqrFDF+9enXu3LmMjAxDQ0M3N7fp06cbGRnh9ObarbZs4718+fLnn39GCLm6uiqgJIB27Njh6uoaFBR08+ZNJyen6urqJUuWmJmZ/fHHH7t37zY0NNy2bVtjY6OxsXFZWVmro5rPLSsra82aNUwmMyoqau7cudnZ2fBjeEVoSy+lpqYeOnRIAcWA1l24cEFLS2vRokUWFhZWVlZLlixhs9lXr14lkUhXrlxpdVTzl6emptJotJCQEDMzM19f361bt37//ff4vZt2S4Zeampq2r17N0Jo+vTpiiwJtJSTk+Pk5CQ+rKSjo2NlZYUFqtzcXEdHx4aGhs9Hibm5uXE4nDVr1pw/f76oqIjBYHy+EQi+nQy9FBERMXjwYEUWA1pXVVXV4jpNNBqNzWZjo2g0Gp1Oxw7jNh8l5uTktHHjRmNj42PHjs2YMSMyMjI1NVW570AtyNBLO3fudHFxUWQxoHU6OjotLjvOZrOxnQfYKDKZLF5riUc15+vru2TJkr///jsiIqKurm7t2rXi3gPyIkMv0Wg02AOLC2dn5/T0dB6Phz2tr68vKCiws7MTj6qvr8earfkosaSkpNevXyOEjI2Ng4ODZ8+e3dDQALfYkDsZeiksLCwvL0+RxYD/srS0/PDhQ0JCQnV19ZAhQ1gs1r59+8rKyvLy8nbs2EGlUgcNGoQQwkb98ccfJSUlLUaJpaWlbd68+fr16zU1NR8+fLh06ZKxsTGcpCJ3MvQSh8OBM8GUZsiQISQSadWqVTk5OVZWVtiDKVOmrFixAtvexs57xEYVFhb++OOPLUaJjRkzZtCgQYcOHQoJCVmxYoW2tvb27dsVcYKsmpPhPHEOh0OlUmEzTxGkPE9cceA88W8Heak94HA4cE803EFeag8EAgFcTw93Mmw0Q14iLNhkIALIS4QAeakdgLzUHrDZbA6Hg3cV6g7yUnsgFArhp4G4g7xECJqamt/ygyIej6ehoQE/ScKXDHmJx+NRKBTYzAOgVTJs42lqakIjEdO5c+euXbuGdxXqToZeCg0NhbxETKWlpeXl5XhXoe5kyEs8Hg/yEjGNHTsWLl+DO8hLAMgH5KX2APISEUBeag8gLxEB5KX2APISEUBeAkA+IC+1B5CXiADyUnsAeYkIIC+1B5CXiADyEgDyAXmpPYC8RASQl9oDyEtEAHmpPYC8RASQl1TYsGHDRCKRSCQS/6dgj2F7DxcyrJc0NTUVWQmQmbm5eXx8PJn83w11oVDo5+eHa1HqC/KSCps0aZKhoWHzIYaGhlOnTsWvIrUmQy9BXiKa/v37Ozo6Nh/i7Ozco0cP/CpSazL0UnR0tK2trSKLATKbOHGigYEB9pjBYEyZMgXvitQXHF9SbQEBAeK7LXXq1AlWSjiCvKTyJkyYoKury2AwwsLC8K5Franv8SWREJUXNQn4Kv+O3J2+c7H119bWdrD0/pSj8ldv1dQkm1hr4V1FW6jj8SWhQHT3dHnG2zoHD/2GGh7e5YD/oa1PyU1t6OxHDwwxw7sW2cjQS+0DlyP8Z1t+n9EdzO1oeNcCvqgoq/HVjfKJK2w0tVTmu1vt8tKJzXnDf+gIjURwVk46QaGW/+7Ix7sQGajX8aV3d6uZfY1punDqmgrQN9Z07sZIfITnrXRkol7Hlwo/snUN4J7HKkOHTvmUy8a7Cmmp1/ElkYhkYKKS+4jUE8NUS8jHuwipqVdeqqvkClV8M1WtCAWiumqV2dGqXnkJAMWRITxER0dTKBA2AGgd/H4JAPlQr7wEgOJAXgJAPmTYxouJiYELdADwJTKsl6CRAJBAhl4KCQnJzc1VZDEAqDAZekkgECiyEgBUG+QlAOQD8hIA8gF56YsEAsHgob337d/x+agrV88HBPrk5eXgURfKyPwQEOjT/N/wkf0WLZn1+Mn9b5xzdnZWQKBPUlK8nCpVL5CXvkhDQyOg34D7929//sbv3rvp3MnV1tZe1nnm5HwMCR0ml/Kmhc/e9dsh7N/0aXNFItGatctfvnr21Reu3/DT9RuXWh1lYGA4JWymmVkHuVSobmTopZiYGPHlo9TEoIHDa2qqX79+3nxgaWlJYuK7AQPa0hLpGWnyqs3OzsGL6YP9Gz3q+317jtjZOcTGRn+9hvQv1mBkZDwtfHaHDhbyKlKtQF6SpGtXL4sOlnF3bzQfePfeTQqFEhw8BCGUmpq0YuX8ESMDwqaOOXBwN4vFEk/2/PnjkNBhgcF+P86efOPmZYTQX8cP/bp9fWlpSUCgz9lzpxBCjY2Nm7asHvf9oIGDe/44e/LFS2ex12LbWi9ePBn3/aCZP0yUsloHe6dPJcXYYz6f/2fUvmkzvh86vM/KyIUvXjzBhgcE+nwqKd6xc+Pwkf0QQmvXrdiwMfLPqH0BgT6PHt9rsY1389aVufPDBw/tPXd++LnYaOyslwWLZqxYOb/5ciN/Xjx3friEhSKERo4OjI39d9GSWQGBPhyOyl8sqVWQl75i4MDhj5/cb2pqEg+w2pepAAAgAElEQVS5fedazx596Pr0wqKCZSvmcpo4+3//a+P6ndnZmUuW/sDn87FG+mXtshnT523buq9374DtOzbE3b05LXx2yIQp5uYd7t99M37cJITQT6sWFhcXbtzw25mY6336BO7d9+v7D6ni04hP/HNkwvdhEUtXS1lqcXGhibEp9njf79vPxUaPHjUh+tSVvn0C165f8fDRXYTQzetPEULLl/1y5dIDbEHZOVnZOVmbN+7q6uHVfG5xd2/+un29cyfX6H8uz5wx71xs9P4DvyGEAvoGv333SvytweFw3rx5EdR/kISFYgu6ev2Ck5PLju1/aGm1z59jQl76iiGDR3K53AcP7mBPs7Iy8vJyBgQPRQjFxd3QpGhuXL/TxsbOzs5hWcQvmVnpT54+wFZBfb7rHxw02NfHP2zyjAnfhzU2slrM+cXLp8nJCcsjfuns6sZgGEwKnebhwfz7RBRCCPv9sq+P//hxkzq7un21yPqG+t//2PkhPQ1bWzY1Nd26fTV0YviI4WMZdMaQwSMD+w86cfLw5y8kkUglJcXr127v2bOPgcH/XOb/+vWLXbt6LV70k6GhUTcv32lTZ1+8eKa6uqpv3yChUPj4yT1ssidPHwiFwn79giUvlEQi0emMBfOW+Xh3b35jjvYE8tJXmJqaeTF97t67iT2Nu3uDwTDo0eM7hFBqaqKrqxuD8Z/LeXfoYGFpaZ2UHC8UCj9mZ7o264HZPy4aMXxsiznn5GTRaDR7+/9eXN+5U+fmYca5U2cJha1dt0K8H2/EyICnTx/MnbNkyOCRCKGMjPdcLtfX57/XQ2Z6emdnZ9XWtXIdElsbexqt5VWZhEJhSmpi8zl4efkKhcKk5HhjYxOmp7d4n+HTpw+8u/kZGRl/daEuzl0kvJ12QIZjtWqYlzADBgzdsXNjVVWloaHRvfu3BgQPxb5ZGxrqP6SnBQT6NJ+4uqqSw+EIhUIq9SuXDausrKDRtJsP0dHRYbMbxU+1qFQJL58WPtvDg4kQYjU0rNuwcvCgkdh2I1YYFmxavKS6qtLCwqrFwFaXwuVyeTze0WMHjh478D9zqK5CCPXrF7z/j50cDkdDQ+P5i8cLF6yQvFAGnYEQaq+bdmIy9FJISMi2bdvUcNUU0G/Avt+3P3gY5+ToXF5eNnjQCGy4kbGJhwdzWvjs5hMz6AZUKpVMJrNYDZJnq6ury+H8z0V2WI0sceD5Kmw/HvZ4YsjUU9HHgoIGW1laI4SMTUwRQhFLf7ay6tj8JdLv7KbRaDo6OgOCh/bpE9h8uKWFNdZL+37f/uz5Iy0tLaFQ2K9vsFwWqupk6CX1zEsIISqV2rdP0KPHdwsKcp07uYq3yhwdOt2+c82zazdxAMjNzba2ttHQ0HBx6ZKckiCew+Ej+7lc7ry5S5vP1sW5C4fDycxK7+Tkgg15/z7Fzv5/7qckpbDJM2/fubZz58bdu/5ECFlb2VCpVISQuNmqq6tEIpGOjk7znSiSOTo61zfUi+fA4/E+fSoyMzNHCDHoDO9ufq9ePWtq4vTq2VdHR0fyQtvwjlQR5CWpDBo4PDk54cHDuOaHlcaNmyQUCvcf+I3D4RQU5P0ZtW/6zAnZOVkIoZHDx71+/fz0mZPxCW8uXT73b8zfWAdaW9tUVlY8efKgoCDPz6+npaX1rl2bP6SnVVVVHj124P37lAnj23KvCi0trfnzliUkvsV2vuvo6IRP/fHEycPJyQlcLvfho7vLVszds3cb9r1gamr25s2L+IQ32C7HL5k1Y/7Tpw+u37gkFAqTkxM2bIxcumw2l8vFxvbtG5SU9O7t25f9+gVjQyQsVE1AXpKKp2c3Q0Oj2toabEcZhq5PP3rkdEzM3z/OmZyfn+vq6rZ82S/OnVwRQgMHDqurr/37RBSLxTI2Nvlh1gJsr4B/994e7sxf1i6bOuWH8Kk/bNrw26E/98ydN1VLS8vBodPGDTuxCNQG3/UO6Oble+jPvb169aPr00MmTHF0dI6OOf7u3StdXT23Ll0jIv6zb31S6PS/jh969frZv9FXJczQw4MZdejUqei//ozax+Gw3bp03bRxF/X/w1W/vsG7dm+hUqm9evYVv0TCQtWBDNfmbwd56eSWvP4TLelGcBEY1VBR3PTyellIREcppsUfHF8CQD7g90sAyAecjweAfMD5eADIB+QlAOQD8hIA8gF5CQD5gLwEgHxAXgJAPiAvASAfkJcAkA/ISwDIB+QlAORDvfKSkYUWCe8agPTIJGRopjK/bFevvEShkCqK2ufF2dqliuImLarKfPvJ0Evjx49X9bxk21m3rpKHdxVAWvXVvI4uKvMT9/Z5pbIvcfXRr6loSnteg3ch4OsSHlTxOHwnTz28C5GWDL+rbTdu/l1CN6YaW1KNLWgk9foyUQFCAaos5pTms5FI2HestFdlIgJ17CWEUMqzuo9JDUIRKi8gRHwSiURCoUhDA5/OFgiEZDKZRIxgYmJFpVBInbz0OvvR8a5FRiKpjRs3LicnR/rpgZSKiorGjRuHbw2DBw+uqanBtwZVJ8M+caAglpaWZ8+exbeG69ev41tAOyDDRsXZs2dV+iJExHT+/PnKykq8q0AIodLS0suXL+NdhQqD6I2nVatW6enpGRsb410IQgiZm5sLBILNmzfjXYiqkmHfw/jx43fs2AGrJnnBzski2hFwYlalEmC9hI/c3Nznz58T8COroaHx8OHDoqIivAtRPWq6TxxfHz582LRp0z///IN3IV80atSoP/74w8qq5Q1mgATQSziora1lMBh4V/EVKlEkoajX+XhEcPv2bex2tMR3//59vEtQJZCXlGr+/Pl0Ol0lbknEYDD4fH5kZCTehagM2MZTnpqaGgqFoqenMidrIoTq6upEIhFs7EkD1ktKUlRUVFpaqlqNhBCi0+kFBQWlpaV4F6ICIC8pQ3x8/Lp161xcXPAupC3c3d2XLFmSkZGBdyFEB+slhRMIBLq6uocPH8a7kLaLjo4WCoUQBySDvKRwr1698vb2JuBhWZk0NTWlpKR4e3vjXQhxwXpJsaZNm6atra3qjYTdNLqpqWnhwoV4F0JccD6eAuXl5RHn1FW5KC0t5fF41tbWeBdCRLBeUpTy8nIajdaeGgk7l5xEIlVVVeFdCBFBXlKIR48eXbx4cdeuXXgXohCzZs2aO3eul5cX3oUQC/SS/LHZ7OTkZD8/P7wLUaBnz575+vqqyslQygHHl+SvrKysfTcSQsjf3//Tp094V0EskJfkbMKECXw+H+8qFI5MJldXV8+cORPvQggEtvHk6e3bt5aWlhYWFngXoiT5+fk1NTVdu3bFuxBCgF6SGzabLRKJVOIccDlisVgaGho0Gg3vQvAHx5fk4+bNm0+ePNm0aRPehbSFSCRqampq88v//vtvPz+/zp07y7UoqWhoaBBn/wdcH08OqqqqKBSKijYS1ksNDQ1tfvnYsWM5HE59fT1J6Zd+1dLSIk4vwTaeHLDZbG1tbbyraDuhUPjth19FIhEuvUSnE+VSybAf71sNGzasrq4O7yrwJxAIqqur8a4CT3B86ZvcvHkzKirK3Nwc70LwR6FQ6HT6t+QuVQd56ZsMGjQI7xIIRENDQ5oz4jdt2tTQ0LBt2zalFKU8cD3xNjp//vyOHTvwroKI6uvrJa+devfu3b9/f+zx5s2bb926pazSFEuG9ZJAIGgHv8ORi8LCQoTQ8uXL8S6EiPT19dlstlAoJJNb/6bu16+f+HFmZqaPj48Sq1MgOL4EWtmPt2nTJjKZbG5ufvbs2dWrV/fu3TstLe3UqVPp6ekMBqN79+6TJ0/W0dG5du3an3/+ef78eQqFghDat2/f9evXDx06hH1Irl27dvjw4XPnzoWGhoaGhj558iQlJeXs2bN79+7FtvHEW8i6urqxsbHYxQOvX7+em5trZ2fXt2/fUaNGYfsGP68He6Gq7seDlRJm1KhRLBYL7yoUi0Kh5Obm5uTkrFu3zt3dvaioaNWqVRwOZ/fu3WvWrMnJyVm+fDmfz/fy8uJyuVlZWdirUlNTzczM3r9/j/VnQkJCt27dKBQKhUK5ceOGo6Pjli1bmh88uHTpEkJoyZIlWCPdv39/165dTk5Of/31V3h4+IULFw4dOtRqPTj9Vb5Chl6KiYmBldLx48cPHTqkq6uLdyGKRSKRSktLV69e7e/vb2BgcP/+fQqFsmbNmo4dO9ra2i5evPjjx4/Pnj2ztLQUN091dXV+fn5gYGBKSgp28mtGRoabmxs2N319/Tlz5mCt9aWF3rx5093dff78+YaGhkwmMyws7MqVK9h+9hb1KPePIS0Zegm7m4iaCw8P79ChA95VKEPHjh3FZ9mlpaW5uLiIrzhpbm5uYWGB9YyXl1daWhpCKCUlxdHRkclkYq1VUVFRWlravXt37CXOzs6SFycUCtPS0ppnJyaTKRQKsaW0qIeYZNj3sGrVqrlz59ra2iqyHuK6fft2eXn5pEmT8C5ESahUqvhxQ0NDRkZGiwMA2BqDyWQePHgQIZScnOzu7u7q6lpaWlpRUZGUlGRqamptbd3Y2IgQ+uqZPlwul8fjHT9+/Pjx482H19TUfF4PMcnQS3l5eep8wlF9fX1+fj7eVeDDyMjIzc1typQpzQdiod/b27uurq6kpCQlJSU0NJRKpTo7O6empqampjKZTIQQn8+X5mNDo9G0tbWDgoLE+xUwKvQDFhl6KSYmRpGVEN2oUaPU9qvE3t7+7t27Hh4e4t3ceXl52N2Z6HS6g4PDixcvsrOzPTw8EEJubm4pKSnx8fFY7+nr60t5np6Dg0NDQ4Onpyf2lMfjlZSUmJqaKvKdyRPkJWlpaGhIyM3t25gxY4RC4aFDhzgcTmFh4dGjR2fPni0+oYzJZF66dMnW1hYLVF26dHn9+nVxcTF2cRUJjUSlUk1MTN6+fZuYmMjn86dNm/b8+fNbt25hMWnr1q0rV67kcrlKfKPfRIZeCgkJUefz8S5evKi2Jzro6+sfOnSIRqMtWLBg5syZSUlJixcvdnJywsYymcxPnz6Jd1W7ubmVlJQ4OjpirVVfXy9hfR4SEpKYmLh+/XoOh+Pu7r5///6UlJSQkJBVq1axWKx169YRPyaJyXCsNiQkZNu2bWq7Wzw2NjYjI6Nd3o9ILr+5+JK6ujoqlaqgliDUsVr4/ZK0BAKBSCRql5t5Cu0lhf6uiVC9BOfjSUud3/u3UP4PBPECeUla6pyXvsVXTxtvN+B8PGkJBAJ1uPCd3KlPiIC8JC3IS20DeakVkJfwLkElqU9egt8vSevixYuZmZnt8vd/QqGQx+MpaObHjx/38PBQ0A0FCXUAXYY61PyLuR3nJTKZrLhDovn5+XZ2dip0yLXNIC9Jqx3nJYXi8/lkMvlLv1dvT2ToJTXPSwBIBseXpAXHl9pm3bp1cXFxeFehDHB8SVrtOC8pFI/HU5NfGEBekhbkpbaBvNQKyEsASAB5SVqQl9oG8lIr1HylBHmpbSAvgZYgL7UN5KVWQF4CQALIS9KCvNQ2kJdaoeYrJchLbQN5CbQEealtIC+1AvISABJAXpIW5KW2UZ+8BL9fkhbkJZmMGzcOu33tp0+f3r59e/jwYezpyZMn8S5NUeB64tJS5+uJt4FAIBBvxdTX12O/3v3uu+/wrkuBZNjG4/F46vxhItTPoYlvwIABLT4tJiYmM2fOxK8ihZOhl0JDQ/Py8hRZDKFBXpLJhAkTWlwaxMPDA7sRRnslQy9pamqqzzVlPgd5SSZGRkbBwcHip8bGxlOnTsW1IoWD40vSguNLsqqqqpo5cyZ2A7h+/frt3LkT74oUC/KStCAvyUq8ajIyMgoLC8O7HIWT4cMRGhqqztfHu3DhQkZGxsqVK5W/6MY6AVLNjevhQ8bF3Xrs5OTUycG9sV41zyQSIR26VEeDZOglNc9LQqFQKBQqc4l5HxoTHtQUZjYyTLSa2Kr5QURoSNfNCKF/d6jqrX5NrGlleWzHrnq9R5po0SRtx0FekpZIJBKJREo7rywroSHpaZ3vABOGiRap/Z/LRmhNbGFNWdPd6E9hP9vq6H9xHSVDL/F4PAqFos6rJqV5/6ouI4HVf4LK3EJcTURvyw5fY0fVbv27DY4vSevChQu//vqrEhbE54rS3zZAIxFQ4ETLp5crvjQWji9JS2l5qayAw+MqNZgBKRmYan1MbvjSWMhL0lJaXnr/qq6qlN+1j5GiFwTa4F7Mp6CJZrqt7dmD40vSIpFIytnxwOMK2SxV3WvX7lUWcb60bQZ5SVpKy0tARUFekpbyjy8B1SLDsdro6GhFVkJ0Y8aMUedNXPBVkJekpbS8BFQU5CVpQV4CkkFekhbkJSAZ5CVpQV4CkkFekhbkJSAZ5CVpQV4CkkFekhbkJSAZ5CVpQV4CksnQS2r++yUSiUTw9/706cMHj+LS09OqqypdXLp4enqPHjVBT09P+ZWsXhPx9OlD8VMymWxhYeXZtdvcOUt1dXXlvrjY8zEHDu66e+eV3OcsE7jeg7RwvN7DVzU1Na3bsPLly6cjR4ybNHEag2GQl59z8eKZa9cv7N4VZdHBUvklWVlaR0Ssxh43sliv3zx/8DCuoDBv7+7Dcv9K6tLZPWzyf65ieeHimQ/pqZEr18t3EdKA6z1Ii8h5Kfrfv168eLJu7a99+wRiQ/z9ew8aOHzh4pmrf1l6+M9o5e+BpGlrezF9xE979erLZPqs3/BTWlqym1tX+S6rc2f3zp3dscfp6Wnynbn0ZPgTR0dH29raKrIYQhszZgwxV0oIobv3bvn6+IsbCcNgGMyaMT87O+vFiycIoZjTJwYP7S0eW1paEhDoI94SS01NWrFy/oiRAWFTxxw4uJvFYmHD165bsWFj5J9R+wICfeLu3hg8tPc/p46JZyIQCEaM6v9n1D5pinSwd0IIFX8qwp5WVVVu2vxzSOiwUWOCNm/9paAgDyF0+UrswME9xdf03LV7S0CgT07OR+zp5Suxg4f25vP5I0cHxsb+u2jJrIBAn7r6utjzMYHBfgihxUt/uHX76u3b1wICfTIyP0j5vh49vvcNf/v/guNL0iLs8aWamuqiogJ//1Yue+/v31tDQyM5JUHyHAqLCpatmMtp4uz//a+N63dmZ2cuWfoD9oHW1NTMzsnKzsnavHGXj7d/QL8BcXdviF8Yn/Cmvr5u0MDh0tRZVFSAEDIxMcWacEnEjwmJb5csXnXsyGlDA6O586YWFRd6e3fncrmZmR+wlySnJJibd0hNS8KepqQm+nj7UygUTU3Nq9cvODm57Nj+h462jngRe3ZFde7sPmDA0Pt33zh3cpXyfXX18JLuL/0VcHxJWoQ9vlRWXooQMjMz/3wUhUIxMTEtKyuRPIe4uBuaFM2N63fa2NjZ2Tksi/glMyv9ydMH2DdISUnx+rXbe/bsY2BgOHTIqLy8nMysdOyFDx/Gubp0sbW1/2qR8Qlvft+/w9LCysOdiRBKTk7Iz89dFbmxu19PIyPjObMX0xkGsbHRVpbW4uaprq7Ky8sZEDw0KTkem0lKckK3bn5YVXQ6Y8G8ZT7e3SVcAFT69/XV+qUBx5ekJRKJCJuXsDj3pVFf3ZpITU10dXVjMAywpx06WFhaWos/wbY29jQaDXvs5tbV2tomLu4GNtuHj+4GBw9tdZ4fP2YGBPqI/y1bPtfJyWXrlr3YRz85JUFTU7Obly82MYlEYnp6Jya9Qwh5d+uekpKIEEpKju/k5OLl5ZuWmoQQKi8v+1RS7OPdHXuJi3OXr/5NpH9fcgHHl6Q1evRoYm7impmaI4RaXfkIhcLKygpT01ZWWc01NNR/SE8LCPRpPrC6qhJ7oEWlNh8+asT4f6KPzf5xUXzCGza7MShocKvzbL4f78qV2Hfxr5ct+4WuTxcvkcfjtVgitn7w8vL9ff8OhFBi4lsPD68unT1KSj+Vl5clJL41MzPv2PE/iV1LS+trfxjZ3te3g+NL0iLs8SUDA0MHB6fnzx+PGxvaYtTrNy/4fD62XdSCQPjfS0oYGZt4eDCnhc9uPgGDbtDq4oIHDD0UtffN25fPXzzu2aOPuD1aaL4fz97OMWzK6AMHd/20Yh02xNjYRFtbe/Om3c1fokHWQAj5+vaoq6v9VFKclBw/JWwWlUp1cemSnJKQkpLQzauVNyKBTO/r20FekhZh8xJCaMTwce/iX8fdvdl8IIvFOnbsgKNjJ2y7SFNTq6mpSbyLLD8vRzylo0OnsrISz67dvJg+2D9DAyMbm9YPJNL16f36Bj18GHfv3q3goCHSlGdgYDhjxrxbt65iG28IIUdHZzabbWbWQbxEc3MLJycXhBCDznBydH729OHHj5meXbshhDzcmcnJ8W/fvfLx8ZfpzyLT+/p2MvQSjUYj5hezchD5+NLQIaO8u/lt3rL69z92vnn7Mj7hze3b1+bMm1JWXrpm9VbsRsNduniIRKKbt65gO8SjY46LXz5u3CShULj/wG8cDqegIO/PqH3TZ07Izsn60uKGDBmF7c3z9+/9pWlaGDlinIOD0/adG7Bm9u7m5+fXc+fOjaWlJbW1NRcvnZ09J+zmzcvYxF5evucvxNjZOWBRx93N8+XLp0VFBeKwJIGVVcf371Pexb+urq6S9X19Ixl66eTJk3B8Ce8qWkehULZt3Td/bkR+Xs4vayKWRsze/8dOz67d/jp6Rvw13NnVbc7sxVFR+wICfTZsipwxba54twRdn370yGltmvaPcyZPCR+bkPh2+bJfnDu5fmlxXkwfCoUSHDRE+pvokEikiKWrCwry/jl1FBuydfOevn2DNmyKHDUm6PyFmKCgwWPGhGCjunn5Fn8qEu+q9vBgfiop7uTkIt6LIMHwoWNIJNLyFfM+ZmfK+r6+kQzXmuRwOFQqVZ1XTcqR9KSmrJDXfbBp215eWloSEjpsWvjsKWGKujlsesb7OXOnnDgea21to6BFENbZ33JCltm0ehcZGdZLYWFhkJfwruLrzM07BAYOOvnPkYeP7sYnvKmpqZbjzLOyMp4+fbhl6y8TQ6aqYSNJJsN+PMhLhM1LLSxauJKiQdmwMZJEIu3Y/kfz8+K+UdThfa/fvAgOHjJ92hx5zbPdgOuJS0tp1xP/xm08oFDy2cbjcDjq3HiEPR8PEATkJWnFxsZu3boV7yoAccHxJQDkQ4Z9DydPnlRkJUQ3duxYvEsAhAZ5CQD5gLwkLchLQDLISwDIB+QlaUFeApJBXgJAPiAvSQvyEpAM8hLhaGqRaTqtnKICiMDEmoa+0ARwPh7hFGdznl6pGBRujXchoKWmRsHFP/JmbnJodSzkJcIx60ijaMKJf0RUXcZz6PrF67NDXpKW0vISRRN19tO/92+xEpYFZHIvuui7kSZfGgu/XyIiVx99LSr51t9FPsEmDBMtihb82fHUWC+oK+fG/Vsc/ou9JvWLqx/IS8RVmMVOeFBTmNmorUdp4gikeAURCYVCwl4OTRpm1rSq0iZHD71ew00kf6nB9R5UQBNbNX7P26qNGzf27NkzMDBQimkJSYSoOlJFIRm28cLCwtT5/kuxsbEZGRmRkZHKXzRVW4V3RYhIXA1NoUq/BSnB8SUA5APOx5MWnI8HJIPjSwDIBxxfkhacjwckg7wEgHxAXpIW5CUgGeQlAOQD8pK0IC8BySAvASAfkJekBXkJSAZ5CQD5gLwkLchLQDIZeklXVxdu9ADAl8iQl44fPy7FVO0W5CUgmQzrGRaLpSo3xgNA+WTopfDw8Pz8fEUWQ2iQl4BkkJekRSaT1fntg6+CvCSt0aNH410CIDTIS9ISiUTq/PbBV0Fektb58+d//fVXvKsAxAV5SVqQl4BkkJekBXkJSCZDL7FYLG1tbYJ8NwsEAj6fr8wlYuciKvlMeS0tLTg3X1XI0Evh4eHEuT4el8tlsVjKXCKHw+Hz+Xp6X7w0uyIwGAxNTU1lLhG0GeQlAOQD8pK0aDQa3iUAQoPjSwDIBxxfkhaHw2loaMC7CkBckJcAkA8ZeuP48eM2NjaKLIbQaDRaG3bibd68+datW4qpCBAL5CXFyszMxLsEoCSqenzpc+/fv9+/f39RUZG7u3toaOjRo0ft7OwWLFiAEKqqqoqKikpLS2tqavL29g4NDbW2tkYI5ebmzp49e+/evadPn3727JmJiUnfvn2nT5+uoaGBEEpLSzt16lR6ejqDwejevfu4ceO0tLT09PQuXrx4+vTpBQsWbNq0afjw4XPmzHn58uWDBw9SUlLq6+tdXFxCQ0M9PT0RQoMGDUII7d69OyoqKjY2FiEUHR19586dyspKU1PTrl27LliwANts/v7770NDQ588eZKSknL27Fl9fX28/5xAZu0kL3E4nHXr1hkaGv7555/h4eFRUVHl5eXYGQMCgWDlypVJSUkLFiw4ePCggYHBokWLiouLEULYYdC9e/f269fvypUrK1eujI2NffToEUKoqKho1apVHA5n9+7da9asycnJWb16NXamhZaWFpvNvnbt2vLly0eMGMHhcH799Vcul7ts2bL169d37Nhx7dq1VVVVCKFLly4hhJYsWYI10okTJ65cuTJr1qzo6OipU6c+evTo/PnzWP0UCuXGjRuOjo5btmzR1tbG+88J2qKd5KVXr17V1tbOmDHD3Nzcyclp2rRpZWVl2KjU1NSCgoIVK1b4+voaGRnNmjWLTqdfvHhR/NrvvvuuT58+mpqaHh4eFhYW2FbZ/fv3KRTKmjVrOnbsaGtru3jx4uzs7KSkJOw0Ig6HM378+ICAACsrKxqNdvDgwYULF3p6enp6es6cOZPD4aSmpraosKGh4ezZsxMnTuzZs6eenl6fPn1GjBjx77//8ng8bJ76+vpz5szp1q0bhSLDxgIgDlU9H6+F3NxcXV1de3t77Kmnp6d4Myk1NVVTU5PJZGJPSSRS165dk5OTxa91ciBynfwAACAASURBVHISP9bV1cV2fKelpbm4uDAYDGy4ubm5hYVFSkpKnz59sCHOzs7iVzU2Nv71119JSUnY6gghVFtb26LCwsJCHo/n6uoqHtKpUycWi1VcXGxra9tihu2JqamplpYW3lUoQzvJSw0NDTo6Os2HiNugoaGBx+Nh0UXMwMBA/LjVb4eGhoaMjIwWryovLxc/Fn8+ysrKli1b5uXlFRkZ6erqSiKRhg0b9vkMsTajUqniIdi2HJvNxp621/PuysvLuVwu3lUogwy9ROS8RKPRsI0lscrKSuyBkZERjUZbv35987HY3gUJjIyM3NzcpkyZIh7C5XJbtCvm0aNHPB4vIiIC642amppWZ6irq4vlOvGQxsZGbEHSvUVAdO3kfDxLS8uampqqqirso5mYmCj+vndwcOBwOKamppaWltiQT58+iddaX2Jvb3/37l0PDw/x10deXp6VldXnU9bX1+vp6Yl3GDx58qTVGTo4OGhoaGCbjtiQ9PR0PT09ExOTtr5pQCzt5PiSr6+vhobGwYMHGxsbi4qKoqOjxZ9RLy8vHx+fPXv2lJWV1dbWXrlyZeHChXfu3JE8wzFjxgiFwkOHDnE4nMLCwqNHj86ePTs3N/fzKe3t7auqqq5du8bn81+/fp2QkMBgMLCtQSqVamJi8vbt28TERG1t7f79+8fExLx48aK+vj4uLu7y5ctjxowh7KoeyKqd5CVjY+MFCxb8/fffEydOdHJymjRp0sGDB8U7xDZs2HDt2rWtW7e+f//e2to6ICBg5MiRkmeor69/6NChM2fOLFiwoKCgwMXFZd68eR06dPh8yn79+uXl5Z06der333/39vaOiIg4e/bs6dOn6+vrFy5cGBIScvLkyTdv3pw4cWL27NlkMnnbtm18Pt/CwmLChAnjx49XzN8D4IAk/a0rwsPDN2zYQJDd4mw2u8VvAYuLi/X19bHddyKRaMyYMVOnTh01apS8lgi/BWybn3/+uU+fPgMHDsS7EIVrJ3mptrZ28eLFDg4O4eHhBgYGx48fJ5PJ4v3XcgG/XwKStZO8xGAwNmzYIBKJNmzYMH/+/IaGhj179sAuMqBM7SQvIYRcXV0Vev06XLbxgAppJ+fjAYC7dpKXlADyEpBMhl6qr6/X0dH56hkDyqGpqankzS2RSCQSiZS8ZibIXxtIQ4Zemj59OnHyEoVCUfL51LGxsRkZGZGRkcpcKFAhMnzL6uvrq/PXJFxPHEgmw1f7sWPHFFkJ0cH1xIFkMnzR1tfXCwQCRRZDaHD/JSCZDL00ffr0goICRRZDaHD/JSAZ5CVpaWhowK/HgQSQl6Qlx9NkQbsEeUlayr/jE1AtkJekdfHixR07duBdBSAuyEvSgrwEJIO8JC3IS0AyyEvSgrwEJIO8JC3IS0AyyEvSgrwEJIO8JC3IS0AyyEvSgrwEJIO8JC3IS0AyGbbxIC9BXpLeiBEjCgsLsRuL3LhxIzIyUigUent7Hz16FO/SFEWGa00CIL3t27efOXOm+RAGg7Fx48aePXviV5RiQV6SFuQlmUyaNKnFrQxcXFzacSNBXpIB5CWZWFlZfffdd+KndDq9+Q142iU4viQtyEuymjBhgnjV5Orq6u/vj3dFigV5CSjQzp07Y2Ji6HT6xo0be/XqhXc5igV5SVqQl9pgwoQJ5ubmnTt3bveNJNt6afz48cS5Pp7yEe36eGUFTe/u15Tlc1h1hO5woVBIIpFIJBLehXyRiRWVzxXauOj2GmH8LfOB40vSIlReyk1rfH690rOvsWdfY21d9f1PkQsSmVRT3lRfxd8fkTVrowNVp41XQYS8pHrSXtalv2UFTbLAu5D2RiRC//76MXyNPVW7Le0EeUlaBMlLbJYw410DNJIikEhoQJj1o/PlbXs5HF+SFkGOL5XksElk4mYPVWdiSU1/W9+218LxJWkRJC/VVfM72GrjXUX7RUIOHnqVJdw2vBR+vyQtgvx+qalRwG3Cu4h2rbaCJxS0ZScC5CVpESQvAcKCvCQtguQlQFiQl6RFkLwECAvykrQIkpcAYcmwXqqpqYG8hHcVgLhk6KVZs2ZBXsK7CkBcMvSSgYEB5CW8qwDEJcOH4/Dhw4qshOggLwHJIC9Ji8/nc7ltORwO1ATkJWldunTpt99+w7sKQFyQl6RFoVC0tLTwrgIQF+QlaY0cORLvEgChQV6SlormpTnzpi5aMqv5kNdvXgQE+ly6fK75wC3b1oweGyxhPtnZWQGBPklJ8dIsNCkpfuOmVZMmjxw4uGfY1DHbtq/LyfnY1ncgg8LC/IBAn9dvXihhWZ+DvCQtFc1LPt7dU1OTmpr+e255QsIbMpmckPCm+WTx8a+9vbvLZYkJCW+XRPyoqaUVEbF629Z9s2bOf/8+ZdGSWR8/Zspl/oQFeUlaKpqXvLt1FwgE8c06Jz7hTffuvRIS34qHFBTkVVSU+8ipl65ev+Di0uWnFeu6efl6MX36fNf/933HdHR0Xr56Kpf5ExbkJWmpaF5yd/ek0Wjx8a/9u/dCCDU2Nqanp61ds+3588fZ2VkODk4IoXfxrxFCPt7+CKGqqsoDB3elpCZyOBxf3x5TJs/s2NFWPLcmbtOBg7sfPooTiUT9AwbOmjn/86/XutqaFkPo+vSY6KvipxIWcf7C6RcvHr9/n6JFpXp27TZjxjwrS2uEUOz5mOh//1qyOHLtuhWjRn2/YN6yuvq6P//ce/3GJQbDwMe7+6yZC8zNO4gX8duuzVevXTA2NunzXf+FC1Yo5k/bEuQlaaloXqJQKEymz7t3r7CnSUnvsLaxsrQWb+YlJLyxs3MwMTEVCARLIn5MSHy7ZPGqY0dOGxoYzZ03tai4UDy3fb9vd3bu/NPK9ZNCp58+c/L6jUufL9Hdnfn+fcruPVtTU5M+vzKPhEUkJyf8vn+Hm5vnhg07f1q5vrq6avOW1dirtLS0GhtZly+fi/xpw+iR3/P5/J8iF1ZUlu/67dCC+cvLykt/WrVQfLbkX8cPde3abddvh74fP/nCxTP37t9WzJ+2JchL0lLRvIQQ6ublm/Uxo66+DlsFubl11dHRcXdnvkt4jRASiURv377s1s0P+zTn5+euitzY3a+nkZHxnNmL6QyD2Nho8ay8u/kFBQ7yYvqMHDGuc2f3+619TCdPmj4lbOa16xfnL5weGOy3aMms6zcuCYVCbKyERXTp4vHX0TOTQqd5MX18ffy/Hz/5/fuU2rpa7MYzHA4nJGRqUOAga2ubFy+fvH+fMm/OUi+mT2D/gfPnLXN0dK6qqsQW4cX0CQ4a7MX0+X78ZHPzDsnJUu0v+XYy9JKtra065yUajaavr493FW3h3a07QujVq2fYKsiL6YN94N69eyUSiTKz0usb6r29/BBCySkJmpqa3bx8sReSSCSmp3di0jvxrHx9eogfd+nsUfyp8PPFkcnkaeGzT/x9fsniyP79B7IbG3fs3DhsRN/c3GzJi9DQ0CguLoxctWjYiL4BgT6rVi9BCNVUV4nn7Orihj34+DFTR0fHxuY/lz117uS6etUmMzNz7KmHO1P8EgbdoPl+F4WSIS/t3LlTkZUQ3dChQ/EuoY0cHJyMjU3i41/7+fXMzEqfNzcCIeTl5ctmszOz0uPjX2toaHh5+SKEGhrqeTxeQKBP85cbGBiKH+vq6okf6+jo1H4WjcQsLaxGDB87YvhYbG/H+g0//Xl439bNeyQs4unTh6vXREwKnfbjD4scHTu9eftyxcr5zScT7/thsRqoVNqXFq2B0ynIMiy1pqZGnX9ay+fzhUKhKu7KQwj5+PhnZLxPS02iUqlubl0RQmZm5tbWNqmpSSkpiR7uTG1tbYSQsbGJtrb25k27m79Wg/zf/3EOhy1+zGpkMRgGLRYkEomKigsNDYx0dXXFA72YPv36Bj18dFfyIq5ev+DhwZw5Yx42sKHhi9fW0tHRZbMbhUIhmdzGa6wqAuQlaaluXkIIeXv5ZedkJSa983Bnin85wvT0TktLysh8j4UlhJCjozObzTYz6+DF9MH+mZtbODm5iOeTkflB/Dg9Pc3KsmOLBdXW1kybPv6fUy1vpPmppNjY2ETyIurqak1NzMQvefz43pfejqtLFw6Hk57xHnuan5+7eOkPuB+/guNL0lLR40sYHx9/oVB47fpFJvO/G1eent4vXz4tKyv1/v9e8u7m5+fXc+fOjaWlJbW1NRcvnZ09J+zmzcvil9y7f+vlq2cIoTtxN96/TwkIGNBiQQYGhpNCp8ecPnHg4O74hDfxCW9evHwa+fPi16+fT5s6W/IinBydX795EZ/whs/nnz13CpthSemnVt+OlVXHqKh9j5/cf/3mxZ6928rLSm1t7RXzx5MWHF+SlooeX8IYGho5ODhlZ2cxPb3FA72YPvUN9TQarXNnd/HArZv3XL4Su2FTZFpacseOtkFBg8eMCUEI8fg8hNDMGfOiDu/7KXKhqalZyIQpgweN+HxZ4VN/sLSwunv/1qPHd0tLSzQ1NZme3tt/3S8+FvylRUyfPrexkbX6l6VsNnvM6JCfVq7/9Knop8iFP6/a1GIRFApl5/YDW39ds2btcoRQjx7fbd2yF/dfaspwbX7IS0TIS69uVTVxELOfEb5ltGNXowqCQs1MraiyvhDykrRUOi8BJYC8JC2VzktACSAvSUul8xJQAjgfT1oqej4eUBrIS9KCvAQkg7wkLchLQDLIS9KCvAQkg7wkLchLQDLIS9KCvAQkg7wkLchLQDLIS9KCvAQkg7wkLchLQDLIS9IiSF7S1CJrahHoB3DtD91Ii9SmF8rwv2JsbIz7ae04Ikhe0mVoVJVw8K6iPctPbzAwa8t/tAy/uQBEUPmJ++JGVZ+xHaSYFsiMVct/c7t82EyLNrxWhvVSZWWlOt+wlSB5ydhCi2FMeXe3Eu9C2qcHZz75BBlKMWErZOil2bNnFxa2cg0nNUGQvIQQ6j3SREMDvbpRwWUL8a6l/WisE1yNKug7xrSD3RevcCSZDPkH8hIR8hKm90jjhAc1144WCAUimi6hD/oJhUISiUQitS3PKwPdWKvgQ4Olk06/8aYWbW0kyEuqTSRCjXV8Vh2hD1QcOHDAy8urR48eUkyLG0MzTU3qt+4dlWE9U1lZyWAw1HbVxOPxBAIBjdb27y25I5GQLoOiyyD0/wiPXEFjNJl1lPnyCSrn/9q777gm7v8P4J8kJJCEhE1AI4KiAoqCxb2KINaFiOBslVqLu8XR9oujVVHrwPFTtNavq+5da6tWqyDWWuuoKCLiYK9AIIzs+fvj/KZUMSZ44S7h/Xz4R3JJLu9g3rl73fgc5CVjnTt3bvPmzUY8EbRQsH/JWAwGg1QLJUA2JvTGzp07zVkJ2Y0aNYroEgCpwf4lY6lUKrkcDjgAbwR5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl4BhkJeMBXkJGAZ5yViQl5pGrVbzeDyiq2gOkJdMs3//fqJLsCQpKSlBQUFBQUFEF9IcIC+ZRiwWZ2ZmEl2FZdiwYYO9vf3EiROJLqSZQF4yzdy5cykUilgsJroQsktKSvLy8oqLiyO6kOYD44k3hVQq3bx585IlS4guhKQSExP79OkTGRlJdCHNCvJSU7BYrICAgKysLKILIaOEhITBgwe3tEaCvNR0Y8aMcXV1LSgoILoQcpkxY0ZMTMyQIUOILoQAJvSSm5sb5KWGeDyek5PTnDlziC6ELKZMmRIfH9+/f3+iCyEG5KV3dfv2bRcXl/bt2xNdCMFiY2NXrFgREBBAdCGEMaGXKisrnZycYNH0OqlU+ujRo549exJdCGFGjhy5bds2Hx8fogshkgnreLNnz4a81CgWi9W5c+eoqCiiCyFGWFjYnj17WngjQV7CDZvN3r59u1AoJLqQZqXRaPr27XvmzJkWcpSQYZCX8KTVak+dOjVu3DiiC2kOYrE4PDz8+vXr5LkkNrFMWC5VVlbC/iXDqFTq2LFje/ToQXQhZldZWTly5Mhbt25BI+lBXsIZjUa7c+cOQkgikRBdi7kUFBRMmTLl2rVrRBdCLpCXzOXIkSMCgYDoKvCXk5OzYMGCixcvEl0I6UBeMqNp06bt3buX6CrwlJGRkZycfOjQIaILISPIS2aENVJhYSHRhTRRfHx8w7s3b95MSUmBRnoTyEtmd/HiRf1RsEOGDBk+fLhFdFdOTk5JSUlYWBh2NzU19dixY7t37ya6LvKCvGR2M2bMuHDhAkIoPDxcJBJVVlbevHmT6KLeLi0traKiora2NiIi4vz585cuXdq6dSvRRZEa5KVmEhERUV1dje2D6tq1K/nPdZ88eXJOTg52m8FgWET/EwvyUnMYMWIE1kjYPiiBQJCdnU10UYbcu3evvLxcf1epVA4bNozQiiwA5CWzi4iIeGXjuEAgIPnOmUuXLolEooZTKisrIyIiiKvIAkBeMjsfHx8ej0elUrVaLTaFQqGkpaURXdcbqVSq+/fvU6kvvxtarZZKpbq4uPj6+hJdGqlBXmoOQqHw9u3bV65cefbsWVVVlUwmc3JyWrt2LTlP00hPT//mm2/q6uqYTKaTk1NgYGBoaGifPn04HA7RpZFaSzl/qfSF7FmGWFqvqa1SEViGTqeTyxUyqVQul7dq3YrASgwrLS21s2OyWEzCh6p1cKGzHWj+IVy3NrbEVvJWJvRSbGzshg0bvL29zVwS/u5fqyl+Lvdoy3RpZUe1yJ+Clkuj0lWVyoueSgJ6cPx7cYkuxxATvlkWmpfuXRFVlavej/UguhDQRO5t7Px7Of5+RqBS67r2cyC6nDey8rxUni+/l1ozcCw0kjVIPVo6IMrV2YOkZ3lY+f6l3EcSJx7Z17OBkRzcbPOyyHsmi5XvX5KKNW6t4UIvVsKNbyepJe+vuZXvX6qrUiEK0UUA/NSLyNtLJvTGjh07zFkJAJbNyvMSAM3GyvMSAM3GyvMSAM0G8hIA+IC8BAA+IC8BgA/ISwDgA/ISAPiAvAQAPiAvAYAPE3rJw8MD8hIAb2JCb2zbts2clZCIWq0+f+Hs3bu3sp88olAo/n5devXq98HQUTQajejSEEIoPz/35/NnsrMfvXjx1MXZ1de304gRY3r17Et0XS2dCcul8vLylpCXikuKpk0fv3t3SocOfp9/9tXsWQt8fNqnbE9OXPy5Uql868t/PHvi23XfvEsBeXkvJkwa+aZHDx3eO236+NLS4mEfRC7/et3w4VFV1cL/JH72w4H/vsub4mXM2CGlZSVEV0EME5ZL8+bNs9DxHkyyIXmlUFixa+dhPt/rf9OG9Ov3/py5cd/t3Pz5Z18ZfnlOzuN3LCDn6RvnkJmZsWfvjlEjoxfMX4xN6dNnwIeTp61dt/zwkb2Ro8Y6OTm/47u/i/LyspoakRFPtE6Ql/6lokLw8OH9yZOmNWgkhBDq2MEvavS4X87/WFtXixBKXJKQuCRB/+ilS7+EhoVIpdKEBfGXLv9y+fL50LCQp8+enDh5KCo6/MaNa9ExEYPDe3w4Zczly+exlxw7fmDYiP76OQgE5aFhIX/8kb5v/85161dgd0+eOvxKeWnXLtva2sZ/+tkr0+fN/eLsmav6Rqqurlq1esmESSOjosNXf7usqKgAm/7j2RPRMRGFhfkffzIuNCzkk08n/HrpZ/1MsrIefvnV3MjRoR9Njd7x3Wb9tdi+Wf7lyqTE73dtDQ0Luf57KkLozz9/X71m6fiJI4aN6L9g4cz7GXcRQvcz7k6cPAohNPnD0Uu/Xoi99sDB3ZM/iho6rO9HU6M3blqtHyFw9Jiw06ePfj7/U+zv9m7/aWRhQi9t27aNz+ebsxjiPcy8jxDq03vA6w/17TtQrVZnP8408PItm3b5+3eJiBiRdvVuxw5+NJqNRCK+mvrr4YM/nf3xatjgoWvXL9d/sxv1cdzMCeOn8HgeaVfvxsZMfuXRrKyH3bp2t7e3f2U6m81msVjYbY1GM3/hjIwH9+YnLN67+7iTo/PsOVNLSosRQnQ6XSyu37pt/RcLl6VeuTNoYPj6DSsFgnJszXbRl7PlCnnKtn1JK5Jzc5/NXxCPrdLT6fTcvOe5ec9XJ23qGhgsl8tXf7tUoVD856sVa1Zv8fLyXrJ0fnV1VXBQyLertyCEDh/6adXKjQihfft3nv3pxKwZCadOXvpk2uxr6b/pfx3odPovF3709e20Yf12W1srGUQA8tK/CIUVCCF390bGWuHxPBFCFZWmXepPrVZHj5nAZDK5HG7c1BlsFvtq6qUml1cprHBze8sVyzMzMwoL8xcnJvXq2dfZ2WXWzASug+Pp00ewR1Uq1dQp8QEBgRQKZWjESJ1O9/x5DkLoypWLdBt60opkLy9vb+92ixYue/Y858Yf17BRZsvLS1d8s75v34GOjk52dna7dx1buGBJcFBIcFDIzBkJMpks81HGK2XUi+uPHvvhow+n9+//Psee8/6g8DFR4w8d3qNSqbB5crkO8+YsCnmvF0m26Lw7E3pp3bp1JSUtIlZqdVoc59axoz92g0KhtGrFLyzMe5e56VeTEEIFBXmhYSH6f/v270QIZT7KoNPp3YN76N80qNt7Dx7+rX+Vn19n7AaHw0UIicX1CKGsrAd+fp0dHByxhzw8PFu14mNLaYRQWy+fhoNOSqWSbSkbYsZ9EBoWgq2pvh6TiooKVCqVv3+Xhn8HsVhcUlKE3e3UMeBd/g4kZEL+UavVVvMT8iauru4IIYGgjMt5dVjDygrBmxZZhjVch7G1s5NIxE0uz83VvaLin8tP8HiemzbuxG4nrXq5NUIsrlepVKFhIQ1f6OjopL9NoTQyAoZYXP8k5/ErrxJVV2E3GA0+gkBQ/vn86d2Dey5bsgZbvg0Z2vv1GVZXCxFCdrb/dCCTyUIIyWQv05H1XYAd9i/9S2CXIITQzZvXO/h2euWh23du0un0gIDA11+l0WoMzFMikbDZbOy2Qi53cmxkU5vhOeh1CQw6d+5UbW0NtgCxs7MLDnr57dd/NV1cXJlM5upVmxu+kEZ9y4+gs4trYGDQx3EzG0504Dq+/sxr6b8plcr/fLWCyWQ2ukTCsNn2CCGZXKafIpVKEELOzq7GfFJLBHnpX3g8j0EDw46fOFBYmN9wekFB3pkfj42OjMWWVww6A/tmYAxvTrifcQe7oVAoCovyfXzaI4TodIZCodD/PQsLjFrxGz0qhkqlbt22vuGaHvaFlv5vs1v79h1lMpm7uweWZ4KDQng8T9/Xfhpe0b5dh4qK8m5du+tf5eTo7OXVyP6PurpaDoeLNRJCKP361cZn2L4jjUbLynqgn5Kd/Yhjz3Fzczfmk1oiE3pp3rx5LeF4vIULl7bht50zL+7EyUP3M+7ez7h7/MTBuZ99/N57vWbEv9wY7e/f5cmTrNzc5wihu/f+wjI6pnXrNtnZj/6+f0ckqsauXHbmzLHCwnyNRrN333cKhSJs8AcIoYCAQJ1Oh22SFgjKjxz75zKBfL5XVZXwxo1rr7eol5f3ksWrrqVfWbBo5u830u5n3L339+2Nm1Z/8ukEd55HeNgwhNB73Xv27Nk3OTlJICivra05+9PJmbM++vXXc4Y/dUzMZK1Wm7Jjo1wuLyoq+H7X1mnTx+fmPX/9me3adaiqEp77+bRarf7r9s2//77t4OCIrXm28fJGCF279tvj7EdcDndI+PBDh/fevHm9rr7u8uXzP549HhMzWX8pGutjwjpeS9i/hBDi2HO2/t+en385fffeX0eO7qfT6X6dOs+ZvTBiyAj99yBq9LjCwvz4mZM1Gs3g0IgPJ01bu345Npr0qBHRT59mf/HlnHVrt2HhZFzshwsWzayqEjKZzP98ubxNm7YIIX+/zrNmJuzatXXjptUBAYHx0+clLIjH5tC7V//ALkHLvlk0dUp83NT4V8obOGDwvj0nzp47eeTo/vz8Fy7Orq5u7mOjJ06aGKd/zrert5z7+fTKVYmPH2e2adM2PHxYdPQEw5+ay+Hu2X382LEfZsz6sLAw38+v8xeLlnXs4Pf6M8MGDy0oyD1w8L+bt3zbI6T3V18uP3b8wJGj++vr6xbMX/zB0FH79u/s0rnb5k3fz5m9kEqlJq1erFarW7XiT5r48cQJU/H4LyIpKx9P/Mz2ksD+zh7eTELe/fSZYzu+23T1t9uEvLv1KXwiyX9UN+ITT6ILaRzkJQDwAXkJAHzA8XhmNDZ6AqzgtRywfwkAfEBeAgAfkJcAwAfkJQDwAXkJAHxAXgIAH5CXAMAH5CUA8AF5CQB8WHlesmPSKFZ7jH+LQ7OhMJjk/e+08rzEsKPUV1tY/4M3qRUqbZnkHSXByvMSz8tOVg+9ZCXkYo07n7wDgFn5+HiB/R2e3K2R1kE7WbyaCmXxM4lfDw7RhbyRleclhNDERV5XjpZWlSiILgQ0XXme7MZZQWwCqX/KTTivNjY21kLHE1fKtZcPCyqLFK07sNRKKzyPWKvTUd4wWJelo9J0JS9knt7MD6Z4vG0wJYK1iPEeGHbUkZ94imvUVWVKhcyo0bMsy+XLl21tbQcNGkR0IfizZdEGRLmxueRuI4Ssf7yHFmL79u1sNjsuLs6I5wJzMSEvlZSUYINBAwBeZ0IvJSQktJDxxAFoAhN6qXXr1nQ63ZzFAGDBTNiWsGXLFnNWAoBlg7wEAD4gLwGAD8hLAOAD8hIA+IC8BAA+IC8BgA/ISwDgA/ISAPiAvAQAPiAvAYAPyEsA4APyEgD4gLwEAD4gLwGAD8hLAOAD8hIA+IC8BAA+IC8BgA8TeonP50NeIieRSOTo6Eh0FS2dyePjnTlzxt/fr4ZacAAACiRJREFU39/f32wlAdMkJydXV1evWbOG6EJaOpOvZtO9e/c1a9YUFBSYpx5ggrq6uokTJ/L5fGgkUtA1SW1trVqtXrZsmVAobNocwDtKTU0NDQ19+vQp0YWAl5p4lTUul0uj0Xr37r1+/XqEkEIBV5FoVsnJyRcuXEhNTe3QoQPRtYCX8BlP/NSpUwUFBQkJCTSaBYyhbtHq6upmzJgxevToCRMmEF0L+BfcxuY/evQon88fMGCATqezyouXkEFaWlpSUtL3338PiyMSwv86F8OGDYuLixs/fjy+swXJyckCgWDDhg1EFwIah/9VqS9evIj1Z25uLu4zb5n02+ugkcjMLFd4x1blVSrVoEGDnj9/bo63aDnS0tKioqJWrlwJAYnkzHstM4lE8vTp0+Dg4PT0dKu8ap25wXqdBTHLckmPzWYHBwcjhHJycmJiYsz6XlYG1ussTvNdY7OsrMzT0zMzM1MkEg0cOLB53tRCwfY6S9R813L29PRECPn6+i5durSysnLs2LHN9taWBVuvS01NJboQYBpirv1cUVHh7u6ekpLSr18/bCUQwH5YS2fevPQm7u7u2J6oHTt2yGQyOAQJttdZAWKWSw1pNJqKiork5OTExERXV1diiyEKbK+zAsQslxqi0Wienp6RkZGnT59GCNXU1BBdUbOC7XVWg/jl0itSUlJEItHixYut8jDZ8PDwK1eu6O/C9jprQvxy6RVz587t1q1bcXGxQqGQSCQNHwoNDZ07dy5xpb2rpKQkkUg0ZMgQ7C6cN2FlSNdLCKHIyMi2bdtSqdThw4efO3dOP722tvb+/fv79+8ntLomysvL++uvvygUikgkioyMhPU660PGXsLQ6fT09HQWi4UQunPnzqBBg6hUqkKhOHHiRFZWFtHVmezgwYNlZWXY7eLiYtheZ33I20uY8PBwhBCVSq2vr8emCASCr7/+mui6TJOfn3/79m39aV1UKnXq1KlEFwVwRvZewsybN49KfVkqhULJz89PTEwkuigT/PDDD6WlpQ2nKJXKoUOHElcRwJ9l9JJcLm94l0Kh3Lhx46effiKuIhPk5uZiCyWNRqPT6TgcTqtWrfh8fuvWrYkuDeCJdNvEXzd48GAqlWpjY4ONmoTtktLpdFQq9fz589hzCnNk1QKlWKQW12rUap1OQ6IPlZ+fX1dXR6PRbBg6e44915nhwmN27OLu4+dCdGkATxbQS5js7GyNRqPVatVqtVar1el0NjY2HGrHJ3frCrIlzq05Oh3FxpZGt6XRbKjk/EhUKkWl0KgUarVCo5QqNUp1u0D7gJ4cD287oksDOLCYXnpF7iPJ72eFDu4sCsOW68amWMa66r+oFZp6oVQllTEYaFC0q5M7jC9t2Syyl349UFFdoXZp62xrbw3fv/pKaWVetX8Ip88IZ6JrAU1nYb0kqVUfWlvo1c2D6WBLdC04qy2p0yhl0bNbEV0IaCJL6iWZRHtgdYFvbz6NboGrdEaoF8qklbXjF8D2PYtkMV9KuUTzw6r8TgO8rLWREEIcV6Y9z/HgmkKiCwFNYTHfy4PfFvr24hNdhdmxne04Hg4X9gmILgSYzDJ66erxSk8/dxtbKzwL43WOnvYKFT3rVh3RhQDTWEAvleXJS3MV9i4taCeMY2tu+ulKoqsAprGAXvr9rNDFu2VtLKZQKbx2Dn9drCK6EGACsvdS8TMZjUFnOZJ0C3hG5pVFy3qJJSLc5+zi7ZSbJdNpcZ8xMBey99KzB2IKnUF0FcTQImreY4kRTwSkQPZeyn0o5rixia6CGCxH1ouHYqKrAMZqvnFbm0BYquS6Mel25tp8l1/48HLa7qLix/ZsJ/9O/SNCp9vZsRFCf9w6+Vv63lnTvjtwLFFQkevJ8x3Yd2KP7iOxV/3y67a7Dy7YMljBXYe6u3qZqTaEENedLcqXmm/+AF+kXi7VV6sUMnMlBmFV0ff756lUirnxu6dOWlcmePbd3lkajRohRLOhy2T1Z88nj4tavGHlra5dBp84u0pUU44Qunn79M3bp6JHfPH5jH0uTq1+S9tjpvIQQjQ6VVAo1agt5sCUFo7UvSSt19AY5loo/f3gVxsaPW7iOp6bt4d7u9jRS0rKch5lp2OPajSqIaHT27YJpFAoIUEjdDpdSdlThNCNP0907RzWtctgFovbo/tI33YhZioPY8u0kdRpzPoWAC+k7iWZWGPDMNdaaH7hwzb8ADbbEbvr7OTp4szPK8jQP8GrdWfsBovJRQjJ5PU6nU5YXcRz99E/h9/Kz0zlYZgchhR6yUKQOi9RKEhrtjNkZXJxUcnjRct6NZxYV//PLp3Xr2AtV0i0Wo2tLUs/hcFgmqk8jEqpoZL65w78g9S9xOLStGpzDdvP4bj4tA0aOji+4UQ228HAS+xs2VQqTaX6Z/AJhdK82wZUcjXboUUcOWUFyN1LHBuNylxrOK14He49uNDOO1g/wlF5Ra6bi6HtchQKxcnRM78wc1C/l1Oyc/4wU3kYhVTD4pL6/wjokXoFwsGVbmPz6ooWXgb2najVas9d3KxUyisqC365lLIxZVKZ4C1Xqu7WJTzzcVpG5hWEUOrvBwqKH5mpPISQSq7htWW+tqYJSIrUveToRlfKVAqxyhwzZ7G4i+YeYdCZW3ZOXb91XG7+37FRS966LSF80Me93ht99sLGRct6Zef8ETksQT86Eu7qKsTubVroMR+WiOzn1f7xc1VpEXLzcSS6EAIUPSgLjXHh+5p38wbAC6mXSwihjsH2WpVZlkskp9XoGLZUaCQLQvZc68a3ZbF0tQKJA6/xo/KqRaWbdnzU6ENMW3uZovHj2Tzc2s2N/y+OdS5dHdbodK1Wo9PpaLRG/s6dfHt9NH7Nm2ZY8byqc48WeiCihSL7Oh5CqFaoOrm1xLdPm0Yf1WjUtXUVjT6kVMoZjMbPIKRSbRwd3HEsslpU+qaHlCoFg97IOSM2NrZcTuNDtypl6uKHZdOWe+NYITA3C+glhNCf56uFQhrH3Z7oQppJVX5V94Fsn86wXLIkZM9LmD4jnGWiemmN3IjnWryqfFFrbxtoJItjGb2EEBqXwC/OFChlaqILMS9hfo0tQ917WMs6J986WMY6nt6uxbmtO/PYztY5jkpVYY2ziy40xpXoQkBTWFgvIYRObCm2c+BwPawqO+l0SJhb5cGnDoiCRrJUltdL2KaIrFt1ru2cHT2sIVQI82oEL0RDJnt07G5VPxAtjUX2EkKoXqS+flYorkOIasN1ZzO5lnesTb1QVi+UqKVK327sviMhIFk8S+0lTHW58lmG+PkDCaJS1SqdDYNGo9tQaSQ9GpRqQ1XJ1RqVWq3QSGuVvLbMTt3Znd7j0m1JWjAwiWX3kp6kRlNTpZTWaSS1apVCS86PRGdQbOhUFteGzaW5e9nBSX5Wxkp6CQDCwW8jAPiAXgIAH9BLAOADegkAfEAvAYAP6CUA8PH/fgtK5VScxQwAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(CRAG.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cfe7d1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LLM Decision Maker': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '5r65jn2ht', 'function': {'arguments': '{\"query\":\"types of agent memory\"}', 'name': 'Jalammar_Blog_Post'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 648, 'prompt_tokens': 149, 'total_tokens': 797, 'completion_time': 2.993420117, 'prompt_time': 0.01057799, 'queue_time': 0.05776752499999999, 'total_time': 3.003998107}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2af0cba8-530d-400c-907c-4d84bd698656-0', tool_calls=[{'name': 'Jalammar_Blog_Post', 'args': {'query': 'types of agent memory'}, 'id': '5r65jn2ht', 'type': 'tool_call'}], usage_metadata={'input_tokens': 149, 'output_tokens': 648, 'total_tokens': 797})]}}\n",
      "----CALLING GRADE FOR CHECKING RELEVANCY----\n",
      "----DECISION: DOCS ARE NOT RELEVANT----\n",
      "{'Vector Retriever': {'messages': [ToolMessage(content='These blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in Character-Level Language Modeling with Deeper Self-Attention to create a language model that predicts one letter/character at a time.\\nThe OpenAI GPT-2 model uses these decoder-only blocks.\\nCrash Course in Brain Surgery: Looking Inside GPT-2\\n\\nGPT-2 holds on to the key and value vectors of the the a token. Every self-attention layer holds on to its respective key and value vectors for that token:\\n\\n\\n\\n\\nNow in the next iteration, when the model processes the word robot, it does not need to generate query, key, and value queries for the a token. It just reuses the ones it saved from the first iteration:\\n\\nThis masking is often implemented as a matrix called an attention mask. Think of a sequence of four words (“robot must obey orders”, for example). In a language modeling scenario, this sequence is absorbed in four steps – one per word (assuming for now that every word is a token). As these models work in batches, we can assume a batch size of 4 for this toy model that will process the entire sequence (with its four steps) as one batch.\\n\\nThey’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.', name='Jalammar_Blog_Post', id='a95e60ed-ec90-4995-b81b-a291d2c36f43', tool_call_id='5r65jn2ht')]}}\n",
      "----TRANSFORM QUERY----\n",
      "Transformed Query is: content='<think>\\nOkay, so I need to figure out the types of agent memory. Hmm, I\\'m not entirely sure what \"agent memory\" refers to. I think it might be related to AI or software agents. Let me break this down.\\n\\nFirst, I should clarify what an \"agent\" is in this context. From what I remember, in AI, an agent is something that perceives its environment and takes actions. So, maybe agent memory is about how these agents store information.\\n\\nNow, the question is asking about the types. I guess there are different ways agents can remember things. Maybe short-term vs. long-term? That\\'s a common division in human memory. So perhaps agents have similar distinctions.\\n\\nI should also consider other possibilities. Maybe episodic memory, which is about specific events, and semantic memory, which is about facts and knowledge. Do agents use these categories too?\\n\\nWait, in AI, especially in areas like reinforcement learning, agents might have memory in terms of experience replay. That\\'s where they store past interactions to learn from them later. So that could be a type of memory.\\n\\nAnother thought: blackboard memory. I think that\\'s a term used in certain AI architectures where different components write and read information to a shared space. It\\'s a way of structuring knowledge, so that might be another type.\\n\\nThen there\\'s working memory, which is about temporarily holding information for tasks. Humans use it for mental calculations, so maybe AI agents have something similar for processing current tasks.\\n\\nI\\'m not sure if I\\'m missing any. Maybe procedural memory, which is about skills and how to do things. In agents, this could relate to learned behaviors or policies they follow.\\n\\nI should also check if there\\'s a standard classification. Perhaps looking into cognitive architectures might help. For example, SOAR or ACT-R have specific memory types.\\n\\nWait, another angle: in robotics, agents might have sensorimory memory, which is brief storage of sensory data. That could be another type.\\n\\nSo, putting it all together, the types could include:\\n\\n1. Short-term memory\\n2. Long-term memory\\n3. Episodic memory\\n4. Semantic memory\\n5. Procedural memory\\n6. Working memory\\n7. Blackboard memory\\n8. Experience-based memory (like experience replay)\\n9. Sensorimory memory\\n\\nI think that covers a broad range. Now, I should present this in a clear way, maybe numbering each type with a brief explanation.\\n</think>\\n\\nThe types of agent memory can be categorized as follows:\\n\\n1. **Short-term Memory**: Stores information temporarily, often for immediate processing.\\n2. **Long-term Memory**: Retains information over an extended period, essential for persistent knowledge.\\n3. **Episodic Memory**: Stores specific events or experiences, allowing agents to recall past interactions.\\n4. **Semantic Memory**: Holds factual knowledge and general information about the world.\\n5. **Procedural Memory**: Involves skills and procedures, enabling agents to perform tasks and follow policies.\\n6. **Working Memory**: Temporarily holds information necessary for current tasks or computations.\\n7. **Blackboard Memory**: A shared space where different components write and read information, facilitating structured knowledge management.\\n8. **Experience-based Memory**: Stores past interactions, such as in experience replay, used for learning and improvement.\\n9. **Sensorimory Memory**: Briefly holds sensory data, crucial for immediate environmental interaction.\\n\\nThese categories provide a comprehensive overview of how agents handle and store information.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 710, 'prompt_tokens': 44, 'total_tokens': 754, 'completion_time': 2.989065222, 'prompt_time': 0.00209846, 'queue_time': 0.058062987999999996, 'total_time': 2.991163682}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None} id='run--509d94b8-a02d-4a6d-862d-815264d5b25f-0' usage_metadata={'input_tokens': 44, 'output_tokens': 710, 'total_tokens': 754}\n",
      "{'Query Rewriter': {'messages': [AIMessage(content='<think>\\nOkay, so I need to figure out the types of agent memory. Hmm, I\\'m not entirely sure what \"agent memory\" refers to. I think it might be related to AI or software agents. Let me break this down.\\n\\nFirst, I should clarify what an \"agent\" is in this context. From what I remember, in AI, an agent is something that perceives its environment and takes actions. So, maybe agent memory is about how these agents store information.\\n\\nNow, the question is asking about the types. I guess there are different ways agents can remember things. Maybe short-term vs. long-term? That\\'s a common division in human memory. So perhaps agents have similar distinctions.\\n\\nI should also consider other possibilities. Maybe episodic memory, which is about specific events, and semantic memory, which is about facts and knowledge. Do agents use these categories too?\\n\\nWait, in AI, especially in areas like reinforcement learning, agents might have memory in terms of experience replay. That\\'s where they store past interactions to learn from them later. So that could be a type of memory.\\n\\nAnother thought: blackboard memory. I think that\\'s a term used in certain AI architectures where different components write and read information to a shared space. It\\'s a way of structuring knowledge, so that might be another type.\\n\\nThen there\\'s working memory, which is about temporarily holding information for tasks. Humans use it for mental calculations, so maybe AI agents have something similar for processing current tasks.\\n\\nI\\'m not sure if I\\'m missing any. Maybe procedural memory, which is about skills and how to do things. In agents, this could relate to learned behaviors or policies they follow.\\n\\nI should also check if there\\'s a standard classification. Perhaps looking into cognitive architectures might help. For example, SOAR or ACT-R have specific memory types.\\n\\nWait, another angle: in robotics, agents might have sensorimory memory, which is brief storage of sensory data. That could be another type.\\n\\nSo, putting it all together, the types could include:\\n\\n1. Short-term memory\\n2. Long-term memory\\n3. Episodic memory\\n4. Semantic memory\\n5. Procedural memory\\n6. Working memory\\n7. Blackboard memory\\n8. Experience-based memory (like experience replay)\\n9. Sensorimory memory\\n\\nI think that covers a broad range. Now, I should present this in a clear way, maybe numbering each type with a brief explanation.\\n</think>\\n\\nThe types of agent memory can be categorized as follows:\\n\\n1. **Short-term Memory**: Stores information temporarily, often for immediate processing.\\n2. **Long-term Memory**: Retains information over an extended period, essential for persistent knowledge.\\n3. **Episodic Memory**: Stores specific events or experiences, allowing agents to recall past interactions.\\n4. **Semantic Memory**: Holds factual knowledge and general information about the world.\\n5. **Procedural Memory**: Involves skills and procedures, enabling agents to perform tasks and follow policies.\\n6. **Working Memory**: Temporarily holds information necessary for current tasks or computations.\\n7. **Blackboard Memory**: A shared space where different components write and read information, facilitating structured knowledge management.\\n8. **Experience-based Memory**: Stores past interactions, such as in experience replay, used for learning and improvement.\\n9. **Sensorimory Memory**: Briefly holds sensory data, crucial for immediate environmental interaction.\\n\\nThese categories provide a comprehensive overview of how agents handle and store information.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 710, 'prompt_tokens': 44, 'total_tokens': 754, 'completion_time': 2.989065222, 'prompt_time': 0.00209846, 'queue_time': 0.058062987999999996, 'total_time': 2.991163682}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None}, id='run--509d94b8-a02d-4a6d-862d-815264d5b25f-0', usage_metadata={'input_tokens': 44, 'output_tokens': 710, 'total_tokens': 754})]}}\n",
      "----Searching Web for requisite context----\n",
      "{'Web Search': {'messages': [AIMessage(content='Sure! Let me break down how sentence sentiment classification works under the hood.\\n\\n### 1. **Understanding the Task**\\n   - **Objective**: Determine the sentiment (e.g., positive, negative, neutral) of a given sentence.\\n   - **Example**: \\n     - Input: \"I loved the movie!\"\\n     - Output: Positive\\n\\n### 2. **Data Collection**\\n   - **Datasets**: Use labeled datasets where each sentence is tagged with its sentiment (e.g., IMDB for movie reviews, Stanford Sentiment Treebank).\\n   - **Preprocessing**: Clean the data by removing noise, handling out-of-vocabulary words, and normalizing text (e.g., lowercasing).\\n\\n### 3. **Text Preprocessing**\\n   - **Tokenization**: Split sentences into words or tokens.\\n   - **Stopword Removal**: Remove common words like \"the,\" \"and\" that don\\'t add much value.\\n   - **Stemming/Lemmatization**: Reduce words to their base form (e.g., \"loving\" becomes \"love\").\\n   - **Vectorization**: Convert text into numerical representations (e.g., word embeddings like Word2Vec, GloVe).\\n\\n### 4. **Feature Extraction**\\n   - **Bag of Words (BoW)**: Represent text as a bag (or histogram) of word frequencies.\\n   - **Term Frequency-Inverse Document Frequency (TF-IDF)**: Weight word importance by its frequency in the document and rarity across documents.\\n   - **Word Embeddings**: Use pre-trained embeddings to capture semantic meanings (e.g., \"king\" is close to \"queen\").\\n\\n### 5. **Model Training**\\n   - **Traditional Models**:\\n     - **Support Vector Machines (SVM)**: Linear or nonlinear classifiers to separate sentiments.\\n     - **Naive Bayes (NB)**: Probabilistic model based on Bayes\\' theorem.\\n   - **Deep Learning Models**:\\n     - **Recurrent Neural Networks (RNNs)**: Process sequences with LSTM or GRU layers.\\n     - **Convolutional Neural Networks (CNNs)**: Apply convolutions over text for feature extraction.\\n     - **Transformers**: Use models like BERT, RoBERTa for state-of-the-art performance with self-attention mechanisms.\\n\\n### 6. **Model Evaluation**\\n   - **Metrics**: Accuracy, Precision, Recall, F1-score, ROC-AUC.\\n   - **Cross-Validation**: Ensure model generalizes well by testing on unseen data.\\n\\n### 7. **Prediction**\\n   - **Input**: A new sentence.\\n   - **Process**: Preprocess, convert to features, feed into the model.\\n   - **Output**: Predicted sentiment label.\\n\\n### 8. **Continuous Improvement**\\n   - **Hyperparameter Tuning**: Adjust model parameters for better performance.\\n   - **Ensemble Methods**: Combine multiple models for improved accuracy.\\n   - **Active Learning**: Use human feedback to enhance the model incrementally.\\n\\n### Example Workflow\\n1. **Data**: Collect and preprocess text data.\\n2. **Features**: Convert text to numerical features using embeddings.\\n3. **Model**: Train a classifier (e.g., Logistic Regression, LSTM).\\n4. **Evaluate**: Test on validation set and fine-tune.\\n5. **Deploy**: Use the trained model to predict sentiments on new sentences.\\n\\n### Tools and Libraries\\n- **Python Libraries**: NLTK, SpaCy, Gensim, Scikit-learn, TensorFlow, PyTorch, Hugging Face Transformers.\\n- **Pre-trained Models**: BERT, RoBERTa, DistilBERT for efficient fine-tuning.\\n\\nBy following these steps, you can build a robust sentiment classification model tailored to your specific needs.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 927, 'prompt_tokens': 153, 'total_tokens': 1080, 'completion_time': 3.475544483, 'prompt_time': 0.02040858, 'queue_time': 0.05812537200000001, 'total_time': 3.495953063}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None}, id='run--527c0f0f-dca0-4dd8-ba53-fbcd048319fe-0', usage_metadata={'input_tokens': 153, 'output_tokens': 927, 'total_tokens': 1080})]}}\n",
      "----RAG OUTPUT GENERATE----\n",
      "this is my response:content=\"<think>\\nOkay, so I need to figure out the types of agent memory. Hmm, the context provided is about sentence sentiment classification. It goes through steps like understanding the task, data collection, preprocessing, feature extraction, model training, evaluation, prediction, and continuous improvement. It also mentions tools and libraries like NLTK, SpaCy, etc.\\n\\nWait, the question is about agent memory types, but the context is all about sentiment analysis. I don't see any mention of memory types in the context. Maybe it's related to AI agents and their memory structures, but since the context doesn't cover that, I can't extract the answer from here.\\n\\nI should probably say that the context doesn't provide the information needed to answer the question about agent memory types. So, I don't know the answer based on the given context.\\n</think>\\n\\nThe context provided does not include information about the types of agent memory. \\n\\nAnswer: I don't know.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 828, 'total_tokens': 1021, 'completion_time': 0.914296435, 'prompt_time': 0.061045178, 'queue_time': 0.054005852, 'total_time': 0.975341613}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None} id='run--9edbc23f-5bc3-46fa-a0c3-94bbfeecbc69-0' usage_metadata={'input_tokens': 828, 'output_tokens': 193, 'total_tokens': 1021}\n",
      "{'Output Generator': {'messages': [AIMessage(content=\"<think>\\nOkay, so I need to figure out the types of agent memory. Hmm, the context provided is about sentence sentiment classification. It goes through steps like understanding the task, data collection, preprocessing, feature extraction, model training, evaluation, prediction, and continuous improvement. It also mentions tools and libraries like NLTK, SpaCy, etc.\\n\\nWait, the question is about agent memory types, but the context is all about sentiment analysis. I don't see any mention of memory types in the context. Maybe it's related to AI agents and their memory structures, but since the context doesn't cover that, I can't extract the answer from here.\\n\\nI should probably say that the context doesn't provide the information needed to answer the question about agent memory types. So, I don't know the answer based on the given context.\\n</think>\\n\\nThe context provided does not include information about the types of agent memory. \\n\\nAnswer: I don't know.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 828, 'total_tokens': 1021, 'completion_time': 0.914296435, 'prompt_time': 0.061045178, 'queue_time': 0.054005852, 'total_time': 0.975341613}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None}, id='run--9edbc23f-5bc3-46fa-a0c3-94bbfeecbc69-0', usage_metadata={'input_tokens': 828, 'output_tokens': 193, 'total_tokens': 1021})]}}\n"
     ]
    }
   ],
   "source": [
    "# CRAG.invoke({\"messages\":[\"What are the types of agent memory?\"]})\n",
    "for output in CRAG.stream({\"messages\":[\"What are the types of agent memory?\"]}):\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
